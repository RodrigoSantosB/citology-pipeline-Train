{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4495a09d",
   "metadata": {},
   "source": [
    "## 1. **Configura√ß√£o de Ambiente e Reprodu√ß√£o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ec217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:12:58.731703: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-02 15:12:58.786802: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# IMPORTS E CONFIGURA√á√ïES\n",
    "# =================================================================\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# TFMOT (TensorFlow Model Optimization) √© essencial para QAT\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.quantization.keras import quantize_model, quantize_scope\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6157cb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Nenhuma GPU detectada. Usando a estrat√©gia padr√£o.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:13:05.358475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-02 15:13:05.359940: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: /home/ampliar/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/tensorflow/python/../../../../libcublas.so.11: undefined symbol: cublasLt_for_cublas_HSS, version libcublasLt.so.11; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2025-12-02 15:13:05.385244: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configura√ß√£o de Reprodu√ß√£o ---\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# --- Configura√ß√£o de GPU/Dispositivo ---\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(f\"üî• Usando estrat√©gia de distribui√ß√£o com {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"‚ö†Ô∏è Nenhuma GPU detectada. Usando a estrat√©gia padr√£o.\")\n",
    "\n",
    "strategy_scope_used = len(gpus) > 0\n",
    "\n",
    "# --- Paths e Configura√ß√µes Globais ---\n",
    "# ATEN√á√ÉO: Ajuste este PATH_ROOT para o seu ambiente\n",
    "ROOT = Path(\"/home/ampliar/cancer-classify-citology/citology-pipeline-Train\") \n",
    "DATA_DIR = ROOT / 'Dataset' / 'pre-processado'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "METRICS_DIR = ROOT / 'metrics' # NOVA PASTA PARA HIST√ìRICOS E GR√ÅFICOS\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_CSV = DATA_DIR / 'train_data.csv'\n",
    "VAL_CSV = DATA_DIR / 'val_data.csv'\n",
    "TEST_CSV = DATA_DIR / 'test_data.csv'\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "IMAGE_ROOT = DATA_DIR \n",
    "\n",
    "# Hyperpar√¢metros de Treinamento\n",
    "EPOCHS_BASELINE = 10\n",
    "LR_BASELINE = 1e-4\n",
    "\n",
    "EPOCHS_ROBUST = 20\n",
    "LR_ROBUST_CANDIDATES = [1e-3, 1e-4, 5e-5] # Candidatos do seu c√≥digo original\n",
    "LR_FINETUNE = 1e-5 # LR para Fine-Tuning FP32 (Etapa 3)\n",
    "EPOCHS_FINETUNE = 10\n",
    "\n",
    "# Hyperpar√¢metros QAT\n",
    "LR_QAT = 1e-5 # LR para Fine-Tuning QAT (Etapa 4)\n",
    "EPOCHS_QAT = 10 \n",
    "QAT_CHECKPOINT = str(MODELS_DIR / 'qat_checkpoint.keras')\n",
    "QAT_TFLITE_PATH = MODELS_DIR / 'qat_model.tflite'\n",
    "CHECKPOINT_BASELINE = str(MODELS_DIR / 'baseline_checkpoint.keras')\n",
    "CHECKPOINT_ROBUST = str(MODELS_DIR / 'robust_best.keras') # Ser√° o ponto de partida do FT/QAT\n",
    "CHECKPOINT_FINETUNE = str(MODELS_DIR / 'final_finetuned_checkpoint.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d044775",
   "metadata": {},
   "source": [
    "## 2. **Carregamento de Dados e Geradores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8a3b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configura√ß√µes de Paths:\n",
      "DATA_DIR = /home/ampliar/cancer-classify-citology/citology-pipeline-Train/Dataset/pre-processado\n",
      "MODELS_DIR = /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models\n",
      "BATCH_SIZE ESTRAT√âGICO = 32\n",
      "Treino: 24951 | Valida√ß√£o: 5347 | Teste: 5347\n",
      "N√∫mero de Classes: 3\n",
      "Found 24949 validated image filenames belonging to 3 classes.\n",
      "Found 5347 validated image filenames belonging to 3 classes.\n",
      "Found 5347 validated image filenames belonging to 3 classes.\n",
      "\n",
      "Classes detectadas: {'HISIL': 0, 'LISIL': 1, 'Normal': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ampliar/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/keras/preprocessing/image.py:1139: UserWarning: Found 2 invalid image filename(s) in x_col=\"image_path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Carregamento de Dados (Para defini√ß√£o de classes) ---\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    val_df = pd.read_csv(VAL_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    num_classes = train_df['lesion_type'].nunique()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao carregar CSVs: {e}. Usando valores padr√£o.\")\n",
    "    train_df = pd.DataFrame({'image_path': [], 'lesion_type': []})\n",
    "    val_df = train_df.copy()\n",
    "    test_df = train_df.copy()\n",
    "    num_classes = 3 # Valor padr√£o seguro (Baseado em HISIL, LISIL, Normal)\n",
    "\n",
    "\n",
    "print('\\nConfigura√ß√µes de Paths:')\n",
    "print('DATA_DIR =', DATA_DIR)\n",
    "print('MODELS_DIR =', MODELS_DIR)\n",
    "print('BATCH_SIZE ESTRAT√âGICO =', BATCH_SIZE)\n",
    "print(f'Treino: {len(train_df)} | Valida√ß√£o: {len(val_df)} | Teste: {len(test_df)}')\n",
    "print(f'N√∫mero de Classes: {num_classes}')\n",
    "\n",
    "# --- Fun√ß√µes Utilit√°rias ---\n",
    "\n",
    "def make_generators(train_df, val_df, test_df, image_root=IMAGE_ROOT, augment=False, batch_size=BATCH_SIZE, img_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "    \"\"\"Cria ImageDataGenerators e retorna (train_gen, val_gen, test_gen).\"\"\"\n",
    "    if augment:\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    common_kwargs = dict(x_col='image_path', y_col='lesion_type', target_size=img_size, class_mode='categorical')\n",
    "\n",
    "    train_gen = train_datagen.flow_from_dataframe(dataframe=train_df, directory=str(image_root) if image_root is not None else None, batch_size=batch_size, shuffle=True, **common_kwargs)\n",
    "    val_gen = val_datagen.flow_from_dataframe(dataframe=val_df, directory=str(image_root) if image_root is not None else None, batch_size=batch_size, shuffle=False, **common_kwargs)\n",
    "    test_gen = test_datagen.flow_from_dataframe(dataframe=test_df, directory=str(image_root) if image_root is not None else None, batch_size=1, shuffle=False, **common_kwargs)\n",
    "\n",
    "    return train_gen, val_gen, test_gen\n",
    "\n",
    "def build_model(input_shape=INPUT_SHAPE, num_classes=num_classes, base_trainable=False, learning_rate=LR_BASELINE):\n",
    "    \"\"\"Constr√≥i um MobileNetV2 com topo customizado.\"\"\"\n",
    "    base = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base.trainable = base_trainable\n",
    "\n",
    "    x = base.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base.input, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model, base\n",
    "\n",
    "\n",
    "def save_history(history: tf.keras.callbacks.History, stage_name: str, checkpoint_path: str) -> None:\n",
    "    \"\"\"Salva o objeto history em um arquivo JSON e a imagem do gr√°fico.\"\"\"\n",
    "    stage_dir = METRICS_DIR / stage_name\n",
    "    os.makedirs(stage_dir, exist_ok=True)\n",
    "    \n",
    "    # üåü CORRE√á√ÉO: Converte valores numpy (float32, etc.) para floats nativos do Python\n",
    "    history_data = {}\n",
    "    for key, values in history.history.items():\n",
    "        # Converte a lista de arrays NumPy para uma lista de floats Python\n",
    "        history_data[key] = [float(v) for v in values]\n",
    "    \n",
    "    # 1. Salva o hist√≥rico como JSON\n",
    "    history_path = stage_dir / f\"{Path(checkpoint_path).stem}_history.json\"\n",
    "    with open(history_path, 'w') as f:\n",
    "        # Usa o dicion√°rio corrigido\n",
    "        json.dump(history_data, f, indent=4) \n",
    "    print(f\"‚úÖ Hist√≥rico de treinamento salvo em: {history_path}\")\n",
    "\n",
    "# 2. Plota e salva o gr√°fico\n",
    "def plot_history(history: tf.keras.callbacks.History, title: str, save_path: Optional[Path] = None) -> None:\n",
    "    \"\"\"Plota as curvas de treinamento e valida√ß√£o e salva a imagem.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Loss (Treino)')\n",
    "    plt.plot(history.history['val_loss'], label='Loss (Valida√ß√£o)')\n",
    "    plt.title(title + ' - Fun√ß√£o de Perda (Categorical Cross-Entropy)')\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Acur√°cia (Treino)')\n",
    "    plt.plot(history.history['val_accuracy'], label='Acur√°cia (Valida√ß√£o)')\n",
    "    plt.title(title + ' - Acur√°cia')\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('Acur√°cia')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"‚úÖ Gr√°fico de m√©tricas salvo em: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def train_and_save(model: tf.keras.Model, train_gen: tf.keras.preprocessing.image.ImageDataGenerator, val_gen: tf.keras.preprocessing.image.ImageDataGenerator, epochs: int, checkpoint_path: str, stage_name: str, early_stop_patience: int = 6, strategy: Optional[tf.distribute.Strategy] = None) -> Tuple[tf.keras.callbacks.History, str]:\n",
    "    \"\"\"Treina `model`, salva melhor checkpoint, e salva o hist√≥rico/gr√°fico.\"\"\"\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        EarlyStopping(monitor='val_loss', patience=early_stop_patience, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    # üåü Tratamento de escopo para `tf.distribute`\n",
    "    if strategy_scope_used and strategy:\n",
    "        with strategy.scope():\n",
    "            history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=callbacks, verbose=1)\n",
    "    else:\n",
    "        history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=callbacks, verbose=1)\n",
    "    \n",
    "    # --- Salvamento Final --\n",
    "    final_path = os.path.join(str(MODELS_DIR), os.path.basename(checkpoint_path).replace('.keras', '_final.keras'))\n",
    "    \n",
    "    try:\n",
    "        # **IMPORTANTE:** Usa `quantize_scope()` para carregar checkpoints QAT\n",
    "        # Usa `compile=False` ao carregar o modelo Keras para evitar recria√ß√£o do otimizador\n",
    "        best = load_model(checkpoint_path, custom_objects=quantize_scope(), compile=False) \n",
    "        best.save(final_path)\n",
    "    except Exception as e:\n",
    "        print(f'Aviso: n√£o foi poss√≠vel recarregar checkpoint para salvar final: {e}')\n",
    "        model.save(final_path) \n",
    "    \n",
    "    # üíæ NOVO: Salva o hist√≥rico e o gr√°fico\n",
    "    save_history(history, stage_name, checkpoint_path)\n",
    "\n",
    "    return history, final_path\n",
    "\n",
    "def convert_to_lite(model: tf.keras.Model, path: Path):\n",
    "    \"\"\"Converte o modelo QAT (j√° treinado) para TFLite INT8 (quantiza√ß√£o total).\"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    # Aplicar otimiza√ß√£o de quantiza√ß√£o (essencial para INT8)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    # Define o conjunto de opera√ß√µes alvo como INT8\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n",
    "    ]\n",
    "    \n",
    "    # Define o tipo de entrada e sa√≠da do TFLite como INT8\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    \n",
    "    tflite_model = converter.convert()\n",
    "    path.write_bytes(tflite_model)\n",
    "    print(f\"‚úÖ Modelo TFLite INT8 Quantizado salvo em: {path}\")\n",
    "\n",
    "# --- Geradores de Dados Iniciais ---\n",
    "train_gen, val_gen, test_gen_baseline = make_generators(train_df, val_df, test_df, image_root=IMAGE_ROOT, augment=False, batch_size=BATCH_SIZE)\n",
    "print('\\nClasses detectadas:', train_gen.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053d620",
   "metadata": {},
   "source": [
    "## 2.  **Etapa 1: Treinamento Baseline (Base Congelada)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0302b60",
   "metadata": {},
   "source": [
    "Treinamento inicial apenas do topo (camadas densas), com pesos congelados da MobileNetV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e284117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Constru√ß√£o e Treinamento do Modelo FP32 (Baseline) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:13:17.218322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  864         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1280)        0           ['out_relu[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1280)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          163968      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            387         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,422,339\n",
      "Trainable params: 164,355\n",
      "Non-trainable params: 2,257,984\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "  2/780 [..............................] - ETA: 5:57 - loss: 1.4714 - accuracy: 0.5000 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m baseline_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# üåü Salva o melhor checkpoint e o hist√≥rico na pasta metrics/etapa_1\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m history_baseline, baseline_final_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS_BASELINE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_BASELINE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstage_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43metapa_1_baseline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline final salvo em:\u001b[39m\u001b[38;5;124m'\u001b[39m, baseline_final_path)\n",
      "Cell \u001b[0;32mIn[3], line 120\u001b[0m, in \u001b[0;36mtrain_and_save\u001b[0;34m(model, train_gen, val_gen, epochs, checkpoint_path, stage_name, early_stop_patience, strategy)\u001b[0m\n\u001b[1;32m    118\u001b[0m         history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_gen, validation_data\u001b[38;5;241m=\u001b[39mval_gen, epochs\u001b[38;5;241m=\u001b[39mepochs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# --- Salvamento Final --\u001b[39;00m\n\u001b[1;32m    123\u001b[0m final_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(MODELS_DIR), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(checkpoint_path)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_final.keras\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n--- Constru√ß√£o e Treinamento do Modelo FP32 (Baseline) ---')\n",
    "if strategy_scope_used:\n",
    "    with strategy.scope():\n",
    "        baseline_model, _ = build_model(input_shape=INPUT_SHAPE, num_classes=num_classes, base_trainable=False, learning_rate=LR_BASELINE)\n",
    "else:\n",
    "    baseline_model, _ = build_model(input_shape=INPUT_SHAPE, num_classes=num_classes, base_trainable=False, learning_rate=LR_BASELINE)\n",
    "\n",
    "baseline_model.summary()\n",
    "\n",
    "# üåü Salva o melhor checkpoint e o hist√≥rico na pasta metrics/etapa_1\n",
    "history_baseline, baseline_final_path = train_and_save(\n",
    "    baseline_model, \n",
    "    train_gen, \n",
    "    val_gen, \n",
    "    epochs=EPOCHS_BASELINE, \n",
    "    checkpoint_path=CHECKPOINT_BASELINE,\n",
    "    stage_name='etapa_1_baseline',\n",
    "    strategy=strategy\n",
    ")\n",
    "\n",
    "print('Baseline final salvo em:', baseline_final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e1a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: tf.keras.Model, test_gen: Any, title: str, stage_name: str) -> Tuple[np.ndarray, np.ndarray, list]:\n",
    "    \"\"\"\n",
    "    Calcula e exibe Matriz de Confus√£o e Relat√≥rio de Classifica√ß√£o,\n",
    "    salvando a imagem da matriz na pasta de m√©tricas.\n",
    "    \n",
    "    Anota√ß√£o de tipo alterada de DataFrameIterator para Any para compatibilidade com vers√µes recentes do TF/Keras.\n",
    "    \"\"\"\n",
    "    stage_dir = METRICS_DIR / stage_name\n",
    "    os.makedirs(stage_dir, exist_ok=True)\n",
    "    \n",
    "    test_gen.reset()\n",
    "    \n",
    "    # 1. Previs√µes\n",
    "    print(f'\\nAvaliando {title} no conjunto de teste...')\n",
    "    \n",
    "    # Define steps usando .n (samples)\n",
    "    steps = test_gen.n\n",
    "    \n",
    "    # Previs√µes: o modelo QAT/FP32 roda a infer√™ncia em Keras\n",
    "    y_pred_probs = model.predict(test_gen, steps=steps, verbose=1)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    # Labels verdadeiros\n",
    "    y_true = test_gen.classes\n",
    "    labels = list(test_gen.class_indices.keys())\n",
    "\n",
    "    # 2. Matriz de Confus√£o\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.title(f'Matriz de Confus√£o - {title}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # üíæ Salva o gr√°fico da Matriz de Confus√£o\n",
    "    matrix_path = stage_dir / f\"{title.lower().replace(' ', '_').replace(':', '')}_confusion_matrix.png\"\n",
    "    plt.savefig(matrix_path)\n",
    "    print(f\"‚úÖ Matriz de Confus√£o salva em: {matrix_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Relat√≥rio de Classifica√ß√£o\n",
    "    print(f'\\nRelat√≥rio de Classifica√ß√£o - {title}:')\n",
    "    print(classification_report(y_true, y_pred, target_names=labels))\n",
    "    \n",
    "    return y_true, y_pred, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3e7c0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando Baseline (Teste) no conjunto de teste...\n",
      "5347/5347 [==============================] - 102s 19ms/step\n",
      "‚úÖ Matriz de Confus√£o salva em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/metrics/etapa_1_baseline/baseline_(teste)_confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relat√≥rio de Classifica√ß√£o - Baseline (Teste):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       HISIL       0.97      0.98      0.97      1350\n",
      "       LISIL       0.91      0.91      0.91      1362\n",
      "      Normal       0.95      0.95      0.95      2635\n",
      "\n",
      "    accuracy                           0.95      5347\n",
      "   macro avg       0.94      0.94      0.94      5347\n",
      "weighted avg       0.95      0.95      0.95      5347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Avalia√ß√£o e Matriz de Confus√£o (Baseline) ---\n",
    "try:\n",
    "    model_baseline_eval = load_model(CHECKPOINT_BASELINE, compile=False)\n",
    "    evaluate_model(model_baseline_eval, test_gen_baseline, 'Baseline (Teste)', 'etapa_1_baseline')\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Aviso: N√£o foi poss√≠vel carregar/avaliar o modelo baseline. Erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3eddd",
   "metadata": {},
   "source": [
    "## 3. **Etapa 2: Treinamento Robusto (com Augmentation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "620928bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Treinamento Robusto (Busca por LR) ---\n",
      "Found 24949 validated image filenames belonging to 3 classes.\n",
      "Found 5347 validated image filenames belonging to 3 classes.\n",
      "Found 5347 validated image filenames belonging to 3 classes.\n",
      "\n",
      "--- Treinando candidato lr=0.001 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ampliar/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/keras/preprocessing/image.py:1139: UserWarning: Found 2 invalid image filename(s) in x_col=\"image_path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 17:54:24.323958: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_189675\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:318\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - ETA: 0s - loss: 0.4156 - accuracy: 0.8272"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 18:02:40.515098: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_198688\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:340\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.22892, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 578s 738ms/step - loss: 0.4156 - accuracy: 0.8272 - val_loss: 0.2289 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2886 - accuracy: 0.8824\n",
      "Epoch 2: val_loss improved from 0.22892 to 0.18341, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 571s 731ms/step - loss: 0.2886 - accuracy: 0.8824 - val_loss: 0.1834 - val_accuracy: 0.9282 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.8951\n",
      "Epoch 3: val_loss improved from 0.18341 to 0.16792, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 570s 730ms/step - loss: 0.2584 - accuracy: 0.8951 - val_loss: 0.1679 - val_accuracy: 0.9287 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9050\n",
      "Epoch 4: val_loss improved from 0.16792 to 0.15140, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 572s 733ms/step - loss: 0.2349 - accuracy: 0.9050 - val_loss: 0.1514 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2271 - accuracy: 0.9086\n",
      "Epoch 5: val_loss improved from 0.15140 to 0.15126, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 572s 733ms/step - loss: 0.2271 - accuracy: 0.9086 - val_loss: 0.1513 - val_accuracy: 0.9351 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9119\n",
      "Epoch 6: val_loss did not improve from 0.15126\n",
      "780/780 [==============================] - 570s 730ms/step - loss: 0.2196 - accuracy: 0.9119 - val_loss: 0.1593 - val_accuracy: 0.9347 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2112 - accuracy: 0.9157\n",
      "Epoch 7: val_loss improved from 0.15126 to 0.14120, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 570s 731ms/step - loss: 0.2112 - accuracy: 0.9157 - val_loss: 0.1412 - val_accuracy: 0.9416 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2025 - accuracy: 0.9161\n",
      "Epoch 8: val_loss did not improve from 0.14120\n",
      "780/780 [==============================] - 570s 730ms/step - loss: 0.2025 - accuracy: 0.9161 - val_loss: 0.1559 - val_accuracy: 0.9385 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1969 - accuracy: 0.9220\n",
      "Epoch 9: val_loss did not improve from 0.14120\n",
      "780/780 [==============================] - 568s 728ms/step - loss: 0.1969 - accuracy: 0.9220 - val_loss: 0.1496 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.9234\n",
      "Epoch 10: val_loss improved from 0.14120 to 0.13602, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 569s 729ms/step - loss: 0.1963 - accuracy: 0.9234 - val_loss: 0.1360 - val_accuracy: 0.9445 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1817 - accuracy: 0.9281\n",
      "Epoch 11: val_loss improved from 0.13602 to 0.12300, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 567s 727ms/step - loss: 0.1817 - accuracy: 0.9281 - val_loss: 0.1230 - val_accuracy: 0.9546 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9269\n",
      "Epoch 12: val_loss improved from 0.12300 to 0.11884, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 570s 731ms/step - loss: 0.1862 - accuracy: 0.9269 - val_loss: 0.1188 - val_accuracy: 0.9532 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.9317\n",
      "Epoch 13: val_loss did not improve from 0.11884\n",
      "780/780 [==============================] - 564s 723ms/step - loss: 0.1767 - accuracy: 0.9317 - val_loss: 0.1264 - val_accuracy: 0.9510 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1772 - accuracy: 0.9301\n",
      "Epoch 14: val_loss improved from 0.11884 to 0.11265, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 568s 728ms/step - loss: 0.1772 - accuracy: 0.9301 - val_loss: 0.1126 - val_accuracy: 0.9585 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1752 - accuracy: 0.9306\n",
      "Epoch 15: val_loss improved from 0.11265 to 0.11165, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 569s 729ms/step - loss: 0.1752 - accuracy: 0.9306 - val_loss: 0.1117 - val_accuracy: 0.9540 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9349\n",
      "Epoch 16: val_loss did not improve from 0.11165\n",
      "780/780 [==============================] - 570s 730ms/step - loss: 0.1705 - accuracy: 0.9349 - val_loss: 0.1142 - val_accuracy: 0.9546 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9343\n",
      "Epoch 17: val_loss did not improve from 0.11165\n",
      "780/780 [==============================] - 566s 726ms/step - loss: 0.1682 - accuracy: 0.9343 - val_loss: 0.1332 - val_accuracy: 0.9446 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1663 - accuracy: 0.9347\n",
      "Epoch 18: val_loss improved from 0.11165 to 0.10783, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001.keras\n",
      "780/780 [==============================] - 569s 730ms/step - loss: 0.1663 - accuracy: 0.9347 - val_loss: 0.1078 - val_accuracy: 0.9611 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9375\n",
      "Epoch 19: val_loss did not improve from 0.10783\n",
      "780/780 [==============================] - 568s 728ms/step - loss: 0.1632 - accuracy: 0.9375 - val_loss: 0.1092 - val_accuracy: 0.9575 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9376\n",
      "Epoch 20: val_loss did not improve from 0.10783\n",
      "780/780 [==============================] - 568s 728ms/step - loss: 0.1615 - accuracy: 0.9376 - val_loss: 0.1227 - val_accuracy: 0.9503 - lr: 0.0010\n",
      "Aviso: n√£o foi poss√≠vel recarregar checkpoint para salvar final: 'CustomObjectScope' object is not iterable\n",
      "‚úÖ Hist√≥rico de treinamento salvo em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/metrics/etapa_2_robust_lr0p001/robust_checkpoint_lr0p001_history.json\n",
      "Falha ao carregar ou avaliar checkpoint: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.\n",
      "lr=0.001 -> val_acc=0.9611\n",
      "\n",
      "--- Treinando candidato lr=0.0001 ---\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 21:04:18.061960: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_331058\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:742\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - ETA: 0s - loss: 0.6653 - accuracy: 0.7117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 21:12:27.953483: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_340071\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:764\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.37959, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 574s 732ms/step - loss: 0.6653 - accuracy: 0.7117 - val_loss: 0.3796 - val_accuracy: 0.8515 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.4340 - accuracy: 0.8174\n",
      "Epoch 2: val_loss improved from 0.37959 to 0.28996, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 572s 733ms/step - loss: 0.4340 - accuracy: 0.8174 - val_loss: 0.2900 - val_accuracy: 0.8945 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.3761 - accuracy: 0.8446\n",
      "Epoch 3: val_loss improved from 0.28996 to 0.26784, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 575s 737ms/step - loss: 0.3761 - accuracy: 0.8446 - val_loss: 0.2678 - val_accuracy: 0.9048 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.8616\n",
      "Epoch 4: val_loss improved from 0.26784 to 0.22949, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 572s 734ms/step - loss: 0.3381 - accuracy: 0.8616 - val_loss: 0.2295 - val_accuracy: 0.9183 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.3155 - accuracy: 0.8737\n",
      "Epoch 5: val_loss improved from 0.22949 to 0.22781, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 573s 734ms/step - loss: 0.3155 - accuracy: 0.8737 - val_loss: 0.2278 - val_accuracy: 0.9127 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2978 - accuracy: 0.8794\n",
      "Epoch 6: val_loss improved from 0.22781 to 0.21413, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 571s 732ms/step - loss: 0.2978 - accuracy: 0.8794 - val_loss: 0.2141 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.8829\n",
      "Epoch 7: val_loss improved from 0.21413 to 0.19771, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 571s 732ms/step - loss: 0.2866 - accuracy: 0.8829 - val_loss: 0.1977 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.8895\n",
      "Epoch 8: val_loss improved from 0.19771 to 0.18934, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 580s 743ms/step - loss: 0.2703 - accuracy: 0.8895 - val_loss: 0.1893 - val_accuracy: 0.9272 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.8943\n",
      "Epoch 9: val_loss did not improve from 0.18934\n",
      "780/780 [==============================] - 599s 767ms/step - loss: 0.2642 - accuracy: 0.8943 - val_loss: 0.1908 - val_accuracy: 0.9237 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.8988\n",
      "Epoch 10: val_loss improved from 0.18934 to 0.18661, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 584s 748ms/step - loss: 0.2545 - accuracy: 0.8988 - val_loss: 0.1866 - val_accuracy: 0.9267 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.9012\n",
      "Epoch 11: val_loss improved from 0.18661 to 0.17622, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 582s 745ms/step - loss: 0.2480 - accuracy: 0.9012 - val_loss: 0.1762 - val_accuracy: 0.9301 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.9054\n",
      "Epoch 12: val_loss did not improve from 0.17622\n",
      "780/780 [==============================] - 554s 710ms/step - loss: 0.2385 - accuracy: 0.9054 - val_loss: 0.1778 - val_accuracy: 0.9319 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2354 - accuracy: 0.9054\n",
      "Epoch 13: val_loss improved from 0.17622 to 0.16304, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 547s 701ms/step - loss: 0.2354 - accuracy: 0.9054 - val_loss: 0.1630 - val_accuracy: 0.9357 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9119\n",
      "Epoch 14: val_loss did not improve from 0.16304\n",
      "780/780 [==============================] - 548s 703ms/step - loss: 0.2236 - accuracy: 0.9119 - val_loss: 0.1651 - val_accuracy: 0.9344 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2213 - accuracy: 0.9120\n",
      "Epoch 15: val_loss did not improve from 0.16304\n",
      "780/780 [==============================] - 546s 700ms/step - loss: 0.2213 - accuracy: 0.9120 - val_loss: 0.1645 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2190 - accuracy: 0.9126\n",
      "Epoch 16: val_loss improved from 0.16304 to 0.16198, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 552s 708ms/step - loss: 0.2190 - accuracy: 0.9126 - val_loss: 0.1620 - val_accuracy: 0.9351 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2150 - accuracy: 0.9138\n",
      "Epoch 17: val_loss improved from 0.16198 to 0.15142, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p0001.keras\n",
      "780/780 [==============================] - 564s 723ms/step - loss: 0.2150 - accuracy: 0.9138 - val_loss: 0.1514 - val_accuracy: 0.9402 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.9174\n",
      "Epoch 18: val_loss did not improve from 0.15142\n",
      "780/780 [==============================] - 564s 723ms/step - loss: 0.2099 - accuracy: 0.9174 - val_loss: 0.1590 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9159\n",
      "Epoch 19: val_loss did not improve from 0.15142\n",
      "780/780 [==============================] - 567s 726ms/step - loss: 0.2106 - accuracy: 0.9159 - val_loss: 0.1605 - val_accuracy: 0.9368 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2036 - accuracy: 0.9184\n",
      "Epoch 20: val_loss did not improve from 0.15142\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "780/780 [==============================] - 563s 721ms/step - loss: 0.2036 - accuracy: 0.9184 - val_loss: 0.1616 - val_accuracy: 0.9362 - lr: 1.0000e-04\n",
      "Aviso: n√£o foi poss√≠vel recarregar checkpoint para salvar final: 'CustomObjectScope' object is not iterable\n",
      "‚úÖ Hist√≥rico de treinamento salvo em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/metrics/etapa_2_robust_lr0p0001/robust_checkpoint_lr0p0001_history.json\n",
      "Falha ao carregar ou avaliar checkpoint: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.\n",
      "lr=0.0001 -> val_acc=0.9402\n",
      "\n",
      "--- Treinando candidato lr=5e-05 ---\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 00:13:39.169724: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_474481\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:1166\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780 [==============================] - ETA: 0s - loss: 0.7886 - accuracy: 0.6591"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 00:21:43.860246: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_483494\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:1188\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.42118, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 567s 723ms/step - loss: 0.7886 - accuracy: 0.6591 - val_loss: 0.4212 - val_accuracy: 0.8354 - lr: 5.0000e-05\n",
      "Epoch 2/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.5143 - accuracy: 0.7810\n",
      "Epoch 2: val_loss improved from 0.42118 to 0.33319, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 563s 723ms/step - loss: 0.5143 - accuracy: 0.7810 - val_loss: 0.3332 - val_accuracy: 0.8764 - lr: 5.0000e-05\n",
      "Epoch 3/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.8142\n",
      "Epoch 3: val_loss improved from 0.33319 to 0.30689, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 562s 720ms/step - loss: 0.4494 - accuracy: 0.8142 - val_loss: 0.3069 - val_accuracy: 0.8846 - lr: 5.0000e-05\n",
      "Epoch 4/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.4092 - accuracy: 0.8291\n",
      "Epoch 4: val_loss improved from 0.30689 to 0.27977, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 562s 720ms/step - loss: 0.4092 - accuracy: 0.8291 - val_loss: 0.2798 - val_accuracy: 0.8958 - lr: 5.0000e-05\n",
      "Epoch 5/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.3865 - accuracy: 0.8392\n",
      "Epoch 5: val_loss improved from 0.27977 to 0.26461, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 563s 722ms/step - loss: 0.3865 - accuracy: 0.8392 - val_loss: 0.2646 - val_accuracy: 0.9024 - lr: 5.0000e-05\n",
      "Epoch 6/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8502\n",
      "Epoch 6: val_loss improved from 0.26461 to 0.24766, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 564s 722ms/step - loss: 0.3602 - accuracy: 0.8502 - val_loss: 0.2477 - val_accuracy: 0.9089 - lr: 5.0000e-05\n",
      "Epoch 7/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.3438 - accuracy: 0.8577\n",
      "Epoch 7: val_loss improved from 0.24766 to 0.23956, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 562s 721ms/step - loss: 0.3438 - accuracy: 0.8577 - val_loss: 0.2396 - val_accuracy: 0.9100 - lr: 5.0000e-05\n",
      "Epoch 8/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.3251 - accuracy: 0.8676\n",
      "Epoch 8: val_loss improved from 0.23956 to 0.22654, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 564s 723ms/step - loss: 0.3251 - accuracy: 0.8676 - val_loss: 0.2265 - val_accuracy: 0.9171 - lr: 5.0000e-05\n",
      "Epoch 9/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.3181 - accuracy: 0.8707\n",
      "Epoch 9: val_loss improved from 0.22654 to 0.22065, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 561s 719ms/step - loss: 0.3181 - accuracy: 0.8707 - val_loss: 0.2207 - val_accuracy: 0.9190 - lr: 5.0000e-05\n",
      "Epoch 10/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.8729\n",
      "Epoch 10: val_loss did not improve from 0.22065\n",
      "780/780 [==============================] - 563s 722ms/step - loss: 0.3094 - accuracy: 0.8729 - val_loss: 0.2268 - val_accuracy: 0.9153 - lr: 5.0000e-05\n",
      "Epoch 11/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2944 - accuracy: 0.8794\n",
      "Epoch 11: val_loss improved from 0.22065 to 0.20981, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 563s 722ms/step - loss: 0.2944 - accuracy: 0.8794 - val_loss: 0.2098 - val_accuracy: 0.9222 - lr: 5.0000e-05\n",
      "Epoch 12/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.8802\n",
      "Epoch 12: val_loss improved from 0.20981 to 0.20191, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 563s 722ms/step - loss: 0.2938 - accuracy: 0.8802 - val_loss: 0.2019 - val_accuracy: 0.9243 - lr: 5.0000e-05\n",
      "Epoch 13/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2833 - accuracy: 0.8844\n",
      "Epoch 13: val_loss improved from 0.20191 to 0.19431, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 561s 720ms/step - loss: 0.2833 - accuracy: 0.8844 - val_loss: 0.1943 - val_accuracy: 0.9299 - lr: 5.0000e-05\n",
      "Epoch 14/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.8877\n",
      "Epoch 14: val_loss did not improve from 0.19431\n",
      "780/780 [==============================] - 564s 723ms/step - loss: 0.2736 - accuracy: 0.8877 - val_loss: 0.1979 - val_accuracy: 0.9254 - lr: 5.0000e-05\n",
      "Epoch 15/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.8895\n",
      "Epoch 15: val_loss improved from 0.19431 to 0.19381, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 562s 720ms/step - loss: 0.2685 - accuracy: 0.8895 - val_loss: 0.1938 - val_accuracy: 0.9274 - lr: 5.0000e-05\n",
      "Epoch 16/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2643 - accuracy: 0.8939\n",
      "Epoch 16: val_loss improved from 0.19381 to 0.18316, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 562s 720ms/step - loss: 0.2643 - accuracy: 0.8939 - val_loss: 0.1832 - val_accuracy: 0.9310 - lr: 5.0000e-05\n",
      "Epoch 17/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.8954\n",
      "Epoch 17: val_loss improved from 0.18316 to 0.18316, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 562s 721ms/step - loss: 0.2594 - accuracy: 0.8954 - val_loss: 0.1832 - val_accuracy: 0.9323 - lr: 5.0000e-05\n",
      "Epoch 18/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.8978\n",
      "Epoch 18: val_loss improved from 0.18316 to 0.18053, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 566s 725ms/step - loss: 0.2533 - accuracy: 0.8978 - val_loss: 0.1805 - val_accuracy: 0.9310 - lr: 5.0000e-05\n",
      "Epoch 19/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.9004\n",
      "Epoch 19: val_loss improved from 0.18053 to 0.17505, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 566s 726ms/step - loss: 0.2467 - accuracy: 0.9004 - val_loss: 0.1750 - val_accuracy: 0.9338 - lr: 5.0000e-05\n",
      "Epoch 20/20\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9016\n",
      "Epoch 20: val_loss improved from 0.17505 to 0.17011, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr5e-05.keras\n",
      "780/780 [==============================] - 564s 723ms/step - loss: 0.2449 - accuracy: 0.9016 - val_loss: 0.1701 - val_accuracy: 0.9349 - lr: 5.0000e-05\n",
      "Aviso: n√£o foi poss√≠vel recarregar checkpoint para salvar final: 'CustomObjectScope' object is not iterable\n",
      "‚úÖ Hist√≥rico de treinamento salvo em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/metrics/etapa_2_robust_lr5e-05/robust_checkpoint_lr5e-05_history.json\n",
      "Falha ao carregar ou avaliar checkpoint: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.\n",
      "lr=5e-05 -> val_acc=0.9349\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "\n",
      "‚úÖ Modelo Robusto final (melhor LR) salvo em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_best.keras com val_acc=0.9611\n"
     ]
    }
   ],
   "source": [
    "print('\\n--- Treinamento Robusto (Busca por LR) ---')\n",
    "best_val_acc = -1.0\n",
    "best_model_path_robust = CHECKPOINT_BASELINE # Fallback inicial\n",
    "\n",
    "# Geradores com augmentation\n",
    "train_gen_aug, val_gen_aug, _ = make_generators(train_df, val_df, test_df, image_root=IMAGE_ROOT, augment=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "for lr in LR_ROBUST_CANDIDATES:\n",
    "    print(f'\\n--- Treinando candidato lr={lr} ---')\n",
    "    checkpoint_path = str(MODELS_DIR / f\"robust_checkpoint_lr{str(lr).replace('.', 'p')}.keras\")\n",
    "\n",
    "    # Reconstruir modelo (com pesos do baseline ou MobileNetV2)\n",
    "    if strategy_scope_used:\n",
    "        with strategy.scope():\n",
    "            # Modelo sem pesos pr√©-carregados, inicia do MobileNetV2/ImageNet\n",
    "            model_candidate, _ = build_model(input_shape=INPUT_SHAPE, num_classes=num_classes, base_trainable=False, learning_rate=lr)\n",
    "            \n",
    "            # Carregar pesos do checkpoint baseline se desejar continuar o treino,\n",
    "            # mas para uma busca limpa, muitas vezes se treina do zero/ImageNet.\n",
    "            # Aqui mantemos a l√≥gica de treino do topo.\n",
    "    else:\n",
    "        model_candidate, _ = build_model(input_shape=INPUT_SHAPE, num_classes=num_classes, base_trainable=False, learning_rate=lr)\n",
    "\n",
    "\n",
    "    history, current_final_path = train_and_save(\n",
    "        model_candidate, \n",
    "        train_gen_aug, \n",
    "        val_gen_aug, \n",
    "        epochs=EPOCHS_ROBUST, \n",
    "        checkpoint_path=checkpoint_path, \n",
    "        stage_name=f\"etapa_2_robust_lr{str(lr).replace('.', 'p')}\",\n",
    "        early_stop_patience=8,\n",
    "        strategy=strategy\n",
    "    )\n",
    "\n",
    "    # Avalia o melhor checkpoint\n",
    "    try:\n",
    "        loaded = load_model(checkpoint_path, compile=False)\n",
    "        val_gen_aug.reset()\n",
    "        res = loaded.evaluate(val_gen_aug, verbose=0)\n",
    "        val_acc = res[1] if len(res) > 1 else -1\n",
    "    except Exception as e:\n",
    "        print('Falha ao carregar ou avaliar checkpoint:', e)\n",
    "        val_acc = max(history.history.get('val_accuracy', [-1]))\n",
    "\n",
    "    print(f'lr={lr} -> val_acc={val_acc:.4f}')\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_path_robust = checkpoint_path\n",
    "\n",
    "# Salvar c√≥pia nomeada do melhor modelo robusto\n",
    "if best_model_path_robust != CHECKPOINT_BASELINE:\n",
    "    try:\n",
    "        robust_best = load_model(best_model_path_robust, compile=False)\n",
    "        robust_best.save(CHECKPOINT_ROBUST)\n",
    "        print(f'\\n‚úÖ Modelo Robusto final (melhor LR) salvo em: {CHECKPOINT_ROBUST} com val_acc={best_val_acc:.4f}')\n",
    "    except Exception as e:\n",
    "        print(f'Falha ao salvar o melhor modelo robusto: {e}')\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è Nenhuma melhora na etapa 2. Usando o baseline como melhor modelo robusto.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28878b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando Robusto (Teste) no conjunto de teste...\n",
      "5347/5347 [==============================] - 99s 18ms/step\n",
      "‚úÖ Matriz de Confus√£o salva em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/metrics/etapa_2_robust/robusto_(teste)_confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relat√≥rio de Classifica√ß√£o - Robusto (Teste):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       HISIL       0.98      0.96      0.97      1350\n",
      "       LISIL       0.89      0.95      0.92      1362\n",
      "      Normal       0.97      0.95      0.96      2635\n",
      "\n",
      "    accuracy                           0.95      5347\n",
      "   macro avg       0.95      0.95      0.95      5347\n",
      "weighted avg       0.95      0.95      0.95      5347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Avalia√ß√£o e Matriz de Confus√£o (Robusto) ---\n",
    "try:\n",
    "    model_robust_eval = load_model(CHECKPOINT_ROBUST, compile=False)\n",
    "    evaluate_model(model_robust_eval, test_gen_baseline, 'Robusto (Teste)', 'etapa_2_robust')\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Aviso: N√£o foi poss√≠vel carregar/avaliar o modelo robusto. Erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfacff",
   "metadata": {},
   "source": [
    "## 4. **Etapa 3: Fine-Tuning FP32 (Ajuste Fino da Base)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7c935",
   "metadata": {},
   "source": [
    "Descongelamos as camadas superiores da MobileNetV2 e ajustamos a rede base (em FP32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c9dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prepara√ß√£o para Fine-Tuning FP32 ---\n",
      "Carregado robust_best.keras para fine-tuning\n",
      "Tornadas trein√°veis as 30 √∫ltimas camadas da MobileNetV2.\n",
      "Found 24949 validated image filenames belonging to 3 classes.\n",
      "Found 5347 validated image filenames belonging to 3 classes.\n",
      "Found 5347 validated image filenames belonging to 3 classes.\n",
      "\n",
      "--- Treinamento Fine-Tuning FP32 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ampliar/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/keras/preprocessing/image.py:1139: UserWarning: Found 2 invalid image filename(s) in x_col=\"image_path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.4580 - accuracy: 0.8375\n",
      "Epoch 1: val_loss improved from inf to 0.14961, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/final_finetuned_checkpoint.keras\n",
      "780/780 [==============================] - 656s 839ms/step - loss: 0.4580 - accuracy: 0.8375 - val_loss: 0.1496 - val_accuracy: 0.9413 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.8957\n",
      "Epoch 2: val_loss improved from 0.14961 to 0.14332, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/final_finetuned_checkpoint.keras\n",
      "780/780 [==============================] - 637s 817ms/step - loss: 0.2635 - accuracy: 0.8957 - val_loss: 0.1433 - val_accuracy: 0.9413 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2242 - accuracy: 0.9119\n",
      "Epoch 3: val_loss did not improve from 0.14332\n",
      "780/780 [==============================] - 628s 805ms/step - loss: 0.2242 - accuracy: 0.9119 - val_loss: 0.1450 - val_accuracy: 0.9422 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9238\n",
      "Epoch 4: val_loss improved from 0.14332 to 0.12160, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/final_finetuned_checkpoint.keras\n",
      "780/780 [==============================] - 630s 808ms/step - loss: 0.1923 - accuracy: 0.9238 - val_loss: 0.1216 - val_accuracy: 0.9491 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1748 - accuracy: 0.9311\n",
      "Epoch 5: val_loss improved from 0.12160 to 0.11834, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/final_finetuned_checkpoint.keras\n",
      "780/780 [==============================] - 630s 808ms/step - loss: 0.1748 - accuracy: 0.9311 - val_loss: 0.1183 - val_accuracy: 0.9527 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9398\n",
      "Epoch 6: val_loss improved from 0.11834 to 0.09797, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/final_finetuned_checkpoint.keras\n",
      "780/780 [==============================] - 631s 808ms/step - loss: 0.1596 - accuracy: 0.9398 - val_loss: 0.0980 - val_accuracy: 0.9607 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9447\n",
      "Epoch 7: val_loss improved from 0.09797 to 0.09026, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/final_finetuned_checkpoint.keras\n",
      "780/780 [==============================] - 633s 812ms/step - loss: 0.1473 - accuracy: 0.9447 - val_loss: 0.0903 - val_accuracy: 0.9633 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9461\n",
      "Epoch 8: val_loss did not improve from 0.09026\n",
      "780/780 [==============================] - 627s 804ms/step - loss: 0.1403 - accuracy: 0.9461 - val_loss: 0.0949 - val_accuracy: 0.9632 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.9506\n",
      "Epoch 9: val_loss improved from 0.09026 to 0.08513, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/final_finetuned_checkpoint.keras\n",
      "780/780 [==============================] - 632s 810ms/step - loss: 0.1320 - accuracy: 0.9506 - val_loss: 0.0851 - val_accuracy: 0.9661 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.9538\n",
      "Epoch 10: val_loss did not improve from 0.08513\n",
      "780/780 [==============================] - 631s 809ms/step - loss: 0.1182 - accuracy: 0.9538 - val_loss: 0.0870 - val_accuracy: 0.9673 - lr: 1.0000e-05\n",
      "Aviso: n√£o foi poss√≠vel recarregar checkpoint para salvar final: 'CustomObjectScope' object is not iterable\n",
      "‚úÖ Hist√≥rico de treinamento salvo em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/metrics/etapa_3_finetune_fp32/final_finetuned_checkpoint_history.json\n",
      "Fine-tuning FP32 final salvo em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/final_finetuned_checkpoint_final.keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n--- Prepara√ß√£o para Fine-Tuning FP32 ---')\n",
    "# Carregar o melhor modelo da Etapa 2\n",
    "if os.path.exists(CHECKPOINT_ROBUST):\n",
    "    model_ft = load_model(CHECKPOINT_ROBUST, compile=False)\n",
    "    print(f'Carregado {Path(CHECKPOINT_ROBUST).name} para fine-tuning')\n",
    "else:\n",
    "    raise FileNotFoundError('Nenhum modelo robusto encontrado para fine-tuning.')\n",
    "\n",
    "# --- Estrat√©gia de Fine-Tuning ---\n",
    "UNFREEZE_LAST_N = 30 \n",
    "total_layers = len(model_ft.layers)\n",
    "base_model_layer = model_ft.get_layer(index=0) if isinstance(model_ft.layers[0], Model) else model_ft\n",
    "total_base_layers = len(base_model_layer.layers)\n",
    "start_idx = max(0, total_base_layers - UNFREEZE_LAST_N)\n",
    "\n",
    "# Descongela as √∫ltimas N camadas da MobileNetV2\n",
    "for i, layer in enumerate(base_model_layer.layers):\n",
    "    if i >= start_idx:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "print(f'Tornadas trein√°veis as {total_base_layers - start_idx} √∫ltimas camadas da MobileNetV2.')\n",
    "\n",
    "# Re-compilar com LR reduzida dentro do strategy scope\n",
    "if strategy_scope_used:\n",
    "    with strategy.scope():\n",
    "        # A recompila√ß√£o √© essencial para que o otimizador reconhe√ßa as novas vari√°veis trein√°veis\n",
    "        model_ft.compile(optimizer=Adam(learning_rate=LR_FINETUNE), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "    model_ft.compile(optimizer=Adam(learning_rate=LR_FINETUNE), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Preparar geradores (mantemos augmentation)\n",
    "train_gen_ft, val_gen_ft, _ = make_generators(train_df, val_df, test_df, image_root=IMAGE_ROOT, augment=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "# üåü Treinamento de Fine-Tuning FP32\n",
    "print('\\n--- Treinamento Fine-Tuning FP32 ---')\n",
    "history_ft, final_ft_path = train_and_save(\n",
    "    model_ft, \n",
    "    train_gen_ft, \n",
    "    val_gen_ft, \n",
    "    epochs=EPOCHS_FINETUNE, \n",
    "    checkpoint_path=CHECKPOINT_FINETUNE, \n",
    "    stage_name='etapa_3_finetune_fp32',\n",
    "    early_stop_patience=5,\n",
    "    strategy=strategy\n",
    ")\n",
    "print('Fine-tuning FP32 final salvo em:', final_ft_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c43f5",
   "metadata": {},
   "source": [
    "## 5. **Etapa 4: Quantization-Aware Training (QAT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca64b0c",
   "metadata": {},
   "source": [
    "Esta √© a etapa chave. O modelo Fine-Tuned (FP32) √© encapsulado com os operadores Fake Quant e passa por um novo Fine-Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af3b520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üöÄ Prepara√ß√£o para QAT (Inser√ß√£o de Fake Quants) ---\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " quantize_layer (QuantizeLayer)  (None, 224, 224, 3)  3          ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " quant_Conv1 (QuantizeWrapperV2  (None, 112, 112, 32  929        ['quantize_layer[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " quant_bn_Conv1 (QuantizeWrappe  (None, 112, 112, 32  129        ['quant_Conv1[0][0]']            \n",
      " rV2)                           )                                                                 \n",
      "                                                                                                  \n",
      " quant_Conv1_relu (QuantizeWrap  (None, 112, 112, 32  3          ['quant_bn_Conv1[0][0]']         \n",
      " perV2)                         )                                                                 \n",
      "                                                                                                  \n",
      " quant_expanded_conv_depthwise   (None, 112, 112, 32  291        ['quant_Conv1_relu[0][0]']       \n",
      " (QuantizeWrapperV2)            )                                                                 \n",
      "                                                                                                  \n",
      " quant_expanded_conv_depthwise_  (None, 112, 112, 32  129        ['quant_expanded_conv_depthwise[0\n",
      " BN (QuantizeWrapperV2)         )                                ][0]']                           \n",
      "                                                                                                  \n",
      " quant_expanded_conv_depthwise_  (None, 112, 112, 32  3          ['quant_expanded_conv_depthwise_B\n",
      " relu (QuantizeWrapperV2)       )                                N[0][0]']                        \n",
      "                                                                                                  \n",
      " quant_expanded_conv_project (Q  (None, 112, 112, 16  545        ['quant_expanded_conv_depthwise_r\n",
      " uantizeWrapperV2)              )                                elu[0][0]']                      \n",
      "                                                                                                  \n",
      " quant_expanded_conv_project_BN  (None, 112, 112, 16  67         ['quant_expanded_conv_project[0][\n",
      "  (QuantizeWrapperV2)           )                                0]']                             \n",
      "                                                                                                  \n",
      " quant_block_1_expand (Quantize  (None, 112, 112, 96  1729       ['quant_expanded_conv_project_BN[\n",
      " WrapperV2)                     )                                0][0]']                          \n",
      "                                                                                                  \n",
      " quant_block_1_expand_BN (Quant  (None, 112, 112, 96  385        ['quant_block_1_expand[0][0]']   \n",
      " izeWrapperV2)                  )                                                                 \n",
      "                                                                                                  \n",
      " quant_block_1_expand_relu (Qua  (None, 112, 112, 96  3          ['quant_block_1_expand_BN[0][0]']\n",
      " ntizeWrapperV2)                )                                                                 \n",
      "                                                                                                  \n",
      " quant_block_1_pad (QuantizeWra  (None, 113, 113, 96  1          ['quant_block_1_expand_relu[0][0]\n",
      " pperV2)                        )                                ']                               \n",
      "                                                                                                  \n",
      " quant_block_1_depthwise (Quant  (None, 56, 56, 96)  867         ['quant_block_1_pad[0][0]']      \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_block_1_depthwise_BN (Qu  (None, 56, 56, 96)  385         ['quant_block_1_depthwise[0][0]']\n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " quant_block_1_depthwise_relu (  (None, 56, 56, 96)  3           ['quant_block_1_depthwise_BN[0][0\n",
      " QuantizeWrapperV2)                                              ]']                              \n",
      "                                                                                                  \n",
      " quant_block_1_project (Quantiz  (None, 56, 56, 24)  2353        ['quant_block_1_depthwise_relu[0]\n",
      " eWrapperV2)                                                     [0]']                            \n",
      "                                                                                                  \n",
      " quant_block_1_project_BN (Quan  (None, 56, 56, 24)  99          ['quant_block_1_project[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_2_expand (Quantize  (None, 56, 56, 144)  3745       ['quant_block_1_project_BN[0][0]'\n",
      " WrapperV2)                                                      ]                                \n",
      "                                                                                                  \n",
      " quant_block_2_expand_BN (Quant  (None, 56, 56, 144)  577        ['quant_block_2_expand[0][0]']   \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_block_2_expand_relu (Qua  (None, 56, 56, 144)  3          ['quant_block_2_expand_BN[0][0]']\n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_2_depthwise (Quant  (None, 56, 56, 144)  1299       ['quant_block_2_expand_relu[0][0]\n",
      " izeWrapperV2)                                                   ']                               \n",
      "                                                                                                  \n",
      " quant_block_2_depthwise_BN (Qu  (None, 56, 56, 144)  577        ['quant_block_2_depthwise[0][0]']\n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " quant_block_2_depthwise_relu (  (None, 56, 56, 144)  3          ['quant_block_2_depthwise_BN[0][0\n",
      " QuantizeWrapperV2)                                              ]']                              \n",
      "                                                                                                  \n",
      " quant_block_2_project (Quantiz  (None, 56, 56, 24)  3505        ['quant_block_2_depthwise_relu[0]\n",
      " eWrapperV2)                                                     [0]']                            \n",
      "                                                                                                  \n",
      " quant_block_2_project_BN (Quan  (None, 56, 56, 24)  99          ['quant_block_2_project[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_2_add (QuantizeWra  (None, 56, 56, 24)  3           ['quant_block_1_project_BN[0][0]'\n",
      " pperV2)                                                         , 'quant_block_2_project_BN[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " quant_block_3_expand (Quantize  (None, 56, 56, 144)  3745       ['quant_block_2_add[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_block_3_expand_BN (Quant  (None, 56, 56, 144)  577        ['quant_block_3_expand[0][0]']   \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_block_3_expand_relu (Qua  (None, 56, 56, 144)  3          ['quant_block_3_expand_BN[0][0]']\n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_3_pad (QuantizeWra  (None, 57, 57, 144)  1          ['quant_block_3_expand_relu[0][0]\n",
      " pperV2)                                                         ']                               \n",
      "                                                                                                  \n",
      " quant_block_3_depthwise (Quant  (None, 28, 28, 144)  1299       ['quant_block_3_pad[0][0]']      \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_block_3_depthwise_BN (Qu  (None, 28, 28, 144)  577        ['quant_block_3_depthwise[0][0]']\n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " quant_block_3_depthwise_relu (  (None, 28, 28, 144)  3          ['quant_block_3_depthwise_BN[0][0\n",
      " QuantizeWrapperV2)                                              ]']                              \n",
      "                                                                                                  \n",
      " quant_block_3_project (Quantiz  (None, 28, 28, 32)  4673        ['quant_block_3_depthwise_relu[0]\n",
      " eWrapperV2)                                                     [0]']                            \n",
      "                                                                                                  \n",
      " quant_block_3_project_BN (Quan  (None, 28, 28, 32)  131         ['quant_block_3_project[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_4_expand (Quantize  (None, 28, 28, 192)  6529       ['quant_block_3_project_BN[0][0]'\n",
      " WrapperV2)                                                      ]                                \n",
      "                                                                                                  \n",
      " quant_block_4_expand_BN (Quant  (None, 28, 28, 192)  769        ['quant_block_4_expand[0][0]']   \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_block_4_expand_relu (Qua  (None, 28, 28, 192)  3          ['quant_block_4_expand_BN[0][0]']\n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_4_depthwise (Quant  (None, 28, 28, 192)  1731       ['quant_block_4_expand_relu[0][0]\n",
      " izeWrapperV2)                                                   ']                               \n",
      "                                                                                                  \n",
      " quant_block_4_depthwise_BN (Qu  (None, 28, 28, 192)  769        ['quant_block_4_depthwise[0][0]']\n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " quant_block_4_depthwise_relu (  (None, 28, 28, 192)  3          ['quant_block_4_depthwise_BN[0][0\n",
      " QuantizeWrapperV2)                                              ]']                              \n",
      "                                                                                                  \n",
      " quant_block_4_project (Quantiz  (None, 28, 28, 32)  6209        ['quant_block_4_depthwise_relu[0]\n",
      " eWrapperV2)                                                     [0]']                            \n",
      "                                                                                                  \n",
      " quant_block_4_project_BN (Quan  (None, 28, 28, 32)  131         ['quant_block_4_project[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_4_add (QuantizeWra  (None, 28, 28, 32)  3           ['quant_block_3_project_BN[0][0]'\n",
      " pperV2)                                                         , 'quant_block_4_project_BN[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " quant_block_5_expand (Quantize  (None, 28, 28, 192)  6529       ['quant_block_4_add[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_block_5_expand_BN (Quant  (None, 28, 28, 192)  769        ['quant_block_5_expand[0][0]']   \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_block_5_expand_relu (Qua  (None, 28, 28, 192)  3          ['quant_block_5_expand_BN[0][0]']\n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_5_depthwise (Quant  (None, 28, 28, 192)  1731       ['quant_block_5_expand_relu[0][0]\n",
      " izeWrapperV2)                                                   ']                               \n",
      "                                                                                                  \n",
      " quant_block_5_depthwise_BN (Qu  (None, 28, 28, 192)  769        ['quant_block_5_depthwise[0][0]']\n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " quant_block_5_depthwise_relu (  (None, 28, 28, 192)  3          ['quant_block_5_depthwise_BN[0][0\n",
      " QuantizeWrapperV2)                                              ]']                              \n",
      "                                                                                                  \n",
      " quant_block_5_project (Quantiz  (None, 28, 28, 32)  6209        ['quant_block_5_depthwise_relu[0]\n",
      " eWrapperV2)                                                     [0]']                            \n",
      "                                                                                                  \n",
      " quant_block_5_project_BN (Quan  (None, 28, 28, 32)  131         ['quant_block_5_project[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_5_add (QuantizeWra  (None, 28, 28, 32)  3           ['quant_block_4_add[0][0]',      \n",
      " pperV2)                                                          'quant_block_5_project_BN[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " quant_block_6_expand (Quantize  (None, 28, 28, 192)  6529       ['quant_block_5_add[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_block_6_expand_BN (Quant  (None, 28, 28, 192)  769        ['quant_block_6_expand[0][0]']   \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_block_6_expand_relu (Qua  (None, 28, 28, 192)  3          ['quant_block_6_expand_BN[0][0]']\n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_6_pad (QuantizeWra  (None, 29, 29, 192)  1          ['quant_block_6_expand_relu[0][0]\n",
      " pperV2)                                                         ']                               \n",
      "                                                                                                  \n",
      " quant_block_6_depthwise (Quant  (None, 14, 14, 192)  1731       ['quant_block_6_pad[0][0]']      \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_block_6_depthwise_BN (Qu  (None, 14, 14, 192)  769        ['quant_block_6_depthwise[0][0]']\n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " quant_block_6_depthwise_relu (  (None, 14, 14, 192)  3          ['quant_block_6_depthwise_BN[0][0\n",
      " QuantizeWrapperV2)                                              ]']                              \n",
      "                                                                                                  \n",
      " quant_block_6_project (Quantiz  (None, 14, 14, 64)  12417       ['quant_block_6_depthwise_relu[0]\n",
      " eWrapperV2)                                                     [0]']                            \n",
      "                                                                                                  \n",
      " quant_block_6_project_BN (Quan  (None, 14, 14, 64)  259         ['quant_block_6_project[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_7_expand (Quantize  (None, 14, 14, 384)  25345      ['quant_block_6_project_BN[0][0]'\n",
      " WrapperV2)                                                      ]                                \n",
      "                                                                                                  \n",
      " quant_block_7_expand_BN (Quant  (None, 14, 14, 384)  1537       ['quant_block_7_expand[0][0]']   \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_block_7_expand_relu (Qua  (None, 14, 14, 384)  3          ['quant_block_7_expand_BN[0][0]']\n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_7_depthwise (Quant  (None, 14, 14, 384)  3459       ['quant_block_7_expand_relu[0][0]\n",
      " izeWrapperV2)                                                   ']                               \n",
      "                                                                                                  \n",
      " quant_block_7_depthwise_BN (Qu  (None, 14, 14, 384)  1537       ['quant_block_7_depthwise[0][0]']\n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " quant_block_7_depthwise_relu (  (None, 14, 14, 384)  3          ['quant_block_7_depthwise_BN[0][0\n",
      " QuantizeWrapperV2)                                              ]']                              \n",
      "                                                                                                  \n",
      " quant_block_7_project (Quantiz  (None, 14, 14, 64)  24705       ['quant_block_7_depthwise_relu[0]\n",
      " eWrapperV2)                                                     [0]']                            \n",
      "                                                                                                  \n",
      " quant_block_7_project_BN (Quan  (None, 14, 14, 64)  259         ['quant_block_7_project[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_7_add (QuantizeWra  (None, 14, 14, 64)  3           ['quant_block_6_project_BN[0][0]'\n",
      " pperV2)                                                         , 'quant_block_7_project_BN[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " quant_block_8_expand (Quantize  (None, 14, 14, 384)  25345      ['quant_block_7_add[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_block_8_expand_BN (Quant  (None, 14, 14, 384)  1537       ['quant_block_8_expand[0][0]']   \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_block_8_expand_relu (Qua  (None, 14, 14, 384)  3          ['quant_block_8_expand_BN[0][0]']\n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_8_depthwise (Quant  (None, 14, 14, 384)  3459       ['quant_block_8_expand_relu[0][0]\n",
      " izeWrapperV2)                                                   ']                               \n",
      "                                                                                                  \n",
      " quant_block_8_depthwise_BN (Qu  (None, 14, 14, 384)  1537       ['quant_block_8_depthwise[0][0]']\n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " quant_block_8_depthwise_relu (  (None, 14, 14, 384)  3          ['quant_block_8_depthwise_BN[0][0\n",
      " QuantizeWrapperV2)                                              ]']                              \n",
      "                                                                                                  \n",
      " quant_block_8_project (Quantiz  (None, 14, 14, 64)  24705       ['quant_block_8_depthwise_relu[0]\n",
      " eWrapperV2)                                                     [0]']                            \n",
      "                                                                                                  \n",
      " quant_block_8_project_BN (Quan  (None, 14, 14, 64)  259         ['quant_block_8_project[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_8_add (QuantizeWra  (None, 14, 14, 64)  3           ['quant_block_7_add[0][0]',      \n",
      " pperV2)                                                          'quant_block_8_project_BN[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " quant_block_9_expand (Quantize  (None, 14, 14, 384)  25345      ['quant_block_8_add[0][0]']      \n",
      " WrapperV2)                                                                                       \n",
      "                                                                                                  \n",
      " quant_block_9_expand_BN (Quant  (None, 14, 14, 384)  1537       ['quant_block_9_expand[0][0]']   \n",
      " izeWrapperV2)                                                                                    \n",
      "                                                                                                  \n",
      " quant_block_9_expand_relu (Qua  (None, 14, 14, 384)  3          ['quant_block_9_expand_BN[0][0]']\n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_9_depthwise (Quant  (None, 14, 14, 384)  3459       ['quant_block_9_expand_relu[0][0]\n",
      " izeWrapperV2)                                                   ']                               \n",
      "                                                                                                  \n",
      " quant_block_9_depthwise_BN (Qu  (None, 14, 14, 384)  1537       ['quant_block_9_depthwise[0][0]']\n",
      " antizeWrapperV2)                                                                                 \n",
      "                                                                                                  \n",
      " quant_block_9_depthwise_relu (  (None, 14, 14, 384)  3          ['quant_block_9_depthwise_BN[0][0\n",
      " QuantizeWrapperV2)                                              ]']                              \n",
      "                                                                                                  \n",
      " quant_block_9_project (Quantiz  (None, 14, 14, 64)  24705       ['quant_block_9_depthwise_relu[0]\n",
      " eWrapperV2)                                                     [0]']                            \n",
      "                                                                                                  \n",
      " quant_block_9_project_BN (Quan  (None, 14, 14, 64)  259         ['quant_block_9_project[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_9_add (QuantizeWra  (None, 14, 14, 64)  3           ['quant_block_8_add[0][0]',      \n",
      " pperV2)                                                          'quant_block_9_project_BN[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " quant_block_10_expand (Quantiz  (None, 14, 14, 384)  25345      ['quant_block_9_add[0][0]']      \n",
      " eWrapperV2)                                                                                      \n",
      "                                                                                                  \n",
      " quant_block_10_expand_BN (Quan  (None, 14, 14, 384)  1537       ['quant_block_10_expand[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_10_expand_relu (Qu  (None, 14, 14, 384)  3          ['quant_block_10_expand_BN[0][0]'\n",
      " antizeWrapperV2)                                                ]                                \n",
      "                                                                                                  \n",
      " quant_block_10_depthwise (Quan  (None, 14, 14, 384)  3459       ['quant_block_10_expand_relu[0][0\n",
      " tizeWrapperV2)                                                  ]']                              \n",
      "                                                                                                  \n",
      " quant_block_10_depthwise_BN (Q  (None, 14, 14, 384)  1537       ['quant_block_10_depthwise[0][0]'\n",
      " uantizeWrapperV2)                                               ]                                \n",
      "                                                                                                  \n",
      " quant_block_10_depthwise_relu   (None, 14, 14, 384)  3          ['quant_block_10_depthwise_BN[0][\n",
      " (QuantizeWrapperV2)                                             0]']                             \n",
      "                                                                                                  \n",
      " quant_block_10_project (Quanti  (None, 14, 14, 96)  37057       ['quant_block_10_depthwise_relu[0\n",
      " zeWrapperV2)                                                    ][0]']                           \n",
      "                                                                                                  \n",
      " quant_block_10_project_BN (Qua  (None, 14, 14, 96)  387         ['quant_block_10_project[0][0]'] \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_11_expand (Quantiz  (None, 14, 14, 576)  56449      ['quant_block_10_project_BN[0][0]\n",
      " eWrapperV2)                                                     ']                               \n",
      "                                                                                                  \n",
      " quant_block_11_expand_BN (Quan  (None, 14, 14, 576)  2305       ['quant_block_11_expand[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_11_expand_relu (Qu  (None, 14, 14, 576)  3          ['quant_block_11_expand_BN[0][0]'\n",
      " antizeWrapperV2)                                                ]                                \n",
      "                                                                                                  \n",
      " quant_block_11_depthwise (Quan  (None, 14, 14, 576)  5187       ['quant_block_11_expand_relu[0][0\n",
      " tizeWrapperV2)                                                  ]']                              \n",
      "                                                                                                  \n",
      " quant_block_11_depthwise_BN (Q  (None, 14, 14, 576)  2305       ['quant_block_11_depthwise[0][0]'\n",
      " uantizeWrapperV2)                                               ]                                \n",
      "                                                                                                  \n",
      " quant_block_11_depthwise_relu   (None, 14, 14, 576)  3          ['quant_block_11_depthwise_BN[0][\n",
      " (QuantizeWrapperV2)                                             0]']                             \n",
      "                                                                                                  \n",
      " quant_block_11_project (Quanti  (None, 14, 14, 96)  55489       ['quant_block_11_depthwise_relu[0\n",
      " zeWrapperV2)                                                    ][0]']                           \n",
      "                                                                                                  \n",
      " quant_block_11_project_BN (Qua  (None, 14, 14, 96)  387         ['quant_block_11_project[0][0]'] \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_11_add (QuantizeWr  (None, 14, 14, 96)  3           ['quant_block_10_project_BN[0][0]\n",
      " apperV2)                                                        ',                               \n",
      "                                                                  'quant_block_11_project_BN[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " quant_block_12_expand (Quantiz  (None, 14, 14, 576)  56449      ['quant_block_11_add[0][0]']     \n",
      " eWrapperV2)                                                                                      \n",
      "                                                                                                  \n",
      " quant_block_12_expand_BN (Quan  (None, 14, 14, 576)  2305       ['quant_block_12_expand[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_12_expand_relu (Qu  (None, 14, 14, 576)  3          ['quant_block_12_expand_BN[0][0]'\n",
      " antizeWrapperV2)                                                ]                                \n",
      "                                                                                                  \n",
      " quant_block_12_depthwise (Quan  (None, 14, 14, 576)  5187       ['quant_block_12_expand_relu[0][0\n",
      " tizeWrapperV2)                                                  ]']                              \n",
      "                                                                                                  \n",
      " quant_block_12_depthwise_BN (Q  (None, 14, 14, 576)  2305       ['quant_block_12_depthwise[0][0]'\n",
      " uantizeWrapperV2)                                               ]                                \n",
      "                                                                                                  \n",
      " quant_block_12_depthwise_relu   (None, 14, 14, 576)  3          ['quant_block_12_depthwise_BN[0][\n",
      " (QuantizeWrapperV2)                                             0]']                             \n",
      "                                                                                                  \n",
      " quant_block_12_project (Quanti  (None, 14, 14, 96)  55489       ['quant_block_12_depthwise_relu[0\n",
      " zeWrapperV2)                                                    ][0]']                           \n",
      "                                                                                                  \n",
      " quant_block_12_project_BN (Qua  (None, 14, 14, 96)  387         ['quant_block_12_project[0][0]'] \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_12_add (QuantizeWr  (None, 14, 14, 96)  3           ['quant_block_11_add[0][0]',     \n",
      " apperV2)                                                         'quant_block_12_project_BN[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " quant_block_13_expand (Quantiz  (None, 14, 14, 576)  56449      ['quant_block_12_add[0][0]']     \n",
      " eWrapperV2)                                                                                      \n",
      "                                                                                                  \n",
      " quant_block_13_expand_BN (Quan  (None, 14, 14, 576)  2305       ['quant_block_13_expand[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_13_expand_relu (Qu  (None, 14, 14, 576)  3          ['quant_block_13_expand_BN[0][0]'\n",
      " antizeWrapperV2)                                                ]                                \n",
      "                                                                                                  \n",
      " quant_block_13_pad (QuantizeWr  (None, 15, 15, 576)  1          ['quant_block_13_expand_relu[0][0\n",
      " apperV2)                                                        ]']                              \n",
      "                                                                                                  \n",
      " quant_block_13_depthwise (Quan  (None, 7, 7, 576)   5187        ['quant_block_13_pad[0][0]']     \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_13_depthwise_BN (Q  (None, 7, 7, 576)   2305        ['quant_block_13_depthwise[0][0]'\n",
      " uantizeWrapperV2)                                               ]                                \n",
      "                                                                                                  \n",
      " quant_block_13_depthwise_relu   (None, 7, 7, 576)   3           ['quant_block_13_depthwise_BN[0][\n",
      " (QuantizeWrapperV2)                                             0]']                             \n",
      "                                                                                                  \n",
      " quant_block_13_project (Quanti  (None, 7, 7, 160)   92481       ['quant_block_13_depthwise_relu[0\n",
      " zeWrapperV2)                                                    ][0]']                           \n",
      "                                                                                                  \n",
      " quant_block_13_project_BN (Qua  (None, 7, 7, 160)   643         ['quant_block_13_project[0][0]'] \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_14_expand (Quantiz  (None, 7, 7, 960)   155521      ['quant_block_13_project_BN[0][0]\n",
      " eWrapperV2)                                                     ']                               \n",
      "                                                                                                  \n",
      " quant_block_14_expand_BN (Quan  (None, 7, 7, 960)   3841        ['quant_block_14_expand[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_14_expand_relu (Qu  (None, 7, 7, 960)   3           ['quant_block_14_expand_BN[0][0]'\n",
      " antizeWrapperV2)                                                ]                                \n",
      "                                                                                                  \n",
      " quant_block_14_depthwise (Quan  (None, 7, 7, 960)   8643        ['quant_block_14_expand_relu[0][0\n",
      " tizeWrapperV2)                                                  ]']                              \n",
      "                                                                                                  \n",
      " quant_block_14_depthwise_BN (Q  (None, 7, 7, 960)   3841        ['quant_block_14_depthwise[0][0]'\n",
      " uantizeWrapperV2)                                               ]                                \n",
      "                                                                                                  \n",
      " quant_block_14_depthwise_relu   (None, 7, 7, 960)   3           ['quant_block_14_depthwise_BN[0][\n",
      " (QuantizeWrapperV2)                                             0]']                             \n",
      "                                                                                                  \n",
      " quant_block_14_project (Quanti  (None, 7, 7, 160)   153921      ['quant_block_14_depthwise_relu[0\n",
      " zeWrapperV2)                                                    ][0]']                           \n",
      "                                                                                                  \n",
      " quant_block_14_project_BN (Qua  (None, 7, 7, 160)   643         ['quant_block_14_project[0][0]'] \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_14_add (QuantizeWr  (None, 7, 7, 160)   3           ['quant_block_13_project_BN[0][0]\n",
      " apperV2)                                                        ',                               \n",
      "                                                                  'quant_block_14_project_BN[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " quant_block_15_expand (Quantiz  (None, 7, 7, 960)   155521      ['quant_block_14_add[0][0]']     \n",
      " eWrapperV2)                                                                                      \n",
      "                                                                                                  \n",
      " quant_block_15_expand_BN (Quan  (None, 7, 7, 960)   3841        ['quant_block_15_expand[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_15_expand_relu (Qu  (None, 7, 7, 960)   3           ['quant_block_15_expand_BN[0][0]'\n",
      " antizeWrapperV2)                                                ]                                \n",
      "                                                                                                  \n",
      " quant_block_15_depthwise (Quan  (None, 7, 7, 960)   8643        ['quant_block_15_expand_relu[0][0\n",
      " tizeWrapperV2)                                                  ]']                              \n",
      "                                                                                                  \n",
      " quant_block_15_depthwise_BN (Q  (None, 7, 7, 960)   3841        ['quant_block_15_depthwise[0][0]'\n",
      " uantizeWrapperV2)                                               ]                                \n",
      "                                                                                                  \n",
      " quant_block_15_depthwise_relu   (None, 7, 7, 960)   3           ['quant_block_15_depthwise_BN[0][\n",
      " (QuantizeWrapperV2)                                             0]']                             \n",
      "                                                                                                  \n",
      " quant_block_15_project (Quanti  (None, 7, 7, 160)   153921      ['quant_block_15_depthwise_relu[0\n",
      " zeWrapperV2)                                                    ][0]']                           \n",
      "                                                                                                  \n",
      " quant_block_15_project_BN (Qua  (None, 7, 7, 160)   643         ['quant_block_15_project[0][0]'] \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_block_15_add (QuantizeWr  (None, 7, 7, 160)   3           ['quant_block_14_add[0][0]',     \n",
      " apperV2)                                                         'quant_block_15_project_BN[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " quant_block_16_expand (Quantiz  (None, 7, 7, 960)   155521      ['quant_block_15_add[0][0]']     \n",
      " eWrapperV2)                                                                                      \n",
      "                                                                                                  \n",
      " quant_block_16_expand_BN (Quan  (None, 7, 7, 960)   3841        ['quant_block_16_expand[0][0]']  \n",
      " tizeWrapperV2)                                                                                   \n",
      "                                                                                                  \n",
      " quant_block_16_expand_relu (Qu  (None, 7, 7, 960)   3           ['quant_block_16_expand_BN[0][0]'\n",
      " antizeWrapperV2)                                                ]                                \n",
      "                                                                                                  \n",
      " quant_block_16_depthwise (Quan  (None, 7, 7, 960)   8643        ['quant_block_16_expand_relu[0][0\n",
      " tizeWrapperV2)                                                  ]']                              \n",
      "                                                                                                  \n",
      " quant_block_16_depthwise_BN (Q  (None, 7, 7, 960)   3841        ['quant_block_16_depthwise[0][0]'\n",
      " uantizeWrapperV2)                                               ]                                \n",
      "                                                                                                  \n",
      " quant_block_16_depthwise_relu   (None, 7, 7, 960)   3           ['quant_block_16_depthwise_BN[0][\n",
      " (QuantizeWrapperV2)                                             0]']                             \n",
      "                                                                                                  \n",
      " quant_block_16_project (Quanti  (None, 7, 7, 320)   307841      ['quant_block_16_depthwise_relu[0\n",
      " zeWrapperV2)                                                    ][0]']                           \n",
      "                                                                                                  \n",
      " quant_block_16_project_BN (Qua  (None, 7, 7, 320)   1283        ['quant_block_16_project[0][0]'] \n",
      " ntizeWrapperV2)                                                                                  \n",
      "                                                                                                  \n",
      " quant_Conv_1 (QuantizeWrapperV  (None, 7, 7, 1280)  412161      ['quant_block_16_project_BN[0][0]\n",
      " 2)                                                              ']                               \n",
      "                                                                                                  \n",
      " quant_Conv_1_bn (QuantizeWrapp  (None, 7, 7, 1280)  5121        ['quant_Conv_1[0][0]']           \n",
      " erV2)                                                                                            \n",
      "                                                                                                  \n",
      " quant_out_relu (QuantizeWrappe  (None, 7, 7, 1280)  3           ['quant_Conv_1_bn[0][0]']        \n",
      " rV2)                                                                                             \n",
      "                                                                                                  \n",
      " quant_global_average_pooling2d  (None, 1280)        3           ['quant_out_relu[0][0]']         \n",
      " _5 (QuantizeWrapperV2)                                                                           \n",
      "                                                                                                  \n",
      " quant_dropout_10 (QuantizeWrap  (None, 1280)        1           ['quant_global_average_pooling2d_\n",
      " perV2)                                                          5[0][0]']                        \n",
      "                                                                                                  \n",
      " quant_dense_10 (QuantizeWrappe  (None, 128)         163973      ['quant_dropout_10[0][0]']       \n",
      " rV2)                                                                                             \n",
      "                                                                                                  \n",
      " quant_dropout_11 (QuantizeWrap  (None, 128)         1           ['quant_dense_10[0][0]']         \n",
      " perV2)                                                                                           \n",
      "                                                                                                  \n",
      " quant_dense_11 (QuantizeWrappe  (None, 3)           392         ['quant_dropout_11[0][0]']       \n",
      " rV2)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,442,508\n",
      "Trainable params: 2,367,555\n",
      "Non-trainable params: 74,953\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "--- Treinamento QAT (Fine-Tuning) ---\n",
      "Epoch 1/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.2884 - accuracy: 0.8943\n",
      "Epoch 1: val_loss improved from inf to 0.09757, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/qat_checkpoint.keras\n",
      "780/780 [==============================] - 869s 1s/step - loss: 0.2884 - accuracy: 0.8943 - val_loss: 0.0976 - val_accuracy: 0.9617 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9536\n",
      "Epoch 2: val_loss improved from 0.09757 to 0.07866, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/qat_checkpoint.keras\n",
      "780/780 [==============================] - 865s 1s/step - loss: 0.1187 - accuracy: 0.9536 - val_loss: 0.0787 - val_accuracy: 0.9719 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9634\n",
      "Epoch 3: val_loss did not improve from 0.07866\n",
      "780/780 [==============================] - 863s 1s/step - loss: 0.0959 - accuracy: 0.9634 - val_loss: 0.0882 - val_accuracy: 0.9667 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9682\n",
      "Epoch 4: val_loss did not improve from 0.07866\n",
      "780/780 [==============================] - 862s 1s/step - loss: 0.0855 - accuracy: 0.9682 - val_loss: 0.1151 - val_accuracy: 0.9607 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9697\n",
      "Epoch 5: val_loss did not improve from 0.07866\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "780/780 [==============================] - 864s 1s/step - loss: 0.0811 - accuracy: 0.9697 - val_loss: 0.0806 - val_accuracy: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9733\n",
      "Epoch 6: val_loss improved from 0.07866 to 0.05171, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/qat_checkpoint.keras\n",
      "780/780 [==============================] - 865s 1s/step - loss: 0.0704 - accuracy: 0.9733 - val_loss: 0.0517 - val_accuracy: 0.9809 - lr: 5.0000e-06\n",
      "Epoch 7/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9750\n",
      "Epoch 7: val_loss did not improve from 0.05171\n",
      "780/780 [==============================] - 870s 1s/step - loss: 0.0691 - accuracy: 0.9750 - val_loss: 0.0604 - val_accuracy: 0.9791 - lr: 5.0000e-06\n",
      "Epoch 8/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.9764\n",
      "Epoch 8: val_loss did not improve from 0.05171\n",
      "780/780 [==============================] - 871s 1s/step - loss: 0.0628 - accuracy: 0.9764 - val_loss: 0.0547 - val_accuracy: 0.9798 - lr: 5.0000e-06\n",
      "Epoch 9/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9772\n",
      "Epoch 9: val_loss improved from 0.05171 to 0.04144, saving model to /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/qat_checkpoint.keras\n",
      "780/780 [==============================] - 873s 1s/step - loss: 0.0604 - accuracy: 0.9772 - val_loss: 0.0414 - val_accuracy: 0.9850 - lr: 5.0000e-06\n",
      "Epoch 10/10\n",
      "780/780 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9792\n",
      "Epoch 10: val_loss did not improve from 0.04144\n",
      "780/780 [==============================] - 877s 1s/step - loss: 0.0569 - accuracy: 0.9792 - val_loss: 0.0796 - val_accuracy: 0.9691 - lr: 5.0000e-06\n",
      "Aviso: n√£o foi poss√≠vel recarregar checkpoint para salvar final: 'CustomObjectScope' object is not iterable\n",
      "‚úÖ Hist√≥rico de treinamento salvo em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/metrics/etapa_4_qat_int8/qat_checkpoint_history.json\n",
      "Modelo QAT final (Keras) salvo em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/qat_checkpoint_final.keras\n"
     ]
    }
   ],
   "source": [
    "print('\\n--- üöÄ Prepara√ß√£o para QAT (Inser√ß√£o de Fake Quants) ---')\n",
    "\n",
    "# Carregar o melhor modelo FP32 da Etapa 3\n",
    "try:\n",
    "    fp32_model_for_qat = load_model(CHECKPOINT_FINETUNE, compile=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao carregar modelo Fine-Tuned: {e}. Usando o modelo robusto.\")\n",
    "    fp32_model_for_qat = load_model(CHECKPOINT_ROBUST, compile=False)\n",
    "\n",
    "# 1. Envolver o modelo FP32 com as opera√ß√µes Fake Quant\n",
    "if strategy_scope_used:\n",
    "    with strategy.scope():\n",
    "        # O modelo QAT deve ser compilado e treinado DENTRO do escopo da estrat√©gia\n",
    "        qat_model = quantize_model(fp32_model_for_qat)\n",
    "        \n",
    "        # 2. Recompilar o modelo QAT (Importante: LR menor)\n",
    "        qat_model.compile(optimizer=Adam(learning_rate=LR_QAT),\n",
    "                          loss='categorical_crossentropy', \n",
    "                          metrics=['accuracy'])\n",
    "else:\n",
    "    qat_model = quantize_model(fp32_model_for_qat)\n",
    "    qat_model.compile(optimizer=Adam(learning_rate=LR_QAT),\n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "qat_model.summary() \n",
    "\n",
    "print('\\n--- Treinamento QAT (Fine-Tuning) ---')\n",
    "\n",
    "# 3. Treinar o modelo QAT (Fine-Tuning)\n",
    "history_qat, qat_final_path_keras = train_and_save(\n",
    "    qat_model, \n",
    "    train_gen_ft, # Usamos o gerador de Fine-Tuning (com augmentation)\n",
    "    val_gen_ft, \n",
    "    epochs=EPOCHS_QAT, \n",
    "    checkpoint_path=QAT_CHECKPOINT,\n",
    "    stage_name='etapa_4_qat_int8',\n",
    "    early_stop_patience=6,\n",
    "    strategy=strategy\n",
    ")\n",
    "print('Modelo QAT final (Keras) salvo em:', qat_final_path_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b0a1c",
   "metadata": {},
   "source": [
    "## 6. **Convers√£o para TFLite e Avalia√ß√£o Final**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa063b",
   "metadata": {},
   "source": [
    "Convers√£o do modelo QAT treinado para o formato TFLite totalmente quantizado (INT8) e avalia√ß√£o no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "693bc4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Convers√£o para TFLite INT8 ---\n",
      "‚úÖ Melhor modelo QAT carregado de: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/qat_checkpoint.keras\n",
      "\n",
      "Preparando dataset representativo para calibra√ß√£o...\n",
      "Convertendo para TFLite INT8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as Conv1_layer_call_fn, Conv1_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, Conv1_relu_layer_call_fn, Conv1_relu_layer_call_and_return_conditional_losses while saving (showing 5 of 254). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp5ow7edzx/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp5ow7edzx/assets\n",
      "/home/ampliar/miniconda3/envs/qat_cancer_env/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2025-12-03 09:24:04.616973: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2025-12-03 09:24:04.617021: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2025-12-03 09:24:04.617171: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp5ow7edzx\n",
      "2025-12-03 09:24:04.643184: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2025-12-03 09:24:04.643222: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp5ow7edzx\n",
      "2025-12-03 09:24:04.784322: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2025-12-03 09:24:05.427390: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmp5ow7edzx\n",
      "2025-12-03 09:24:06.627587: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 2010414 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando 100 amostras para calibra√ß√£o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Modelo TFLite INT8 salvo em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/qat_model.tflite\n",
      "üìè Tamanho do arquivo: 2.74 MB\n",
      "\n",
      "üìä Testando modelo TFLite...\n",
      "‚úÖ Modelo TFLite carregado com sucesso!\n",
      "   - Formato de entrada: [  1 224 224   3]\n",
      "   - Tipo de entrada: <class 'numpy.uint8'>\n",
      "   - Formato de sa√≠da: [1 3]\n",
      "   - Tipo de sa√≠da: <class 'numpy.uint8'>\n",
      "\n",
      "üß™ Realizando teste de infer√™ncia...\n",
      "   ‚úÖ Infer√™ncia realizada com sucesso!\n",
      "   üìä Dimens√µes da sa√≠da: (1, 3)\n",
      "\n",
      "--- Avalia√ß√£o da Precis√£o do Modelo QAT (Teste - Simulado FP32) ---\n",
      "\n",
      "Avaliando QAT (Simulado FP32 - Teste) no conjunto de teste...\n",
      "5347/5347 [==============================] - 235s 44ms/step\n",
      "‚úÖ Matriz de Confus√£o salva em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/metrics/etapa_4_qat_int8/qat_(simulado_fp32_-_teste)_confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relat√≥rio de Classifica√ß√£o - QAT (Simulado FP32 - Teste):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       HISIL       0.99      0.98      0.99      1350\n",
      "       LISIL       0.90      0.99      0.94      1362\n",
      "      Normal       0.99      0.95      0.97      2635\n",
      "\n",
      "    accuracy                           0.97      5347\n",
      "   macro avg       0.96      0.97      0.97      5347\n",
      "weighted avg       0.97      0.97      0.97      5347\n",
      "\n",
      "\n",
      "--- Compara√ß√£o Final de Tamanhos ---\n",
      "\n",
      "==================================================\n",
      "COMPARA√á√ÉO DE TAMANHOS DE MODELOS\n",
      "==================================================\n",
      "Modelo FP32 (Keras)            21.42 MB  (../models/final_finetuned_checkpoint.keras)\n",
      "Modelo QAT (Keras)             28.39 MB  (../models/qat_checkpoint.keras)\n",
      "Modelo TFLite                   2.74 MB  (../models/qat_model.tflite)\n",
      "\n",
      "==================================================\n",
      "RESUMO DE COMPRESS√ÉO\n",
      "==================================================\n",
      "Tamanho original (FP32):     21.42 MB\n",
      "Tamanho quantizado (TFLite): 2.74 MB\n",
      "Redu√ß√£o de tamanho:          87.20%\n",
      "Taxa de compress√£o:          7.81x\n",
      "\n",
      "üìà INTERPRETA√á√ÉO:\n",
      "  ‚úÖ Excelente compress√£o! O modelo est√° muito otimizado.\n",
      "\n",
      "==================================================\n",
      "VERIFICA√á√ÉO FINAL DO MODELO TFLITE\n",
      "==================================================\n",
      "\n",
      "üîç Verificando: qat_model.tflite\n",
      "   ‚úÖ Modelo v√°lido!\n",
      "   üì• Entrada: [  1 224 224   3] (<class 'numpy.uint8'>)\n",
      "   üì§ Sa√≠da: [1 3] (<class 'numpy.uint8'>)\n",
      "   üéØ Quantiza√ß√£o: escala=0.003921568859368563, zero_point=0\n",
      "\n",
      "==================================================\n",
      "FIM DO PIPELINE DE QUANTIZA√á√ÉO\n",
      "==================================================\n",
      "‚úÖ Pipeline conclu√≠do com sucesso!\n",
      "üìä Modelo TFLite criado: 2874120 bytes\n"
     ]
    }
   ],
   "source": [
    "print('\\n--- Convers√£o para TFLite INT8 ---')\n",
    "\n",
    "# Definir o tamanho da imagem se n√£o estiver definido\n",
    "if 'IMG_SIZE' not in globals():\n",
    "    # Verificar o tamanho esperado pelo modelo\n",
    "    try:\n",
    "        with quantize_scope():\n",
    "            temp_model = tf.keras.models.load_model(QAT_CHECKPOINT, compile=False)\n",
    "            IMG_SIZE = temp_model.input_shape[1]  # Obter do modelo\n",
    "            print(f\"‚úÖ Tamanho da imagem detectado do modelo: {IMG_SIZE}\")\n",
    "            del temp_model\n",
    "    except:\n",
    "        IMG_SIZE = 224  # Valor padr√£o para MobileNetV2\n",
    "        print(f\"‚ö†Ô∏è  Usando tamanho padr√£o da imagem: {IMG_SIZE}\")\n",
    "\n",
    "# Verificar se o checkpoint QAT existe\n",
    "if not os.path.exists(QAT_CHECKPOINT):\n",
    "    print(f\"‚ùå Checkpoint QAT n√£o encontrado: {QAT_CHECKPOINT}\")\n",
    "    print(\"Por favor, execute a etapa de QAT primeiro.\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. Carregar o melhor modelo QAT salvo para a convers√£o\n",
    "        with quantize_scope():\n",
    "            model_qat_lite = tf.keras.models.load_model(QAT_CHECKPOINT, compile=False)\n",
    "            print(f\"‚úÖ Melhor modelo QAT carregado de: {QAT_CHECKPOINT}\")\n",
    "        \n",
    "        # 2. Preparar dataset representativo (para quantiza√ß√£o INT8 completa)\n",
    "        print(\"\\nPreparando dataset representativo para calibra√ß√£o...\")\n",
    "        \n",
    "        # Fun√ß√£o geradora para dataset representativo - vers√£o simplificada\n",
    "        def representative_dataset_gen():\n",
    "            \"\"\"Gerador para dataset representativo\"\"\"\n",
    "            num_samples = 100  # N√∫mero de amostras para calibra√ß√£o\n",
    "            \n",
    "            # Usar um subconjunto do dataset de valida√ß√£o\n",
    "            calib_df = val_df.sample(min(num_samples, len(val_df)), random_state=42)\n",
    "            \n",
    "            print(f\"Usando {len(calib_df)} amostras para calibra√ß√£o...\")\n",
    "            \n",
    "            for idx, row in calib_df.iterrows():\n",
    "                try:\n",
    "                    img_path = os.path.join(IMAGE_ROOT, row['image_path'])\n",
    "                    img = tf.io.read_file(img_path)\n",
    "                    img = tf.image.decode_jpeg(img, channels=3)\n",
    "                    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "                    img = img / 255.0  # Normalizar\n",
    "                    img = tf.expand_dims(img, axis=0)  # Adicionar dimens√£o batch\n",
    "                    \n",
    "                    # Converter para float32\n",
    "                    img = tf.cast(img, tf.float32)\n",
    "                    \n",
    "                    # Verificar se a imagem tem o formato correto\n",
    "                    if img.shape == (1, IMG_SIZE, IMG_SIZE, 3):\n",
    "                        yield [img]\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è  Formato incorreto: {img.shape}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Erro ao processar imagem {row['image_path']}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # 3. Converter para TFLite\n",
    "        print(\"Convertendo para TFLite INT8...\")\n",
    "        \n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model_qat_lite)\n",
    "        \n",
    "        # Configurar quantiza√ß√£o\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "        \n",
    "        # Para quantiza√ß√£o completa INT8\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.uint8\n",
    "        converter.inference_output_type = tf.uint8\n",
    "        \n",
    "        # Converter\n",
    "        tflite_quant_model = converter.convert()\n",
    "        \n",
    "        # Salvar o modelo quantizado\n",
    "        # CORRE√á√ÉO: Converter QAT_TFLITE_PATH para string se necess√°rio\n",
    "        tflite_path_str = str(QAT_TFLITE_PATH) if hasattr(QAT_TFLITE_PATH, '__fspath__') else QAT_TFLITE_PATH\n",
    "        with open(tflite_path_str, 'wb') as f:\n",
    "            f.write(tflite_quant_model)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Modelo TFLite INT8 salvo em: {tflite_path_str}\")\n",
    "        print(f\"üìè Tamanho do arquivo: {len(tflite_quant_model) / (1024 * 1024):.2f} MB\")\n",
    "        \n",
    "        # Testar o modelo TFLite\n",
    "        print(\"\\nüìä Testando modelo TFLite...\")\n",
    "        try:\n",
    "            # CORRE√á√ÉO: Usar string em vez de objeto Path\n",
    "            interpreter = tf.lite.Interpreter(model_path=tflite_path_str)\n",
    "            interpreter.allocate_tensors()\n",
    "            input_details = interpreter.get_input_details()\n",
    "            output_details = interpreter.get_output_details()\n",
    "            \n",
    "            print(f\"‚úÖ Modelo TFLite carregado com sucesso!\")\n",
    "            print(f\"   - Formato de entrada: {input_details[0]['shape']}\")\n",
    "            print(f\"   - Tipo de entrada: {input_details[0]['dtype']}\")\n",
    "            print(f\"   - Formato de sa√≠da: {output_details[0]['shape']}\")\n",
    "            print(f\"   - Tipo de sa√≠da: {output_details[0]['dtype']}\")\n",
    "            \n",
    "            # Testar uma infer√™ncia simples\n",
    "            print(f\"\\nüß™ Realizando teste de infer√™ncia...\")\n",
    "            # Criar um tensor de entrada aleat√≥rio no formato correto\n",
    "            test_input = np.random.randint(0, 255, size=(1, IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Configurar a entrada\n",
    "            interpreter.set_tensor(input_details[0]['index'], test_input)\n",
    "            \n",
    "            # Executar infer√™ncia\n",
    "            interpreter.invoke()\n",
    "            \n",
    "            # Obter resultado\n",
    "            output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "            print(f\"   ‚úÖ Infer√™ncia realizada com sucesso!\")\n",
    "            print(f\"   üìä Dimens√µes da sa√≠da: {output_data.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  N√£o foi poss√≠vel testar o modelo TFLite: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERRO durante a convers√£o TFLite: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Tentar convers√£o mais simples como fallback\n",
    "        print(\"\\nüîÑ Tentando convers√£o alternativa (quantiza√ß√£o din√¢mica)...\")\n",
    "        try:\n",
    "            with quantize_scope():\n",
    "                model_qat_lite = tf.keras.models.load_model(QAT_CHECKPOINT, compile=False)\n",
    "            \n",
    "            converter = tf.lite.TFLiteConverter.from_keras_model(model_qat_lite)\n",
    "            \n",
    "            # Apenas quantiza√ß√£o din√¢mica (n√£o requer dataset representativo)\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "            \n",
    "            tflite_model = converter.convert()\n",
    "            \n",
    "            # CORRE√á√ÉO: Converter caminho para string\n",
    "            fallback_path = str(QAT_TFLITE_PATH).replace('.tflite', '_dynamic.tflite')\n",
    "            with open(fallback_path, 'wb') as f:\n",
    "                f.write(tflite_model)\n",
    "            \n",
    "            print(f\"‚úÖ Modelo TFLite (quantiza√ß√£o din√¢mica) salvo em: {fallback_path}\")\n",
    "            print(f\"üìè Tamanho: {len(tflite_model) / (1024 * 1024):.2f} MB\")\n",
    "            \n",
    "            # Atualizar o caminho para compara√ß√£o (como string)\n",
    "            QAT_TFLITE_PATH = fallback_path\n",
    "            \n",
    "        except Exception as fallback_error:\n",
    "            print(f\"‚ùå Falha na convers√£o fallback: {fallback_error}\")\n",
    "\n",
    "# --- Avalia√ß√£o da Precis√£o do Modelo QAT (Simulado FP32) ---\n",
    "print('\\n--- Avalia√ß√£o da Precis√£o do Modelo QAT (Teste - Simulado FP32) ---')\n",
    "# O modelo QAT Keras simula INT8, mas roda em FP32.\n",
    "\n",
    "# Verificar se qat_model est√° definido\n",
    "try:\n",
    "    if 'qat_model' in globals():\n",
    "        evaluate_model(qat_model, test_gen_baseline, 'QAT (Simulado FP32 - Teste)', 'etapa_4_qat_int8')\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Modelo QAT n√£o encontrado na mem√≥ria. Carregando do checkpoint...\")\n",
    "        with quantize_scope():\n",
    "            qat_model = tf.keras.models.load_model(QAT_CHECKPOINT, compile=False)\n",
    "            evaluate_model(qat_model, test_gen_baseline, 'QAT (Simulado FP32 - Teste)', 'etapa_4_qat_int8')\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro na avalia√ß√£o do modelo QAT: {e}\")\n",
    "\n",
    "print(\"\\n--- Compara√ß√£o Final de Tamanhos ---\")\n",
    "\n",
    "# Lista de arquivos para comparar\n",
    "files_to_compare = [\n",
    "    (CHECKPOINT_FINETUNE, \"Modelo FP32 (Keras)\"),\n",
    "    (QAT_CHECKPOINT, \"Modelo QAT (Keras)\"),\n",
    "]\n",
    "\n",
    "# Adicionar arquivo TFLite se existir\n",
    "# CORRE√á√ÉO: Garantir que estamos usando strings para os caminhos\n",
    "tflite_files = []\n",
    "\n",
    "# Converter QAT_TFLITE_PATH para string se necess√°rio\n",
    "if hasattr(QAT_TFLITE_PATH, '__fspath__'):\n",
    "    base_tflite_path = str(QAT_TFLITE_PATH)\n",
    "else:\n",
    "    base_tflite_path = QAT_TFLITE_PATH\n",
    "\n",
    "tflite_files.extend([\n",
    "    base_tflite_path,\n",
    "    base_tflite_path.replace('.tflite', '_dynamic.tflite'),\n",
    "    base_tflite_path.replace('.tflite', '_no_quant.tflite')\n",
    "])\n",
    "\n",
    "# Remover duplicatas\n",
    "tflite_files = list(dict.fromkeys(tflite_files))\n",
    "\n",
    "for tflite_file in tflite_files:\n",
    "    if os.path.exists(tflite_file):\n",
    "        files_to_compare.append((tflite_file, \"Modelo TFLite\"))\n",
    "        break\n",
    "\n",
    "# Calcular e mostrar tamanhos\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARA√á√ÉO DE TAMANHOS DE MODELOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for file_path, desc in files_to_compare:\n",
    "    if os.path.exists(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        # Mostrar caminho relativo se poss√≠vel\n",
    "        try:\n",
    "            rel_path = os.path.relpath(file_path)\n",
    "            print(f\"{desc:25} {size_mb:10.2f} MB  ({rel_path})\")\n",
    "        except:\n",
    "            print(f\"{desc:25} {size_mb:10.2f} MB  ({file_path})\")\n",
    "    else:\n",
    "        print(f\"{desc:25} {'N/A':>10}  (arquivo n√£o encontrado)\")\n",
    "\n",
    "# Calcular redu√ß√£o se ambos FP32 e TFLite existirem\n",
    "if os.path.exists(CHECKPOINT_FINETUNE):\n",
    "    fp32_size_mb = os.path.getsize(CHECKPOINT_FINETUNE) / (1024 * 1024)\n",
    "    \n",
    "    # Encontrar o primeiro arquivo TFLite existente\n",
    "    tflite_path = None\n",
    "    for tflite_file in tflite_files:\n",
    "        if os.path.exists(tflite_file):\n",
    "            tflite_path = tflite_file\n",
    "            break\n",
    "    \n",
    "    if tflite_path:\n",
    "        tflite_size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "        \n",
    "        if fp32_size_mb > 0:\n",
    "            reduction = 100 * (1 - tflite_size_mb / fp32_size_mb)\n",
    "            compression_ratio = fp32_size_mb / tflite_size_mb\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"RESUMO DE COMPRESS√ÉO\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Tamanho original (FP32):     {fp32_size_mb:.2f} MB\")\n",
    "            print(f\"Tamanho quantizado (TFLite): {tflite_size_mb:.2f} MB\")\n",
    "            print(f\"Redu√ß√£o de tamanho:          {reduction:.2f}%\")\n",
    "            print(f\"Taxa de compress√£o:          {compression_ratio:.2f}x\")\n",
    "            \n",
    "            # Interpreta√ß√£o dos resultados\n",
    "            print(\"\\nüìà INTERPRETA√á√ÉO:\")\n",
    "            if reduction > 70:\n",
    "                print(\"  ‚úÖ Excelente compress√£o! O modelo est√° muito otimizado.\")\n",
    "            elif reduction > 50:\n",
    "                print(\"  üëç Boa compress√£o. Bom para implanta√ß√£o em dispositivos m√≥veis.\")\n",
    "            elif reduction > 30:\n",
    "                print(\"  üì± Compress√£o razo√°vel. Adequado para muitos casos de uso.\")\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è  Compress√£o limitada. Considere otimiza√ß√µes adicionais.\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  N√£o foi poss√≠vel calcular a redu√ß√£o (tamanho FP32 √© zero)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Nenhum arquivo TFLite encontrado para compara√ß√£o\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Arquivo FP32 n√£o encontrado para compara√ß√£o\")\n",
    "\n",
    "# Adicionar verifica√ß√£o final do modelo TFLite\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFICA√á√ÉO FINAL DO MODELO TFLITE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Encontrar qualquer arquivo TFLite v√°lido\n",
    "for tflite_file in tflite_files:\n",
    "    if os.path.exists(tflite_file):\n",
    "        try:\n",
    "            print(f\"\\nüîç Verificando: {os.path.basename(tflite_file)}\")\n",
    "            \n",
    "            # Carregar modelo TFLite\n",
    "            interpreter = tf.lite.Interpreter(model_path=tflite_file)\n",
    "            interpreter.allocate_tensors()\n",
    "            \n",
    "            # Obter detalhes\n",
    "            input_details = interpreter.get_input_details()\n",
    "            output_details = interpreter.get_output_details()\n",
    "            \n",
    "            print(f\"   ‚úÖ Modelo v√°lido!\")\n",
    "            print(f\"   üì• Entrada: {input_details[0]['shape']} ({input_details[0]['dtype']})\")\n",
    "            print(f\"   üì§ Sa√≠da: {output_details[0]['shape']} ({output_details[0]['dtype']})\")\n",
    "            \n",
    "            # Verificar quantiza√ß√£o\n",
    "            if 'quantization' in input_details[0]:\n",
    "                quant_info = input_details[0]['quantization']\n",
    "                if quant_info and len(quant_info) == 2:\n",
    "                    scale, zero_point = quant_info\n",
    "                    print(f\"   üéØ Quantiza√ß√£o: escala={scale}, zero_point={zero_point}\")\n",
    "            \n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro ao verificar {tflite_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FIM DO PIPELINE DE QUANTIZA√á√ÉO\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚úÖ Pipeline conclu√≠do com sucesso!\")\n",
    "print(f\"üìä Modelo TFLite criado: {len(tflite_quant_model) if 'tflite_quant_model' in locals() else 'N/A'} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4160f13",
   "metadata": {},
   "source": [
    "## 7. **Infer√™ncia em Produ√ß√£o (TFLite)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7272c6",
   "metadata": {},
   "source": [
    "A fun√ß√£o de infer√™ncia original foi mantida, mas agora √© poss√≠vel testar o modelo TFLite INT8 carregando-o com o interpretador TFLite (requer uma pequena modifica√ß√£o na fun√ß√£o load_inference_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66662338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd6c4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configura√ß√µes Globais ---\n",
    "_LOADED_MODEL = None\n",
    "_CLASS_NAMES = None\n",
    "_IMG_SIZE = (224, 224)  # Tamanho que o modelo espera\n",
    "_INTERPRETER = None\n",
    "_INPUT_DETAILS = None\n",
    "_OUTPUT_DETAILS = None\n",
    "_INPUT_SCALE = 1.0\n",
    "_INPUT_ZERO_POINT = 0\n",
    "_OUTPUT_SCALE = 1.0\n",
    "_OUTPUT_ZERO_POINT = 0\n",
    "\n",
    "# Patch dimensions para extra√ß√£o (tiles grandes da WSI)\n",
    "PATCH_WIDTH = 1024  # Tamanho dos tiles extra√≠dos\n",
    "PATCH_HEIGHT = 768   # Tamanho dos tiles extra√≠dos\n",
    "STRIDE_X = 340\n",
    "STRIDE_Y = 256\n",
    "CONTENT_THRESHOLD = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2f03e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURA√á√ïES FIXAS (SEM ENTRADA DO USU√ÅRIO) ---\n",
    "ROOT_PROJECT = Path(\"/home/ampliar/cancer-classify-citology/citology-pipeline-Train\")\n",
    "MODEL_PATH = ROOT_PROJECT / 'models' / 'qat_model.tflite'\n",
    "CLASS_NAMES = ['HISIL', 'LISIL', 'Normal']\n",
    "OUTPUT_DIR = ROOT_PROJECT / 'inferencias'\n",
    "\n",
    "# Imagem de teste padr√£o\n",
    "DEFAULT_IMAGE = ROOT_PROJECT / 'Dataset' / 'inteiras' / '3 Classes' / 'Tile' / 'HISIL' / 'HSIL_1 (2).jpg'\n",
    "\n",
    "# Configura√ß√µes de processamento\n",
    "SAVE_TILES = True  # Salvar tiles redimensionados\n",
    "SAVE_ORIGINAL_SIZE = False  # N√£o salvar tiles no tamanho original (apenas redimensionados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90325d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# FUN√á√ïES DE CARREGAMENTO DO MODELO TFLITE\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def load_inference_model():\n",
    "    \"\"\"Carrega o modelo TFLite globalmente uma √∫nica vez.\"\"\"\n",
    "    global _LOADED_MODEL, _CLASS_NAMES, _INTERPRETER\n",
    "    global _INPUT_DETAILS, _OUTPUT_DETAILS, _INPUT_SCALE, _INPUT_ZERO_POINT, _OUTPUT_SCALE, _OUTPUT_ZERO_POINT\n",
    "    \n",
    "    if _INTERPRETER is None:\n",
    "        print(f\"--- Carregando modelo TFLite para infer√™ncia de: {MODEL_PATH} ---\")\n",
    "        _CLASS_NAMES = CLASS_NAMES\n",
    "        \n",
    "        try:\n",
    "            model_path_str = str(MODEL_PATH)\n",
    "            \n",
    "            if Path(model_path_str).suffix == '.tflite':\n",
    "                # Carregar o interpretador TFLite\n",
    "                _INTERPRETER = tf.lite.Interpreter(model_path=model_path_str)\n",
    "                _INTERPRETER.allocate_tensors()\n",
    "                \n",
    "                # Obter detalhes de entrada/sa√≠da\n",
    "                _INPUT_DETAILS = _INTERPRETER.get_input_details()\n",
    "                _OUTPUT_DETAILS = _INTERPRETER.get_output_details()\n",
    "                \n",
    "                # Extrair informa√ß√µes de quantiza√ß√£o\n",
    "                input_quant = _INPUT_DETAILS[0]['quantization']\n",
    "                output_quant = _OUTPUT_DETAILS[0]['quantization']\n",
    "                \n",
    "                if input_quant and input_quant[0] != 0:\n",
    "                    _INPUT_SCALE, _INPUT_ZERO_POINT = input_quant\n",
    "                    print(f\"üìä Input quantization: scale={_INPUT_SCALE}, zero_point={_INPUT_ZERO_POINT}\")\n",
    "                else:\n",
    "                    print(\"üìä Input: Sem quantiza√ß√£o (float32 ou uint8 raw)\")\n",
    "                \n",
    "                if output_quant and output_quant[0] != 0:\n",
    "                    _OUTPUT_SCALE, _OUTPUT_ZERO_POINT = output_quant\n",
    "                    print(f\"üìä Output quantization: scale={_OUTPUT_SCALE}, zero_point={_OUTPUT_ZERO_POINT}\")\n",
    "                else:\n",
    "                    print(\"üìä Output: Sem quantiza√ß√£o\")\n",
    "                \n",
    "                print(f\"‚úÖ Modelo TFLite carregado:\")\n",
    "                print(f\"   Input: {_INPUT_DETAILS[0]['shape']} ({_INPUT_DETAILS[0]['dtype']})\")\n",
    "                print(f\"   Output: {_OUTPUT_DETAILS[0]['shape']} ({_OUTPUT_DETAILS[0]['dtype']})\")\n",
    "                print(f\"   Tiles extra√≠dos em: {PATCH_HEIGHT}x{PATCH_WIDTH}\")\n",
    "                print(f\"   Tiles redimensionados para: {_IMG_SIZE[0]}x{_IMG_SIZE[1]}\")\n",
    "                \n",
    "            else:\n",
    "                # Para modelos Keras (.keras)\n",
    "                _LOADED_MODEL = load_model(model_path_str, compile=False, custom_objects=quantize_scope()) \n",
    "                print(f\"‚úÖ Modelo Keras (FP32/QAT) carregado.\")\n",
    "                print(f\"   Tiles extra√≠dos em: {PATCH_HEIGHT}x{PATCH_WIDTH}\")\n",
    "                print(f\"   Tiles redimensionados para: {_IMG_SIZE[0]}x{_IMG_SIZE[1]}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERRO ao carregar o modelo: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "def preprocess_patch(patch_img: np.ndarray, is_tflite: bool) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pr√©-processa um patch BGR (OpenCV).\n",
    "    1. Extrai patch grande (PATCH_HEIGHT x PATCH_WIDTH)\n",
    "    2. Redimensiona para o tamanho do modelo (224x224)\n",
    "    3. Converte para formato apropriado (uint8/int8/float32)\n",
    "    \"\"\"\n",
    "    global _INPUT_SCALE, _INPUT_ZERO_POINT, _INPUT_DETAILS\n",
    "    \n",
    "    # 1. Converter BGR para RGB\n",
    "    rgb_patch = cv2.cvtColor(patch_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 2. REDIMENSIONAR para o tamanho que o modelo espera\n",
    "    target_width, target_height = _IMG_SIZE[1], _IMG_SIZE[0]  # (224, 224)\n",
    "    \n",
    "    if rgb_patch.shape[:2] != (target_height, target_width):\n",
    "        # Redimensiona para o tamanho do modelo\n",
    "        rgb_patch = cv2.resize(rgb_patch, (target_width, target_height))\n",
    "    \n",
    "    if is_tflite:\n",
    "        # Para modelos TFLite\n",
    "        input_dtype = _INPUT_DETAILS[0]['dtype']\n",
    "        \n",
    "        if input_dtype == np.uint8:\n",
    "            # Modelo espera UINT8 (0-255) - comum para QAT INT8\n",
    "            img_array = np.expand_dims(rgb_patch, axis=0).astype(np.uint8)\n",
    "            return img_array\n",
    "            \n",
    "        elif input_dtype == np.int8:\n",
    "            # Modelo espera INT8 (-128 to 127)\n",
    "            img_array = np.expand_dims(rgb_patch, axis=0).astype(np.float32)\n",
    "            \n",
    "            if _INPUT_SCALE != 0:\n",
    "                # Aplicar quantiza√ß√£o\n",
    "                img_quantized = img_array / _INPUT_SCALE + _INPUT_ZERO_POINT\n",
    "                img_array = np.clip(img_quantized, -128, 127).astype(np.int8)\n",
    "            else:\n",
    "                # Converter de 0-255 para -128-127\n",
    "                img_array = (img_array - 128).astype(np.int8)\n",
    "            \n",
    "            return img_array\n",
    "            \n",
    "        else:\n",
    "            # Float32 - normalizar para 0-1\n",
    "            img_array = np.expand_dims(rgb_patch, axis=0).astype(np.float32)\n",
    "            return img_array / 255.0\n",
    "            \n",
    "    else:\n",
    "        # Para modelos Keras (FP32 ou QAT simulado)\n",
    "        img_array = np.expand_dims(rgb_patch, axis=0).astype(np.float32)\n",
    "        return img_array / 255.0\n",
    "\n",
    "def dequantize_output(output_tensor: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Desquantiza a sa√≠da do modelo TFLite se necess√°rio.\"\"\"\n",
    "    global _OUTPUT_SCALE, _OUTPUT_ZERO_POINT\n",
    "    \n",
    "    if _OUTPUT_SCALE != 0 and _OUTPUT_SCALE != 1.0:\n",
    "        # Aplicar desquantiza√ß√£o: (output - zero_point) * scale\n",
    "        output_float = (output_tensor.astype(np.float32) - _OUTPUT_ZERO_POINT) * _OUTPUT_SCALE\n",
    "        return output_float\n",
    "    else:\n",
    "        return output_tensor.astype(np.float32)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FUN√á√ÉO PRINCIPAL DE INFER√äNCIA COM TILING E REDIMENSIONAMENTO\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def process_and_infer(\n",
    "    image_path: str = None,\n",
    "    save_tiles: bool = SAVE_TILES,\n",
    "    save_original_size: bool = SAVE_ORIGINAL_SIZE\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Processa uma imagem grande (WSI):\n",
    "    1. Extrai tiles grandes (PATCH_HEIGHT x PATCH_WIDTH)\n",
    "    2. Redimensiona cada tile para o tamanho do modelo (224x224)\n",
    "    3. Realiza infer√™ncia com TFLite\n",
    "    4. Agrega resultados por m√©dia\n",
    "    5. Salva tiles redimensionados na pasta inferencias\n",
    "    \n",
    "    Args:\n",
    "        image_path: Caminho para a imagem (se None, usa DEFAULT_IMAGE)\n",
    "        save_tiles: Se True, salva os tiles redimensionados\n",
    "        save_original_size: Se True, salva tamb√©m os tiles no tamanho original\n",
    "    \"\"\"\n",
    "    global _LOADED_MODEL, _CLASS_NAMES, _IMG_SIZE, _INTERPRETER\n",
    "    global _INPUT_DETAILS, _OUTPUT_DETAILS\n",
    "    \n",
    "    if _LOADED_MODEL is None and _INTERPRETER is None:\n",
    "        raise ValueError(\"Modelo n√£o carregado. Chame load_inference_model() primeiro.\")\n",
    "    \n",
    "    # Usar imagem padr√£o se n√£o for fornecida\n",
    "    if image_path is None:\n",
    "        image_path = str(DEFAULT_IMAGE)\n",
    "    \n",
    "    # Verifica o tipo de modelo\n",
    "    is_tflite = _INTERPRETER is not None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    inference_times = []\n",
    "    \n",
    "    # --- 1. CONFIGURA√á√ÉO DE CAMINHOS E DIRET√ìRIOS ---\n",
    "    image_name = Path(image_path).stem\n",
    "    image_output_dir = OUTPUT_DIR / image_name\n",
    "    \n",
    "    # Criar diret√≥rio ANTES de qualquer opera√ß√£o\n",
    "    os.makedirs(image_output_dir, exist_ok=True)\n",
    "    \n",
    "    if save_tiles:\n",
    "        # Criar subdiret√≥rios organizados\n",
    "        tiles_dir = image_output_dir / \"tiles_redimensionados\"  # Tiles redimensionados\n",
    "        os.makedirs(tiles_dir, exist_ok=True)\n",
    "        \n",
    "        if save_original_size:\n",
    "            orig_tiles_dir = image_output_dir / \"tiles_originais\"  # Tiles originais\n",
    "            os.makedirs(orig_tiles_dir, exist_ok=True)\n",
    "    \n",
    "    # --- 2. CARREGAMENTO E INICIALIZA√á√ÉO ---\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Erro: N√£o foi poss√≠vel ler a imagem em {image_path}\")\n",
    "\n",
    "    # Tamanho dos tiles para extra√ß√£o\n",
    "    H, W = PATCH_HEIGHT, PATCH_WIDTH\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    height, width, _ = img.shape\n",
    "    \n",
    "    total_patches = 0\n",
    "    good_patches = 0\n",
    "    all_probabilities = []\n",
    "    pre_process_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"--- Iniciando Processamento ({width}x{height}) ---\")\n",
    "    print(f\"üîπ Imagem: {Path(image_path).name}\")\n",
    "    print(f\"üîπ Extraindo tiles de {PATCH_HEIGHT}x{PATCH_WIDTH} pixels\")\n",
    "    print(f\"üîπ Redimensionando para {_IMG_SIZE[0]}x{_IMG_SIZE[1]} pixels para infer√™ncia\")\n",
    "    print(f\"üîπ Modelo: {'TFLite INT8' if is_tflite else 'Keras FP32/QAT'}\")\n",
    "    if save_tiles:\n",
    "        print(f\"üîπ Salvando tiles em: {tiles_dir}\")\n",
    "\n",
    "    # --- 3. TILING, REDIMENSIONAMENTO E INFER√äNCIA ---\n",
    "    # Ajustar limites para n√£o ultrapassar dimens√µes da imagem\n",
    "    max_y = max(0, height - H)\n",
    "    max_x = max(0, width - W)\n",
    "    \n",
    "    tile_counter = 0\n",
    "    \n",
    "    for y in range(0, max_y + 1, STRIDE_Y):\n",
    "        for x in range(0, max_x + 1, STRIDE_X):\n",
    "            total_patches += 1\n",
    "            \n",
    "            # Garantir que n√£o ultrapasse os limites\n",
    "            y_end = min(y + H, height)\n",
    "            x_end = min(x + W, width)\n",
    "            \n",
    "            # Extrair o patch no tamanho ORIGINAL (grande)\n",
    "            patch_color = img[y:y_end, x:x_end]\n",
    "            patch_gray = gray[y:y_end, x:x_end]\n",
    "            \n",
    "            # Verificar se o patch tem tamanho m√≠nimo razo√°vel\n",
    "            if patch_gray.shape[0] < H * 0.5 or patch_gray.shape[1] < W * 0.5:\n",
    "                continue\n",
    "                \n",
    "            # Filtra por conte√∫do (desvio padr√£o)\n",
    "            std_dev = np.std(patch_gray)\n",
    "            \n",
    "            if std_dev > CONTENT_THRESHOLD:\n",
    "                good_patches += 1\n",
    "                tile_counter += 1\n",
    "                \n",
    "                try:\n",
    "                    # A. PR√â-PROCESSAMENTO (inclui REDIMENSIONAMENTO)\n",
    "                    input_tensor = preprocess_patch(patch_color, is_tflite)\n",
    "                    \n",
    "                    # Verificar shape\n",
    "                    expected_shape = (1, _IMG_SIZE[0], _IMG_SIZE[1], 3)\n",
    "                    if input_tensor.shape != expected_shape:\n",
    "                        print(f\"‚ö†Ô∏è  Shape incorreto: esperado {expected_shape}, obtido {input_tensor.shape}\")\n",
    "                        print(f\"   Patch original: {patch_color.shape}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # B. INFER√äNCIA\n",
    "                    inf_start = time.time()\n",
    "                    \n",
    "                    if is_tflite:\n",
    "                        _INTERPRETER.set_tensor(_INPUT_DETAILS[0]['index'], input_tensor)\n",
    "                        _INTERPRETER.invoke()\n",
    "                        output_tensor = _INTERPRETER.get_tensor(_OUTPUT_DETAILS[0]['index'])\n",
    "                        \n",
    "                        # Desquantizar se necess√°rio\n",
    "                        predictions = dequantize_output(output_tensor)\n",
    "                        \n",
    "                    else:\n",
    "                        predictions = _LOADED_MODEL.predict(input_tensor, verbose=0)\n",
    "                    \n",
    "                    inf_end = time.time()\n",
    "                    inference_times.append(inf_end - inf_start)\n",
    "                    \n",
    "                    # Processar predi√ß√µes\n",
    "                    if len(predictions.shape) > 1:\n",
    "                        predictions = predictions[0]\n",
    "                    \n",
    "                    all_probabilities.append(predictions)\n",
    "                    \n",
    "                    # C. SALVAR TILE REDIMENSIONADO (sempre na pasta inferencias)\n",
    "                    if save_tiles:\n",
    "                        # Nome do arquivo com informa√ß√µes √∫teis\n",
    "                        tile_filename = f\"tile_{tile_counter:04d}_x{x}_y{y}_std{std_dev:.1f}_size{_IMG_SIZE[1]}x{_IMG_SIZE[0]}.jpg\"\n",
    "                        tile_path = tiles_dir / tile_filename\n",
    "                        \n",
    "                        # Salvar tile redimensionado\n",
    "                        if is_tflite and _INPUT_DETAILS[0]['dtype'] == np.uint8:\n",
    "                            resized_img = input_tensor[0]  # J√° est√° em uint8\n",
    "                        else:\n",
    "                            resized_img = (input_tensor[0] * 255).astype(np.uint8)\n",
    "                        \n",
    "                        resized_img_bgr = cv2.cvtColor(resized_img, cv2.COLOR_RGB2BGR)\n",
    "                        cv2.imwrite(str(tile_path), resized_img_bgr)\n",
    "                        \n",
    "                        # Salvar tile original se solicitado\n",
    "                        if save_original_size:\n",
    "                            orig_filename = f\"tile_orig_{tile_counter:04d}_x{x}_y{y}_size{patch_color.shape[1]}x{patch_color.shape[0]}.jpg\"\n",
    "                            orig_path = orig_tiles_dir / orig_filename\n",
    "                            cv2.imwrite(str(orig_path), patch_color)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Erro no patch ({x},{y}): {e}\")\n",
    "                    continue\n",
    "    \n",
    "    # --- 4. AGREGA√á√ÉO DE RESULTADOS ---\n",
    "    if good_patches > 0 and len(all_probabilities) > 0:\n",
    "        # Calcular probabilidade m√©dia\n",
    "        avg_probabilities = np.mean(all_probabilities, axis=0)\n",
    "        \n",
    "        # Aplicar softmax se n√£o estiver normalizado\n",
    "        if np.sum(avg_probabilities) > 1.1:  # Se soma > 1.1, aplicar softmax\n",
    "            exp_probs = np.exp(avg_probabilities - np.max(avg_probabilities))\n",
    "            avg_probabilities = exp_probs / np.sum(exp_probs)\n",
    "        \n",
    "        # Obter classe predita\n",
    "        predicted_class_idx = np.argmax(avg_probabilities)\n",
    "        predicted_class = _CLASS_NAMES[predicted_class_idx]\n",
    "        confidence = avg_probabilities[predicted_class_idx]\n",
    "        \n",
    "        # Calcular distribui√ß√£o de classes\n",
    "        class_distribution = {\n",
    "            _CLASS_NAMES[i]: float(avg_probabilities[i]) \n",
    "            for i in range(len(_CLASS_NAMES))\n",
    "        }\n",
    "    else:\n",
    "        predicted_class = \"Nenhum patch v√°lido encontrado\"\n",
    "        confidence = 0.0\n",
    "        class_distribution = {}\n",
    "    \n",
    "    # --- 5. C√ÅLCULO DE TEMPOS ---\n",
    "    total_inference_time = sum(inference_times)\n",
    "    avg_inference_time = total_inference_time / good_patches if good_patches > 0 else 0\n",
    "    total_runtime = time.time() - start_time\n",
    "    \n",
    "    # --- 6. RESULTADO FINAL ---\n",
    "    result = {\n",
    "        \"image_name\": image_name,\n",
    "        \"image_path\": str(image_path),\n",
    "        \"image_dimensions\": f\"{width}x{height}\",\n",
    "        \"total_patches\": total_patches,\n",
    "        \"valid_patches\": good_patches,\n",
    "        \"predicted_class\": predicted_class,\n",
    "        \"confidence\": float(confidence),\n",
    "        \"class_distribution\": class_distribution,\n",
    "        \"processing_info\": {\n",
    "            \"tile_extraction_size\": f\"{PATCH_HEIGHT}x{PATCH_WIDTH}\",\n",
    "            \"model_input_size\": f\"{_IMG_SIZE[0]}x{_IMG_SIZE[1]}\",\n",
    "            \"resize_applied\": True,\n",
    "            \"model_type\": \"TFLite INT8\" if is_tflite else \"Keras\",\n",
    "            \"tiles_saved\": save_tiles,\n",
    "            \"tiles_saved_count\": tile_counter if save_tiles else 0,\n",
    "            \"tiles_directory\": str(tiles_dir) if save_tiles else None\n",
    "        },\n",
    "        \"time_breakdown\": {\n",
    "            \"preprocessing_s\": pre_process_time,\n",
    "            \"total_inference_time_s\": total_inference_time,\n",
    "            \"avg_inference_per_patch_ms\": avg_inference_time * 1000,\n",
    "            \"total_runtime_s\": total_runtime\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Salvar resultados em JSON na pasta inferencias\n",
    "    result_file = image_output_dir / \"inference_result.json\"\n",
    "    try:\n",
    "        with open(result_file, 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        print(f\"‚úÖ Resultados salvos em: {result_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  N√£o foi poss√≠vel salvar JSON: {e}\")\n",
    "    \n",
    "    # Salvar um resumo mais leg√≠vel\n",
    "    summary_file = image_output_dir / \"summary.txt\"\n",
    "    try:\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(f\"RESUMO DA INFER√äNCIA\\n\")\n",
    "            f.write(f\"=\"*50 + \"\\n\")\n",
    "            f.write(f\"Imagem: {image_name}\\n\")\n",
    "            f.write(f\"Dimens√µes: {width}x{height}\\n\")\n",
    "            f.write(f\"Patches extra√≠dos: {total_patches}\\n\")\n",
    "            f.write(f\"Patches v√°lidos: {good_patches}\\n\")\n",
    "            f.write(f\"Classe predita: {predicted_class}\\n\")\n",
    "            f.write(f\"Confian√ßa: {confidence:.2%}\\n\")\n",
    "            f.write(f\"\\nDistribui√ß√£o:\\n\")\n",
    "            for class_name, prob in class_distribution.items():\n",
    "                f.write(f\"  {class_name}: {prob:.2%}\\n\")\n",
    "            f.write(f\"\\nTempos:\\n\")\n",
    "            f.write(f\"  Pr√©-processamento: {pre_process_time:.2f}s\\n\")\n",
    "            f.write(f\"  Infer√™ncia total: {total_inference_time:.2f}s\\n\")\n",
    "            f.write(f\"  M√©dia por patch: {avg_inference_time*1000:.2f}ms\\n\")\n",
    "            f.write(f\"  Tempo total: {total_runtime:.2f}s\\n\")\n",
    "            if save_tiles:\n",
    "                f.write(f\"\\nTiles salvos: {tile_counter}\\n\")\n",
    "                f.write(f\"Local: {tiles_dir}\\n\")\n",
    "        print(f\"üìÑ Resumo salvo em: {summary_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  N√£o foi poss√≠vel salvar resumo: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FUN√á√ïES DE TESTE E EXECU√á√ÉO\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"Teste r√°pido do pipeline TFLite.\"\"\"\n",
    "    global _INTERPRETER, _INPUT_DETAILS, _OUTPUT_DETAILS\n",
    "    \n",
    "    print(\"üß™ TESTE R√ÅPIDO DO PIPELINE TFLITE...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Verificar se arquivos existem\n",
    "    if not MODEL_PATH.exists():\n",
    "        print(f\"‚ùå Modelo n√£o encontrado: {MODEL_PATH}\")\n",
    "        return\n",
    "    \n",
    "    if not DEFAULT_IMAGE.exists():\n",
    "        print(f\"‚ùå Imagem padr√£o n√£o encontrada: {DEFAULT_IMAGE}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # 1. Carregar modelo\n",
    "        load_inference_model()\n",
    "        \n",
    "        # 2. Testar com uma √∫nica imagem\n",
    "        print(\"\\nüîç Testando pr√©-processamento e infer√™ncia direta...\")\n",
    "        img = cv2.imread(str(DEFAULT_IMAGE))\n",
    "        \n",
    "        if img is None:\n",
    "            print(\"‚ùå N√£o foi poss√≠vel ler a imagem\")\n",
    "            return\n",
    "        \n",
    "        # Extrair um patch para teste\n",
    "        patch = img[0:PATCH_HEIGHT, 0:PATCH_WIDTH] if img.shape[0] > PATCH_HEIGHT and img.shape[1] > PATCH_WIDTH else img\n",
    "        \n",
    "        print(f\"üìê Patch original: {patch.shape}\")\n",
    "        \n",
    "        # Pr√©-processar (vai redimensionar para 224x224)\n",
    "        input_tensor = preprocess_patch(patch, is_tflite=True)\n",
    "        \n",
    "        print(f\"üìä Tensor ap√≥s pr√©-processamento:\")\n",
    "        print(f\"   Formato: {input_tensor.shape}\")\n",
    "        print(f\"   Tipo: {input_tensor.dtype}\")\n",
    "        print(f\"   Faixa: [{input_tensor.min()}, {input_tensor.max()}]\")\n",
    "        \n",
    "        # Infer√™ncia\n",
    "        if _INTERPRETER:\n",
    "            _INTERPRETER.set_tensor(_INPUT_DETAILS[0]['index'], input_tensor)\n",
    "            _INTERPRETER.invoke()\n",
    "            output_tensor = _INTERPRETER.get_tensor(_OUTPUT_DETAILS[0]['index'])\n",
    "            \n",
    "            # Desquantizar\n",
    "            predictions = dequantize_output(output_tensor)[0]\n",
    "            \n",
    "            print(f\"\\nüéØ RESULTADO DO TESTE:\")\n",
    "            for i, class_name in enumerate(CLASS_NAMES):\n",
    "                print(f\"   {class_name}: {predictions[i]:.4f}\")\n",
    "            \n",
    "            predicted_idx = np.argmax(predictions)\n",
    "            print(f\"   ‚úÖ Classe predita: {CLASS_NAMES[predicted_idx]} ({predictions[predicted_idx]:.2%})\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Teste r√°pido conclu√≠do!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERRO no teste: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def run_full_inference(image_path: str = None):\n",
    "    \"\"\"\n",
    "    Executa a infer√™ncia completa com tiling e salva resultados.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Caminho para a imagem (se None, usa DEFAULT_IMAGE)\n",
    "    \"\"\"\n",
    "    print(\"üöÄ EXECUTANDO INFER√äNCIA COMPLETA COM TILING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Verificar se arquivos existem\n",
    "    if not MODEL_PATH.exists():\n",
    "        print(f\"‚ùå Modelo n√£o encontrado: {MODEL_PATH}\")\n",
    "        return\n",
    "    \n",
    "    # Determinar qual imagem usar\n",
    "    if image_path is None:\n",
    "        image_path = str(DEFAULT_IMAGE)\n",
    "    \n",
    "    if not Path(image_path).exists():\n",
    "        print(f\"‚ùå Imagem n√£o encontrada: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Garantir que o diret√≥rio de sa√≠da existe\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # 1. Carregar modelo\n",
    "        load_inference_model()\n",
    "        \n",
    "        # 2. Executar infer√™ncia completa\n",
    "        print(f\"\\nüì∑ Processando imagem: {Path(image_path).name}\")\n",
    "        print(f\"üìê Tiles: {PATCH_HEIGHT}x{PATCH_WIDTH} ‚Üí 224x224\")\n",
    "        print(f\"üíæ Salvando tiles em: {OUTPUT_DIR}\")\n",
    "        \n",
    "        result = process_and_infer(\n",
    "            image_path=image_path,\n",
    "            save_tiles=SAVE_TILES,\n",
    "            save_original_size=SAVE_ORIGINAL_SIZE\n",
    "        )\n",
    "        \n",
    "        # 3. Mostrar resultados\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"‚ú® RESULTADO FINAL ‚ú®\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"üì∑ Imagem: {result['image_name']}\")\n",
    "        print(f\"üìê Dimens√µes: {result['image_dimensions']}\")\n",
    "        print(f\"üß© Patches extra√≠dos: {result['total_patches']}\")\n",
    "        print(f\"‚úÖ Patches v√°lidos: {result['valid_patches']}\")\n",
    "        print(f\"üè∑Ô∏è  Classe: {result['predicted_class']}\")\n",
    "        print(f\"üìä Confian√ßa: {result['confidence']:.2%}\")\n",
    "        \n",
    "        if result['class_distribution']:\n",
    "            print(\"\\nüìà Distribui√ß√£o:\")\n",
    "            for class_name, prob in result['class_distribution'].items():\n",
    "                print(f\"   {class_name}: {prob:.2%}\")\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è  Tempos:\")\n",
    "        print(f\"   Pr√©-processamento: {result['time_breakdown']['preprocessing_s']:.2f}s\")\n",
    "        print(f\"   Infer√™ncia total: {result['time_breakdown']['total_inference_time_s']:.2f}s\")\n",
    "        print(f\"   M√©dia por patch: {result['time_breakdown']['avg_inference_per_patch_ms']:.2f}ms\")\n",
    "        print(f\"   Tempo total: {result['time_breakdown']['total_runtime_s']:.2f}s\")\n",
    "        \n",
    "        if SAVE_TILES and 'processing_info' in result and 'tiles_saved_count' in result['processing_info']:\n",
    "            print(f\"\\nüíæ Tiles salvos: {result['processing_info']['tiles_saved_count']}\")\n",
    "            print(f\"üìÅ Local: {result['processing_info']['tiles_directory']}\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(f\"‚úÖ Infer√™ncia conclu√≠da! Resultados em: {OUTPUT_DIR / result['image_name']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERRO: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def process_multiple_images(image_paths: list):\n",
    "    \"\"\"\n",
    "    Processa m√∫ltiplas imagens em sequ√™ncia.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: Lista de caminhos para imagens\n",
    "    \"\"\"\n",
    "    print(\"üîÑ PROCESSANDO M√öLTIPLAS IMAGENS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Garantir que o diret√≥rio de sa√≠da existe\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Carregar modelo uma vez\n",
    "    load_inference_model()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(f\"\\n[{i+1}/{len(image_paths)}] Processando: {Path(image_path).name}\")\n",
    "        \n",
    "        if not Path(image_path).exists():\n",
    "            print(f\"   ‚ùå Imagem n√£o encontrada: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            result = process_and_infer(\n",
    "                image_path=image_path,\n",
    "                save_tiles=SAVE_TILES,\n",
    "                save_original_size=SAVE_ORIGINAL_SIZE\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"   ‚úÖ Classe: {result['predicted_class']} ({result['confidence']:.2%})\")\n",
    "            print(f\"   ‚è±Ô∏è  Tempo: {result['time_breakdown']['total_runtime_s']:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Salvar resumo de todas as imagens\n",
    "    if results:\n",
    "        summary = {\n",
    "            \"total_images\": len(image_paths),\n",
    "            \"processed_successfully\": len(results),\n",
    "            \"results\": results,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        summary_file = OUTPUT_DIR / \"batch_summary.json\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüìä RESUMO DO LOTE salvo em: {summary_file}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# EXECU√á√ÉO AUTOM√ÅTICA (SEM ENTRADA DO USU√ÅRIO)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fun√ß√£o principal para execu√ß√£o autom√°tica.\"\"\"\n",
    "    \n",
    "    print(\"üî¨ SISTEMA DE INFER√äNCIA TFLITE COM TILING\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìê Tiles extra√≠dos: {PATCH_HEIGHT}x{PATCH_WIDTH} pixels\")\n",
    "    print(f\"üîÑ Redimensionados para: 224x224 pixels\")\n",
    "    print(f\"üíæ Sa√≠da: {OUTPUT_DIR}\")\n",
    "    print(f\"üî¢ Classes: {', '.join(CLASS_NAMES)}\")\n",
    "    \n",
    "    # Verificar se os arquivos necess√°rios existem\n",
    "    if not MODEL_PATH.exists():\n",
    "        print(f\"\\n‚ùå ERRO: Modelo n√£o encontrado em {MODEL_PATH}\")\n",
    "        return\n",
    "    \n",
    "    if not DEFAULT_IMAGE.exists():\n",
    "        print(f\"\\n‚ö†Ô∏è  AVISO: Imagem padr√£o n√£o encontrada em {DEFAULT_IMAGE}\")\n",
    "        print(\"Por favor, forne√ßa um caminho de imagem v√°lido.\")\n",
    "        return\n",
    "    \n",
    "    # Executar pipeline completo\n",
    "    print(f\"\\nüöÄ Iniciando pipeline de infer√™ncia...\")\n",
    "    \n",
    "    # 1. Teste r√°pido\n",
    "    quick_test()\n",
    "    \n",
    "    # 2. Infer√™ncia completa na imagem padr√£o\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    run_full_inference()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pipeline conclu√≠do com sucesso!\")\n",
    "    print(f\"üìÅ Todos os resultados est√£o em: {OUTPUT_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "238148fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ SISTEMA DE INFER√äNCIA TFLITE COM TILING\n",
      "============================================================\n",
      "üìê Tiles extra√≠dos: 768x1024 pixels\n",
      "üîÑ Redimensionados para: 224x224 pixels\n",
      "üíæ Sa√≠da: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias\n",
      "üî¢ Classes: HISIL, LISIL, Normal\n",
      "\n",
      "üöÄ Iniciando pipeline de infer√™ncia...\n",
      "üß™ TESTE R√ÅPIDO DO PIPELINE TFLITE...\n",
      "==================================================\n",
      "--- Carregando modelo TFLite para infer√™ncia de: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/qat_model.tflite ---\n",
      "üìä Input quantization: scale=0.003921568859368563, zero_point=0\n",
      "üìä Output quantization: scale=0.00390625, zero_point=0\n",
      "‚úÖ Modelo TFLite carregado:\n",
      "   Input: [  1 224 224   3] (<class 'numpy.uint8'>)\n",
      "   Output: [1 3] (<class 'numpy.uint8'>)\n",
      "   Tiles extra√≠dos em: 768x1024\n",
      "   Tiles redimensionados para: 224x224\n",
      "\n",
      "üîç Testando pr√©-processamento e infer√™ncia direta...\n",
      "üìê Patch original: (768, 1024, 3)\n",
      "üìä Tensor ap√≥s pr√©-processamento:\n",
      "   Formato: (1, 224, 224, 3)\n",
      "   Tipo: uint8\n",
      "   Faixa: [43, 253]\n",
      "\n",
      "üéØ RESULTADO DO TESTE:\n",
      "   HISIL: 0.9961\n",
      "   LISIL: 0.0000\n",
      "   Normal: 0.0000\n",
      "   ‚úÖ Classe predita: HISIL (99.61%)\n",
      "\n",
      "‚úÖ Teste r√°pido conclu√≠do!\n",
      "\n",
      "============================================================\n",
      "üöÄ EXECUTANDO INFER√äNCIA COMPLETA COM TILING\n",
      "============================================================\n",
      "\n",
      "üì∑ Processando imagem: HSIL_1 (2).jpg\n",
      "üìê Tiles: 768x1024 ‚Üí 224x224\n",
      "üíæ Salvando tiles em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias\n",
      "--- Iniciando Processamento (2048x1536) ---\n",
      "üîπ Imagem: HSIL_1 (2).jpg\n",
      "üîπ Extraindo tiles de 768x1024 pixels\n",
      "üîπ Redimensionando para 224x224 pixels para infer√™ncia\n",
      "üîπ Modelo: TFLite INT8\n",
      "üîπ Salvando tiles em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias/HSIL_1 (2)/tiles_redimensionados\n",
      "‚úÖ Resultados salvos em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias/HSIL_1 (2)/inference_result.json\n",
      "üìÑ Resumo salvo em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias/HSIL_1 (2)/summary.txt\n",
      "\n",
      "==================================================\n",
      "‚ú® RESULTADO FINAL ‚ú®\n",
      "==================================================\n",
      "üì∑ Imagem: HSIL_1 (2)\n",
      "üìê Dimens√µes: 2048x1536\n",
      "üß© Patches extra√≠dos: 16\n",
      "‚úÖ Patches v√°lidos: 16\n",
      "üè∑Ô∏è  Classe: HISIL\n",
      "üìä Confian√ßa: 99.61%\n",
      "\n",
      "üìà Distribui√ß√£o:\n",
      "   HISIL: 99.61%\n",
      "   LISIL: 0.00%\n",
      "   Normal: 0.00%\n",
      "\n",
      "‚è±Ô∏è  Tempos:\n",
      "   Pr√©-processamento: 0.03s\n",
      "   Infer√™ncia total: 1.80s\n",
      "   M√©dia por patch: 112.39ms\n",
      "   Tempo total: 1.90s\n",
      "\n",
      "üíæ Tiles salvos: 16\n",
      "üìÅ Local: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias/HSIL_1 (2)/tiles_redimensionados\n",
      "==================================================\n",
      "‚úÖ Infer√™ncia conclu√≠da! Resultados em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias/HSIL_1 (2)\n",
      "\n",
      "‚úÖ Pipeline conclu√≠do com sucesso!\n",
      "üìÅ Todos os resultados est√£o em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias\n",
      "üöÄ EXECUTANDO INFER√äNCIA COMPLETA COM TILING\n",
      "============================================================\n",
      "\n",
      "üì∑ Processando imagem: HSIL_1 (8).jpg\n",
      "üìê Tiles: 768x1024 ‚Üí 224x224\n",
      "üíæ Salvando tiles em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias\n",
      "--- Iniciando Processamento (2048x1536) ---\n",
      "üîπ Imagem: HSIL_1 (8).jpg\n",
      "üîπ Extraindo tiles de 768x1024 pixels\n",
      "üîπ Redimensionando para 224x224 pixels para infer√™ncia\n",
      "üîπ Modelo: TFLite INT8\n",
      "üîπ Salvando tiles em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias/HSIL_1 (8)/tiles_redimensionados\n",
      "‚úÖ Resultados salvos em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias/HSIL_1 (8)/inference_result.json\n",
      "üìÑ Resumo salvo em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias/HSIL_1 (8)/summary.txt\n",
      "\n",
      "==================================================\n",
      "‚ú® RESULTADO FINAL ‚ú®\n",
      "==================================================\n",
      "üì∑ Imagem: HSIL_1 (8)\n",
      "üìê Dimens√µes: 2048x1536\n",
      "üß© Patches extra√≠dos: 16\n",
      "‚úÖ Patches v√°lidos: 16\n",
      "üè∑Ô∏è  Classe: HISIL\n",
      "üìä Confian√ßa: 99.61%\n",
      "\n",
      "üìà Distribui√ß√£o:\n",
      "   HISIL: 99.61%\n",
      "   LISIL: 0.00%\n",
      "   Normal: 0.00%\n",
      "\n",
      "‚è±Ô∏è  Tempos:\n",
      "   Pr√©-processamento: 0.03s\n",
      "   Infer√™ncia total: 0.48s\n",
      "   M√©dia por patch: 29.70ms\n",
      "   Tempo total: 0.56s\n",
      "\n",
      "üíæ Tiles salvos: 16\n",
      "üìÅ Local: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias/HSIL_1 (8)/tiles_redimensionados\n",
      "==================================================\n",
      "‚úÖ Infer√™ncia conclu√≠da! Resultados em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias/HSIL_1 (8)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Exemplo 1: Execu√ß√£o autom√°tica com imagem padr√£o\n",
    "    main()\n",
    "    \n",
    "    # Exemplo 2: Processar imagem espec√≠fica\n",
    "    IMAGEM_ESPECIFICA = \"/home/ampliar/cancer-classify-citology/citology-pipeline-Train/Dataset/inteiras/3 Classes/Tile/HISIL/HSIL_1 (8).jpg\"\n",
    "    run_full_inference(IMAGEM_ESPECIFICA)\n",
    "    \n",
    "    # Exemplo 3: Processar m√∫ltiplas imagens\n",
    "    # IMAGENS = [\n",
    "    #     \"/caminho/para/imagem1.jpg\",\n",
    "    #     \"/caminho/para/imagem2.jpg\",\n",
    "    #     \"/caminho/para/imagem3.jpg\"\n",
    "    # ]\n",
    "    # process_multiple_images(IMAGENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a48b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qat_cancer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
