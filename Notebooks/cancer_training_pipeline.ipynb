{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110baa0f",
   "metadata": {},
   "source": [
    "## 1. **Configura√ß√£o de Ambiente e Reprodu√ß√£o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "998971b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports e configura√ß√µes reproduc√≠veis\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1317e6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "üî• Usando estrat√©gia de distribui√ß√£o com 1 GPU(s).\n",
      "\n",
      "Configura√ß√µes de Paths:\n",
      "DATA_DIR = /home/ampliar/cancer-classify-citology/citology-pipeline-Train/Dataset/pre-processado\n",
      "MODELS_DIR = /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models\n",
      "BATCH_SIZE ESTRAT√âGICO = 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configura√ß√£o de Reprodu√ß√£o ---\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# --- Configura√ß√£o de GPU/Dispositivo ---\n",
    "# Verifica e configura o TensorFlow para usar a GPU, se dispon√≠vel\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Limita o crescimento da mem√≥ria da GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Define a estrat√©gia de distribui√ß√£o para uso de GPU(s)\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(f\"üî• Usando estrat√©gia de distribui√ß√£o com {len(gpus)} GPU(s).\")\n",
    "    except RuntimeError as e:\n",
    "        # Deve ser chamado antes que os dispositivos l√≥gicos sejam configurados\n",
    "        print(e)\n",
    "        strategy = tf.distribute.get_strategy() # Estrat√©gia padr√£o\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # Estrat√©gia padr√£o (CPU/TPU)\n",
    "    print(\"‚ö†Ô∏è Nenhuma GPU detectada. Usando a estrat√©gia padr√£o.\")\n",
    "\n",
    "# Vari√°veis para a etapa de treino, onde o modelo √© constru√≠do e compilado dentro do escopo da estrat√©gia\n",
    "strategy_scope_used = len(gpus) > 0\n",
    "\n",
    "# --- Paths e Configura√ß√µes Globais ---\n",
    "ROOT = Path(\"/home/ampliar/cancer-classify-citology/citology-pipeline-Train\")\n",
    "DATA_DIR = ROOT / 'Dataset' / 'pre-processado'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_CSV = DATA_DIR / 'train_data.csv'\n",
    "VAL_CSV = DATA_DIR / 'val_data.csv'\n",
    "TEST_CSV = DATA_DIR / 'test_data.csv'\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync # Ajusta BATCH_SIZE para m√∫ltiplas GPUs\n",
    "EPOCHS_BASELINE = 10\n",
    "EPOCHS_ROBUST = 20\n",
    "EPOCHS_FINETUNE = 10\n",
    "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "IMAGE_ROOT = DATA_DIR # Diret√≥rio raiz para as imagens\n",
    "\n",
    "print('\\nConfigura√ß√µes de Paths:')\n",
    "print('DATA_DIR =', DATA_DIR)\n",
    "print('MODELS_DIR =', MODELS_DIR)\n",
    "print('BATCH_SIZE ESTRAT√âGICO =', BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887a46b",
   "metadata": {},
   "source": [
    "## 2. **Carregamento de Dados e Geradores**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dcf232",
   "metadata": {},
   "source": [
    "Assegura o carregamento dos DataFrames e a cria√ß√£o dos geradores de dados para alimentar o modelo. A separa√ß√£o dos geradores com e sem augmentation permite uma compara√ß√£o justa entre as etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deea4843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribui√ß√£o de Dados:\n",
      "Treino: 24951 | Valida√ß√£o: 5347 | Teste: 5347\n",
      "N√∫mero de Classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Carregar CSVs e verificar\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df = pd.read_csv(VAL_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print('\\nDistribui√ß√£o de Dados:')\n",
    "print(f'Treino: {len(train_df)} | Valida√ß√£o: {len(val_df)} | Teste: {len(test_df)}')\n",
    "num_classes = train_df['lesion_type'].nunique()\n",
    "print(f'N√∫mero de Classes: {num_classes}')\n",
    "\n",
    "# --- Fun√ß√µes Utilit√°rias: Geradores e Construtor de Modelo ---\n",
    "def make_generators(train_df, val_df, test_df, image_root=IMAGE_ROOT, augment=False, batch_size=BATCH_SIZE, img_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "    \"\"\"Cria ImageDataGenerators e retorna (train_gen, val_gen, test_gen).\"\"\"\n",
    "    if augment:\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest')\n",
    "    else:\n",
    "        # Apenas rescale para geradores de valida√ß√£o/teste e baseline\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Note: 'image_path' e 'lesion_type' dependem do seu CSV\n",
    "    common_kwargs = dict(x_col='image_path', y_col='lesion_type', target_size=img_size, class_mode='categorical')\n",
    "\n",
    "    train_gen = train_datagen.flow_from_dataframe(dataframe=train_df, directory=str(image_root) if image_root is not None else None, batch_size=batch_size, shuffle=True, **common_kwargs)\n",
    "    val_gen = val_datagen.flow_from_dataframe(dataframe=val_df, directory=str(image_root) if image_root is not None else None, batch_size=batch_size, shuffle=False, **common_kwargs)\n",
    "    # Batch size = 1 para teste facilita a previs√£o exata se o len(test_df) n√£o for m√∫ltiplo de BATCH_SIZE\n",
    "    test_gen = test_datagen.flow_from_dataframe(dataframe=test_df, directory=str(image_root) if image_root is not None else None, batch_size=1, shuffle=False, **common_kwargs)\n",
    "\n",
    "    return train_gen, val_gen, test_gen\n",
    "\n",
    "def build_model(input_shape=INPUT_SHAPE, num_classes=num_classes, base_trainable=False, learning_rate=1e-4):\n",
    "    \"\"\"Constr√≥i um MobileNetV2 com topo customizado e retorna (model, base_model)\"\"\"\n",
    "    base = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base.trainable = base_trainable\n",
    "\n",
    "    x = base.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base.input, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model, base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fde2bb",
   "metadata": {},
   "source": [
    "## 3. **Etapa 1: Treinamento Baseline (Base Congelada)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe20a3",
   "metadata": {},
   "source": [
    "O treinamento Baseline estabelece a performance inicial do modelo, treinando apenas o top (cabe√ßa) da rede, enquanto as camadas de MobileNetV2 (pesos da ImageNet) permanecem congeladas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738868fc",
   "metadata": {},
   "source": [
    "### 3.1. **Treinamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a20ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Geradores sem augmentation (Baseline) ---\n",
    "train_gen, val_gen, test_gen_baseline = make_generators(train_df, val_df, test_df, image_root=IMAGE_ROOT, augment=False, batch_size=BATCH_SIZE)\n",
    "print('\\nClasses detectadas:', train_gen.class_indices)\n",
    "\n",
    "checkpoint_baseline = str(MODELS_DIR / 'baseline_checkpoint.keras')\n",
    "\n",
    "import tensorflow as tf\n",
    "# Then use a specific strategy like:\n",
    "Strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "def train_and_save(model, train_gen, val_gen, epochs, checkpoint_path, early_stop_patience=6, strategy: Optional[Strategy] = None):\n",
    "    \"\"\"\n",
    "    Treina `model` e salva melhor checkpoint em `checkpoint_path` (.keras). \n",
    "    Se `strategy` for fornecida, garante que o treinamento (`model.fit`) \n",
    "    ocorra dentro do escopo da estrat√©gia. Retorna (history, final_path).\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1),\n",
    "        EarlyStopping(monitor='val_loss', patience=early_stop_patience, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    # üåü O AJUSTE CHAVE: Envolver model.fit() com strategy.scope() se a estrat√©gia existir\n",
    "    if strategy:\n",
    "        with strategy.scope():\n",
    "            history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=callbacks, verbose=1)\n",
    "    else:\n",
    "        history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=callbacks, verbose=1)\n",
    "    \n",
    "    # --- Salvamento Final ---\n",
    "    final_path = os.path.join(str(MODELS_DIR), os.path.basename(checkpoint_path).replace('.keras', '_final.keras'))\n",
    "    \n",
    "    # √â uma boa pr√°tica salvar o modelo final *fora* do escopo da estrat√©gia,\n",
    "    # embora o Keras moderno geralmente lide com isso\n",
    "    try:\n",
    "        # Tenta carregar o melhor peso salvo pelo ModelCheckpoint\n",
    "        best = load_model(checkpoint_path) \n",
    "        best.save(final_path)\n",
    "    except Exception as e:\n",
    "        print('Aviso: n√£o foi poss√≠vel recarregar checkpoint para salvar final:', e)\n",
    "        # Se falhar, salva o modelo atual no final do treinamento (que pode n√£o ser o melhor)\n",
    "        model.save(final_path) \n",
    "\n",
    "    return history, final_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir e Treinar Modelo dentro do strategy scope (se houver GPU)\n",
    "if strategy_scope_used:\n",
    "    with strategy.scope():\n",
    "        baseline_model, _ = build_model(input_shape=INPUT_SHAPE, num_classes=num_classes, base_trainable=False, learning_rate=1e-4)\n",
    "else:\n",
    "    baseline_model, _ = build_model(input_shape=INPUT_SHAPE, num_classes=num_classes, base_trainable=False, learning_rate=1e-4)\n",
    "\n",
    "baseline_model.summary()\n",
    "\n",
    "print('\\n--- Treinamento Baseline (Apenas Topo) ---')\n",
    "history_baseline, baseline_final_path = train_and_save(baseline_model, train_gen, val_gen, epochs=EPOCHS_BASELINE, checkpoint_path=checkpoint_baseline)\n",
    "print('Baseline final salvo em:', baseline_final_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d9b21",
   "metadata": {},
   "source": [
    "### 3.2. **An√°lise de M√©tricas (Baseline)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65672423",
   "metadata": {},
   "source": [
    "**Fun√ß√£o de Perda e Acur√°cia**\\\n",
    "A fun√ß√£o de perda (loss) mede o qu√£o distante as previs√µes do modelo est√£o dos r√≥tulos verdadeiros. A queda em val_loss (e o aumento em val_accuracy) sem uma grande diverg√™ncia em rela√ß√£o √†s m√©tricas de treino (loss e accuracy) indica um bom aprendizado inicial, sem overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title=''):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Loss (Treino)')\n",
    "    plt.plot(history.history['val_loss'], label='Loss (Valida√ß√£o)')\n",
    "    plt.title(title + ' - Fun√ß√£o de Perda (Categorical Cross-Entropy)')\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Acur√°cia (Treino)')\n",
    "    plt.plot(history.history['val_accuracy'], label='Acur√°cia (Valida√ß√£o)')\n",
    "    plt.title(title + ' - Acur√°cia')\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('Acur√°cia')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_baseline, 'Baseline (Apenas Topo)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7fe53",
   "metadata": {},
   "source": [
    "**Matriz de Confus√£o**\\\n",
    "A matriz de confus√£o (Confusion Matrix) √© crucial para o baseline pois revela a performance por classe. Espera-se que, no in√≠cio, o modelo tenha alta confus√£o entre classes, especialmente se elas forem visualmente semelhantes, resultando em valores altos fora da diagonal principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd791984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24949 validated image filenames belonging to 3 classes.\n",
      "Found 5347 validated image filenames belonging to 3 classes.\n",
      "Found 5347 validated image filenames belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ampliar/miniconda3/envs/train-pipeline-gpu/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 2 invalid image filename(s) in x_col=\"image_path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "/home/ampliar/miniconda3/envs/train-pipeline-gpu/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2025-12-01 12:04:04.582033: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 90201\n",
      "W0000 00:00:1764601444.804640   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601444.836993   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601444.844817   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601444.866470   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601444.874157   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601444.881508   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601444.888981   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601444.896467   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601444.951654   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601444.963650   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601444.973081   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601444.996801   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.005068   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.016658   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.024450   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.032313   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.039734   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.066430   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.075844   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.083484   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.092960   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.102601   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.110325   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.119496   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.138879   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.146771   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.159025   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1764601445.167341   61981 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relat√≥rio de Classifica√ß√£o - Baseline (Teste):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       HISIL       0.98      0.97      0.97      1350\n",
      "       LISIL       0.91      0.91      0.91      1362\n",
      "      Normal       0.95      0.96      0.95      2635\n",
      "\n",
      "    accuracy                           0.95      5347\n",
      "   macro avg       0.95      0.94      0.94      5347\n",
      "weighted avg       0.95      0.95      0.95      5347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregar o melhor modelo baseline para avalia√ß√£o\n",
    "# carregar o melhor modelo salvo\n",
    "path = MODELS_DIR / 'baseline_checkpoint.keras'\n",
    "if not path.exists():\n",
    "    path = baseline_final_path  # Usa o final se o checkpoint n√£o existir\n",
    "\n",
    "\n",
    "model_baseline_eval = load_model(path)\n",
    "\n",
    "# Gerador de teste (batch_size=1)\n",
    "_, _, test_gen_eval = make_generators(train_df, val_df, test_df, image_root=IMAGE_ROOT, augment=False, batch_size=BATCH_SIZE)\n",
    "test_gen_eval.reset()\n",
    "\n",
    "y_pred_probs = model_baseline_eval.predict(test_gen_eval, steps=len(test_gen_eval), verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = test_gen_eval.classes\n",
    "labels = list(test_gen_eval.class_indices.keys())\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Verdadeiro')\n",
    "plt.title('Matriz de Confus√£o - Baseline (Teste)')\n",
    "plt.show()\n",
    "\n",
    "print('\\nRelat√≥rio de Classifica√ß√£o - Baseline (Teste):')\n",
    "print(classification_report(y_true, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae8799",
   "metadata": {},
   "source": [
    "## 4. **Etapa 2: Treinamento Robusto (com Augmentation e Otimiza√ß√£o de LR)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d2ec9",
   "metadata": {},
   "source": [
    "Esta etapa aplica Data Augmentation para melhorar a capacidade de generaliza√ß√£o do modelo e mitigar o overfitting. Realiza-se tamb√©m uma otimiza√ß√£o simples de taxa de aprendizado (learning_rate) para encontrar um hiperpar√¢metro melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa0b31",
   "metadata": {},
   "source": [
    "### 4.1. **Treinamento e Otimiza√ß√£o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446ebca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperpar√¢metros candidatos\n",
    "candidate_lrs = [1e-3, 1e-4, 5e-5]\n",
    "best_val_acc = -1.0\n",
    "best_model_path = None\n",
    "robust_best_path = str(MODELS_DIR / 'robust_best.keras')\n",
    "\n",
    "# Geradores com augmentation\n",
    "train_gen_aug, val_gen_aug, _ = make_generators(train_df, val_df, test_df, image_root=IMAGE_ROOT, augment=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('\\n--- Treinamento Robusto (Busca por LR) ---')\n",
    "for lr in candidate_lrs:\n",
    "    print(f'\\n--- Treinando candidato lr={lr} ---')\n",
    "    checkpoint_path = str(MODELS_DIR / f'robust_checkpoint_lr{str(lr).replace('.', 'p')}.keras')\n",
    "\n",
    "    # Reconstruir/compilar modelo dentro do strategy scope\n",
    "    if strategy_scope_used:\n",
    "        with strategy.scope():\n",
    "            model_candidate, _ = build_model(input_shape=INPUT_SHAPE, num_classes=num_classes, base_trainable=False, learning_rate=lr)\n",
    "    else:\n",
    "        model_candidate, _ = build_model(input_shape=INPUT_SHAPE, num_classes=num_classes, base_trainable=False, learning_rate=lr)\n",
    "\n",
    "\n",
    "    history, final_path = train_and_save(model_candidate, train_gen_aug, val_gen_aug, epochs=EPOCHS_ROBUST, checkpoint_path=checkpoint_path, early_stop_patience=8)\n",
    "\n",
    "    # Avalia o melhor checkpoint (monitorado por val_loss)\n",
    "    try:\n",
    "        loaded = load_model(checkpoint_path)\n",
    "        # Necess√°rio usar o gerador de valida√ß√£o com augmentation para a avalia√ß√£o\n",
    "        val_gen_aug.reset()\n",
    "        res = loaded.evaluate(val_gen_aug, verbose=0)\n",
    "        val_acc = res[1] if len(res) > 1 else None\n",
    "    except Exception as e:\n",
    "        print('Falha ao carregar ou avaliar checkpoint:', e)\n",
    "        # Fallback para o melhor da hist√≥ria\n",
    "        val_acc = max(history.history.get('val_accuracy', [-1]))\n",
    "\n",
    "    print(f'lr={lr} -> val_acc={val_acc}')\n",
    "    if val_acc is not None and val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_path = checkpoint_path\n",
    "\n",
    "print('\\nMelhor modelo da etapa 2 salvo em:', best_model_path, f'com val_acc={best_val_acc:.4f}')\n",
    "\n",
    "# Salvar c√≥pia nomeada do melhor modelo\n",
    "if best_model_path is not None and os.path.exists(best_model_path):\n",
    "    robust_best = load_model(best_model_path)\n",
    "    robust_best.save(robust_best_path)\n",
    "    print('Robust best salvo em:', robust_best_path)\n",
    "else:\n",
    "    print('Nenhum modelo v√°lido encontrado na etapa 2. Revertendo para o baseline.')\n",
    "    robust_best_path = checkpoint_baseline # Usa o baseline como fallback\n",
    "\n",
    "# Plot do hist√≥rico do melhor modelo robusto (necess√°rio carregar o hist√≥rico correspondente, ou usar um plot gen√©rico)\n",
    "# Neste caso, vamos carregar o hist√≥rico do melhor modelo salvo (se o nome seguir um padr√£o)\n",
    "# Como o history n√£o est√° salvo, usaremos o √∫ltimo history se o melhor_model_path for o √∫ltimo lr testado.\n",
    "# Se o melhor modelo n√£o foi o √∫ltimo, este plot pode ser enganoso.\n",
    "# Para um relat√≥rio completo, o hist√≥rico deveria ser persistido.\n",
    "# Para este exemplo, apenas plote o √∫ltimo history (lr=5e-5) para ilustra√ß√£o:\n",
    "if 'history' in locals():\n",
    "    plot_history(history, 'Robusto (√öltimo LR Testado)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af96a8",
   "metadata": {},
   "source": [
    "### 4.2. **An√°lise de M√©tricas (Robusto)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ce94a",
   "metadata": {},
   "source": [
    "O Data Augmentation deve ter como impacto a redu√ß√£o da diferen√ßa entre a accuracy de treino e de valida√ß√£o, indicando melhor generaliza√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff1a57d",
   "metadata": {},
   "source": [
    "**Matriz de Confus√£o**\\\n",
    "A matriz de confus√£o do melhor modelo robusto deve apresentar uma melhora significativa em rela√ß√£o ao baseline, com mais previs√µes corretas (valores maiores na diagonal principal) e menos erros de classifica√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o melhor modelo robusto para avalia√ß√£o\n",
    "robust_best_path = \"/home/ampliar/cancer-classify-citology/citology-pipeline-Train/models/robust_checkpoint_lr0p001_final.keras\"\n",
    "model_robust_eval = load_model(robust_best_path)\n",
    "test_gen_eval.reset()\n",
    "\n",
    "y_pred_probs = model_robust_eval.predict(test_gen_eval, steps=len(test_gen_eval), verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Verdadeiro')\n",
    "plt.title('Matriz de Confus√£o - Robusto (Teste)')\n",
    "plt.show()\n",
    "\n",
    "print('\\nRelat√≥rio de Classifica√ß√£o - Robusto (Teste):')\n",
    "print(classification_report(y_true, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd793f61",
   "metadata": {},
   "source": [
    "## 5. **Etapa 3: Fine-Tuning (Ajuste Fino)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf17e5",
   "metadata": {},
   "source": [
    "O Fine-Tuning descongela as camadas superiores da rede base (mais pr√≥ximas do topo) e as treina com uma taxa de aprendizado muito baixa. Isso permite que o modelo ajuste os features de alto n√≠vel aprendidos na ImageNet para o dom√≠nio espec√≠fico das imagens de citologia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d67ea",
   "metadata": {},
   "source": [
    "### 5.1. **Treinamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o melhor modelo da Etapa 2 (ou fallback)\n",
    "if os.path.exists(robust_best_path):\n",
    "    model_ft = load_model(robust_best_path)\n",
    "    print('\\nCarregado robust_best.keras para fine-tuning')\n",
    "else:\n",
    "    raise FileNotFoundError('Nenhum modelo robusto ou baseline encontrado para fine-tuning.')\n",
    "\n",
    "# --- Estrat√©gia de Fine-Tuning ---\n",
    "UNFREEZE_LAST_N = 30 # N√∫mero de camadas a descongelar\n",
    "total_layers = len(model_ft.layers)\n",
    "# Encontra a camada MobileNetV2 dentro do modelo, se o modelo n√£o for o pr√≥prio MobileNetV2\n",
    "base_model_layer = model_ft.get_layer(index=0) if isinstance(model_ft.layers[0], Model) else model_ft\n",
    "total_base_layers = len(base_model_layer.layers)\n",
    "start_idx = max(0, total_base_layers - UNFREEZE_LAST_N)\n",
    "\n",
    "# Descongela as √∫ltimas N camadas da MobileNetV2\n",
    "for i, layer in enumerate(base_model_layer.layers):\n",
    "    if i >= start_idx:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "print(f'Tornadas trein√°veis as {total_base_layers - start_idx} √∫ltimas camadas da MobileNetV2.')\n",
    "\n",
    "# Re-compilar com LR reduzida dentro do strategy scope\n",
    "FT_LR = 1e-5\n",
    "if strategy_scope_used:\n",
    "    with strategy.scope():\n",
    "        # A recompila√ß√£o deve ser feita no escopo para otimizar o novo otimizador\n",
    "        model_ft.compile(optimizer=Adam(learning_rate=FT_LR), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "    model_ft.compile(optimizer=Adam(learning_rate=FT_LR), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Preparar geradores (mantemos augmentation)\n",
    "train_gen_ft, val_gen_ft, _ = make_generators(train_df, val_df, test_df, image_root=IMAGE_ROOT, augment=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "checkpoint_ft = str(MODELS_DIR / 'final_finetuned_checkpoint.keras')\n",
    "print('\\n--- Treinamento Fine-Tuning ---')\n",
    "history_ft, final_ft_path = train_and_save(model_ft, train_gen_ft, val_gen_ft, epochs=EPOCHS_FINETUNE, checkpoint_path=checkpoint_ft, early_stop_patience=5)\n",
    "print('Fine-tuning final salvo em:', final_ft_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e28c0f",
   "metadata": {},
   "source": [
    "### 5.2. **An√°lise de M√©tricas (Fine-Tuning)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5273b",
   "metadata": {},
   "source": [
    "**Fun√ß√£o de Perda e Acur√°cia**\\\n",
    "O loss e a accuracy devem apresentar uma melhora marginal e mais lenta (devido ao baixo learning_rate) em rela√ß√£o √† Etapa 2. Se a loss de valida√ß√£o come√ßar a aumentar rapidamente, indica sobreajuste (overfitting), e o Early Stopping deve intervir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920abb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_ft, 'Fine-Tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b7582",
   "metadata": {},
   "source": [
    "## 6. **Avalia√ß√£o Final e Relat√≥rio**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80855cb3",
   "metadata": {},
   "source": [
    "O relat√≥rio final √© gerado no conjunto de teste (dados in√©ditos). O modelo usado √© o melhor modelo salvo em qualquer etapa, preferencialmente o Fine-Tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8f559",
   "metadata": {},
   "source": [
    "### 6.1. **Avalia√ß√£o no Conjunto de Teste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar modelo final para avalia√ß√£o\n",
    "candidates = [str(MODELS_DIR / 'final_finetuned_checkpoint.keras'), str(MODELS_DIR / 'robust_best.keras'), str(MODELS_DIR / 'baseline_checkpoint.keras')]\n",
    "model_eval = None\n",
    "best_path_eval = None\n",
    "for p in candidates:\n",
    "    if os.path.exists(p):\n",
    "        best_path_eval = p\n",
    "        model_eval = load_model(p)\n",
    "        print(f'\\nUsando modelo para avalia√ß√£o final: {os.path.basename(p)}')\n",
    "        break\n",
    "\n",
    "if model_eval is None:\n",
    "    raise FileNotFoundError('Nenhum modelo dispon√≠vel para avalia√ß√£o. Execute as etapas anteriores.')\n",
    "\n",
    "# Criar gerador de teste (batch_size=1 para previs√µes exatas)\n",
    "_, _, test_gen_eval = make_generators(train_df, val_df, test_df, image_root=IMAGE_ROOT, augment=False, batch_size=BATCH_SIZE)\n",
    "test_gen_eval.reset()\n",
    "\n",
    "print('\\nAvaliando no conjunto de teste...')\n",
    "test_loss, test_acc = model_eval.evaluate(test_gen_eval, steps=len(test_gen_eval), verbose=1)\n",
    "print(f'\\n‚úÖ Performance Final - Teste Loss: {test_loss:.4f} | Teste Acur√°cia: {test_acc:.4f}')\n",
    "\n",
    "# Previs√µes completas para relat√≥rio e matriz de confus√£o\n",
    "test_gen_eval.reset()\n",
    "y_pred_probs = model_eval.predict(test_gen_eval, steps=len(test_gen_eval), verbose=1)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = test_gen_eval.classes\n",
    "labels = list(test_gen_eval.class_indices.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17e9af",
   "metadata": {},
   "source": [
    "### 6.2. **Matriz de Confus√£o Final e Relat√≥rio de Classifica√ß√£o**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb3f41",
   "metadata": {},
   "source": [
    "**Matriz de Confus√£o**\\\n",
    "A matriz final deve ser a mais clara poss√≠vel, com a maioria dos valores concentrados na diagonal principal, indicando que o modelo acertou a classifica√ß√£o na maioria dos casos. Os valores fora da diagonal principal representam os erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cac6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Verdadeiro')\n",
    "plt.title(f'Matriz de Confus√£o Final (Teste) - {os.path.basename(best_path_eval)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0577c417",
   "metadata": {},
   "source": [
    "Relat√≥rio de Classifica√ß√£o\n",
    "\n",
    "* **Precis√£o:** De todas as vezes que o modelo previu a Classe X, quantas vezes ele acertou.\n",
    "\n",
    "* **Recall:** De todas as imagens que s√£o da Classe X, quantas o modelo conseguiu identificar corretamente.\n",
    "\n",
    "* **F1-Score:** M√©dia harm√¥nica entre Precis√£o e Recall (uma m√©trica de balan√ßo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d998e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n--- Relat√≥rio de Classifica√ß√£o Final (Teste) ---')\n",
    "print(classification_report(y_true, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645bcab",
   "metadata": {},
   "source": [
    "## 7. **Infer√™ncia em Produ√ß√£o (predict_single_image)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5d7ab",
   "metadata": {},
   "source": [
    "Esta fun√ß√£o simula um ambiente de produ√ß√£o. Ela carrega o modelo apenas uma vez e, em seguida, aceita um caminho de imagem por vez, pr√©-processa-a rapidamente e retorna a previs√£o e as probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929f4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37c71c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configura√ß√µes Globais ---\n",
    "_LOADED_MODEL = None\n",
    "_CLASS_NAMES = None\n",
    "_IMG_SIZE = (224, 224)  # Tamanho esperado pelo modelo\n",
    "\n",
    "# Configura√ß√µes de TILING (tamanho original dos patches que voc√™ quer extrair)\n",
    "PATCH_WIDTH = 1024   # Tamanho do patch extra√≠do da imagem grande\n",
    "PATCH_HEIGHT = 768   # Tamanho do patch extra√≠do da imagem grande\n",
    "STRIDE_X = 340\n",
    "STRIDE_Y = 256\n",
    "CONTENT_THRESHOLD = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc2a1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# FUN√á√ïES DE CARREGAMENTO E PR√â-PROCESSAMENTO DO MODELO\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def load_inference_model(model_path: str, class_names: list, img_size: tuple = (224, 224)):\n",
    "    \"\"\"Carrega o modelo globalmente uma √∫nica vez.\"\"\"\n",
    "    global _LOADED_MODEL, _CLASS_NAMES, _IMG_SIZE\n",
    "    if _LOADED_MODEL is None:\n",
    "        print(f\"--- Carregando modelo para infer√™ncia de: {model_path} ---\")\n",
    "        try:\n",
    "            _LOADED_MODEL = load_model(model_path, compile=False) \n",
    "            _CLASS_NAMES = class_names\n",
    "            _IMG_SIZE = img_size\n",
    "            print(f\"‚úÖ Modelo carregado. Input Size: {_IMG_SIZE}\")\n",
    "            print(f\"‚úÖ Tiles ser√£o extra√≠dos em {PATCH_HEIGHT}x{PATCH_WIDTH} e redimensionados para {_IMG_SIZE}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERRO ao carregar o modelo: {e}\")\n",
    "            raise\n",
    "\n",
    "def preprocess_patch(patch_img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Pr√©-processa um patch BGR (OpenCV) para o formato esperado pelo Keras/TF.\"\"\"\n",
    "    # 1. Converte BGR para RGB\n",
    "    rgb_patch = cv2.cvtColor(patch_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 2. VERIFICA se precisa redimensionar\n",
    "    if rgb_patch.shape[:2] != _IMG_SIZE:\n",
    "        # REDIMENSIONA para o tamanho esperado pelo modelo\n",
    "        rgb_patch = cv2.resize(rgb_patch, (_IMG_SIZE[1], _IMG_SIZE[0]))  # width, height\n",
    "        print(f\"üîπ Patch redimensionado de {patch_img.shape[:2]} para {_IMG_SIZE}\")\n",
    "    \n",
    "    # 3. Adiciona dimens√£o de batch e normaliza\n",
    "    img_array = np.expand_dims(rgb_patch, axis=0).astype('float32')\n",
    "    img_array /= 255.0\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FUN√á√ÉO PRINCIPAL DE INFER√äNCIA BASEADA EM TILING\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def process_and_infer(\n",
    "    image_path: str,\n",
    "    output_dir_base: str,\n",
    "    save_tiles: bool = True,\n",
    "    save_original_size: bool = False\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Processa uma imagem grande (WSI), gera tiles, REDIMENSIONA cada tile para o tamanho do modelo,\n",
    "    realiza a infer√™ncia e agrega os resultados por m√©dia.\n",
    "    \"\"\"\n",
    "    global _LOADED_MODEL, _CLASS_NAMES, _IMG_SIZE\n",
    "    \n",
    "    if _LOADED_MODEL is None:\n",
    "        raise ValueError(\"Modelo n√£o carregado. Chame load_inference_model() primeiro.\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    inference_times = []\n",
    "    \n",
    "    # --- 1. CONFIGURA√á√ÉO DE CAMINHOS E DIRET√ìRIOS ---\n",
    "    image_filename = os.path.basename(image_path)\n",
    "    base_filename = os.path.splitext(image_filename)[0]\n",
    "    \n",
    "    output_dir = os.path.join(output_dir_base, f\"tiles_{base_filename}\")\n",
    "    if save_tiles:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # --- 2. CARREGAMENTO E INICIALIZA√á√ÉO ---\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Erro: N√£o foi poss√≠vel ler a imagem em {image_path}\")\n",
    "\n",
    "    # Usa PATCH_HEIGHT e PATCH_WIDTH para extrair tiles da imagem grande\n",
    "    H, W = PATCH_HEIGHT, PATCH_WIDTH\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    height, width, _ = img.shape\n",
    "    \n",
    "    total_patches = 0\n",
    "    good_patches = 0\n",
    "    all_probabilities = []\n",
    "    \n",
    "    pre_process_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"--- Iniciando Processamento ({width}x{height}) ---\")\n",
    "    print(f\"üîπ Extraindo tiles de {PATCH_HEIGHT}x{PATCH_WIDTH} pixels\")\n",
    "    print(f\"üîπ Redimensionando para {_IMG_SIZE[0]}x{_IMG_SIZE[1]} pixels para infer√™ncia\")\n",
    "\n",
    "    # --- 3. TILING E INFER√äNCIA ---\n",
    "    for y in range(0, height - H + 1, STRIDE_Y):\n",
    "        for x in range(0, width - W + 1, STRIDE_X):\n",
    "            total_patches += 1\n",
    "            \n",
    "            # 3.1. Extrai o patch no tamanho ORIGINAL\n",
    "            patch_gray = gray[y:y + H, x:x + W]\n",
    "            patch_color = img[y:y + H, x:x + W]\n",
    "            \n",
    "            # Pula se o patch n√£o tiver o tamanho esperado\n",
    "            if patch_gray.shape[0] != H or patch_gray.shape[1] != W:\n",
    "                continue\n",
    "                \n",
    "            # 3.2. Filtra por conte√∫do\n",
    "            std_dev = np.std(patch_gray)\n",
    "            \n",
    "            if std_dev > CONTENT_THRESHOLD:\n",
    "                good_patches += 1\n",
    "                \n",
    "                # A. Pr√©-processamento do Patch (inclui REDIMENSIONAMENTO)\n",
    "                input_tensor = preprocess_patch(patch_color)\n",
    "                \n",
    "                # VERIFICA√á√ÉO DE SEGURAN√áA: Confirma o shape do tensor\n",
    "                expected_shape = (1, _IMG_SIZE[0], _IMG_SIZE[1], 3)\n",
    "                if input_tensor.shape != expected_shape:\n",
    "                    print(f\"‚ùå ERRO: Shape esperado {expected_shape}, mas obteve {input_tensor.shape}\")\n",
    "                    print(f\"‚ùå Patch original shape: {patch_color.shape}\")\n",
    "                    continue\n",
    "                \n",
    "                # B. Infer√™ncia\n",
    "                inf_start = time.time()\n",
    "                predictions = _LOADED_MODEL.predict(input_tensor, verbose=0)\n",
    "                inf_end = time.time()\n",
    "                inference_times.append(inf_end - inf_start)\n",
    "                \n",
    "                all_probabilities.append(predictions[0])\n",
    "                \n",
    "                # C. Salvar Tile (Opcional)\n",
    "                if save_tiles:\n",
    "                    if save_original_size:\n",
    "                        # Salva o tile no tamanho original extra√≠do\n",
    "                        file_name = f\"{base_filename}_patch_y{y}_x{x}_{H}x{W}.png\"\n",
    "                        save_path = os.path.join(output_dir, file_name)\n",
    "                        cv2.imwrite(save_path, patch_color)\n",
    "                    else:\n",
    "                        # Salva o tile REDIMENSIONADO (como foi processado pelo modelo)\n",
    "                        file_name = f\"{base_filename}_patch_y{y}_x{x}_{_IMG_SIZE[0]}x{_IMG_SIZE[1]}.png\"\n",
    "                        save_path = os.path.join(output_dir, file_name)\n",
    "                        # Recupera a imagem redimensionada do tensor\n",
    "                        resized_img = (input_tensor[0] * 255.0).astype('uint8')\n",
    "                        resized_img_bgr = cv2.cvtColor(resized_img, cv2.COLOR_RGB2BGR)\n",
    "                        cv2.imwrite(save_path, resized_img_bgr)\n",
    "\n",
    "    # --- 4. C√ÅLCULO DE M√âTRICAS E AGREGA√á√ÉO ---\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if not all_probabilities:\n",
    "        return {\n",
    "            \"error\": \"Nenhum patch com conte√∫do suficiente encontrado para infer√™ncia.\",\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "\n",
    "    mean_probabilities = np.mean(all_probabilities, axis=0)\n",
    "    predicted_index = np.argmax(mean_probabilities)\n",
    "    predicted_class = _CLASS_NAMES[predicted_index]\n",
    "    confidence_score = float(mean_probabilities[predicted_index])\n",
    "    \n",
    "    all_probs_dict = {name: float(prob) for name, prob in zip(_CLASS_NAMES, mean_probabilities)}\n",
    "\n",
    "    # --- 5. RESULTADO FINAL ---\n",
    "    return {\n",
    "        \"image_filename\": image_filename,\n",
    "        \"total_patches_extracted\": total_patches,\n",
    "        \"good_patches_inferred\": good_patches,\n",
    "        \"final_prediction\": predicted_class,\n",
    "        \"confidence_score\": confidence_score,\n",
    "        \"mean_probabilities\": all_probs_dict,\n",
    "        \"time_breakdown\": {\n",
    "            \"pre_process_time_s\": pre_process_time,\n",
    "            \"total_inference_time_s\": sum(inference_times),\n",
    "            \"avg_inference_per_patch_ms\": (sum(inference_times) / good_patches) * 1000 if good_patches > 0 else 0,\n",
    "            \"total_runtime_s\": total_time\n",
    "        },\n",
    "        \"processing_info\": {\n",
    "            \"tile_extraction_size\": f\"{PATCH_HEIGHT}x{PATCH_WIDTH}\",\n",
    "            \"model_input_size\": f\"{_IMG_SIZE[0]}x{_IMG_SIZE[1]}\",\n",
    "            \"resize_applied\": True\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f9ca799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEFINI√á√ïES DE CAMINHOS  ---\n",
    "ROOT_PROJECT = Path(\"/home/ampliar/cancer-classify-citology/citology-pipeline-Train\")\n",
    "MODEL_DIR = ROOT_PROJECT / 'models2'\n",
    "OUTPUT_INFERENCE_DIR = ROOT_PROJECT / 'inferencias'\n",
    "IMAGE_INPUT_PATH = \"/home/ampliar/cancer-classify-citology/citology-pipeline-Train/Dataset/inteiras/3 Classes/Tile/HISIL/HSIL_6 (7).jpg\"\n",
    "\n",
    "# --- PAR√ÇMETROS DO MODELO ---\n",
    "MODEL_PATH = MODEL_DIR / 'robust_checkpoint_lr0p001_final.keras'\n",
    "CLASS_NAMES = ['HISIL', 'LISIL', 'Normal']\n",
    "\n",
    "# Garante que a pasta de sa√≠da existe\n",
    "os.makedirs(OUTPUT_INFERENCE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc1cf7",
   "metadata": {},
   "source": [
    "#### **Sem Paralelismo** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7b14fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verificando o modelo...\n",
      "‚úÖ Modelo espera input shape: (None, 224, 224, 3)\n",
      "‚úÖ Usando tamanho: (224, 224)\n",
      "--- Carregando modelo para infer√™ncia de: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/models2/robust_checkpoint_lr0p001_final.keras ---\n",
      "‚úÖ Modelo carregado. Input Size: (224, 224)\n",
      "‚úÖ Tiles ser√£o extra√≠dos em 768x1024 e redimensionados para (224, 224)\n",
      "--- Iniciando Processamento (2048x1536) ---\n",
      "üîπ Extraindo tiles de 768x1024 pixels\n",
      "üîπ Redimensionando para 224x224 pixels para infer√™ncia\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764639196.123860   61973 service.cc:146] XLA service 0x769f2c0034c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1764639196.125998   61973 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2025-12-01 22:33:16.265878: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1764639200.532682   61973 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "üîπ Patch redimensionado de (768, 1024) para (224, 224)\n",
      "\n",
      "==================================================\n",
      "‚ú® RESULTADO FINAL DA INFER√äNCIA POR TILING ‚ú®\n",
      "==================================================\n",
      "IMAGEM ANALISADA: HSIL_6 (7).jpg\n",
      "TOTAL DE PATCHES √öTEIS: 16 / 16\n",
      "\n",
      "CLASSIFICA√á√ÉO FINAL (M√©dia): **HISIL**\n",
      "CONFIAN√áA (M√©dia): 0.9962\n",
      "\n",
      "--- DETALHES DE PROBABILIDADES M√âDIAS ---\n",
      "  HISIL: 0.9962\n",
      "  LISIL: 0.0027\n",
      "  Normal: 0.0011\n",
      "\n",
      "--- INFORMA√á√ïES DE PROCESSAMENTO ---\n",
      "Tamanho dos tiles extra√≠dos: 768x1024\n",
      "Tamanho para infer√™ncia: 224x224\n",
      "Redimensionamento aplicado: SIM\n",
      "\n",
      "--- DESEMPENHO (TEMPO) ---\n",
      "1. Tempo de Pr√©-Processamento (Tiling/Filtro): 0.0319 segundos\n",
      "2. Tempo Total de Infer√™ncia (todos patches): 6.2248 segundos\n",
      "3. Tempo M√©dio por Patch Inferred: 389.05 milissegundos\n",
      "4. Tempo Total de Execu√ß√£o: 6.3347 segundos\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# PRIMEIRO: Verificar o tamanho real que o modelo espera\n",
    "try:\n",
    "    print(\"üîç Verificando o modelo...\")\n",
    "    test_model = load_model(str(MODEL_PATH), compile=False)\n",
    "    expected_shape = test_model.input_shape\n",
    "    ACTUAL_IMG_SIZE = (expected_shape[1], expected_shape[2])  # (height, width)\n",
    "    print(f\"‚úÖ Modelo espera input shape: {expected_shape}\")\n",
    "    print(f\"‚úÖ Usando tamanho: {ACTUAL_IMG_SIZE}\")\n",
    "    \n",
    "    # AGORA carregar com o tamanho correto\n",
    "    load_inference_model(\n",
    "        model_path=str(MODEL_PATH),\n",
    "        class_names=CLASS_NAMES,\n",
    "        img_size=ACTUAL_IMG_SIZE  # Usa o tamanho REAL do modelo\n",
    "    )\n",
    "\n",
    "    # EXECUTAR INFER√äNCIA\n",
    "    result = process_and_infer(\n",
    "        image_path=str(IMAGE_INPUT_PATH),\n",
    "        output_dir_base=str(OUTPUT_INFERENCE_DIR),\n",
    "        save_tiles=True,\n",
    "        save_original_size=False\n",
    "    )\n",
    "\n",
    "    # MOSTRAR RESULTADOS\n",
    "    if \"error\" in result:\n",
    "        print(f\"\\n‚ùå ERRO: {result['error']}\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"‚ú® RESULTADO FINAL DA INFER√äNCIA POR TILING ‚ú®\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"IMAGEM ANALISADA: {result['image_filename']}\")\n",
    "        print(f\"TOTAL DE PATCHES √öTEIS: {result['good_patches_inferred']} / {result['total_patches_extracted']}\")\n",
    "        print(f\"\\nCLASSIFICA√á√ÉO FINAL (M√©dia): **{result['final_prediction']}**\")\n",
    "        print(f\"CONFIAN√áA (M√©dia): {result['confidence_score']:.4f}\")\n",
    "        \n",
    "        print(\"\\n--- DETALHES DE PROBABILIDADES M√âDIAS ---\")\n",
    "        for cls, prob in result['mean_probabilities'].items():\n",
    "            print(f\"  {cls}: {prob:.4f}\")\n",
    "\n",
    "        print(\"\\n--- INFORMA√á√ïES DE PROCESSAMENTO ---\")\n",
    "        print(f\"Tamanho dos tiles extra√≠dos: {result['processing_info']['tile_extraction_size']}\")\n",
    "        print(f\"Tamanho para infer√™ncia: {result['processing_info']['model_input_size']}\")\n",
    "        print(f\"Redimensionamento aplicado: {'SIM' if result['processing_info']['resize_applied'] else 'N√ÉO'}\")\n",
    "\n",
    "        print(\"\\n--- DESEMPENHO (TEMPO) ---\")\n",
    "        print(f\"1. Tempo de Pr√©-Processamento (Tiling/Filtro): {result['time_breakdown']['pre_process_time_s']:.4f} segundos\")\n",
    "        print(f\"2. Tempo Total de Infer√™ncia (todos patches): {result['time_breakdown']['total_inference_time_s']:.4f} segundos\")\n",
    "        print(f\"3. Tempo M√©dio por Patch Inferred: {result['time_breakdown']['avg_inference_per_patch_ms']:.2f} milissegundos\")\n",
    "        print(f\"4. Tempo Total de Execu√ß√£o: {result['time_breakdown']['total_runtime_s']:.4f} segundos\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n‚ö†Ô∏è ERRO: Certifique-se de que o modelo e a imagem existem nos caminhos definidos.\")\n",
    "    print(e)\n",
    "except ValueError as e:\n",
    "    print(f\"\\n‚ö†Ô∏è ERRO: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERRO FATAL: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773aa87",
   "metadata": {},
   "source": [
    "#### **Com paralelismo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faef6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e782294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√¢metros de otimiza√ß√£o (ajuste conforme seu hardware)\n",
    "BATCH_SIZE = 16      # Tamanho do batch para infer√™ncia (maior = melhor uso da GPU)\n",
    "NUM_THREADS = 8      # N√∫mero de threads para pr√©-processamento paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce8eba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inference_model(model_path: str, class_names: list, img_size: tuple = (224, 224)):\n",
    "    \"\"\"Carrega o modelo globalmente uma √∫nica vez.\"\"\"\n",
    "    global _LOADED_MODEL, _CLASS_NAMES, _IMG_SIZE\n",
    "    if _LOADED_MODEL is None:\n",
    "        print(f\"--- Carregando modelo para infer√™ncia de: {model_path} ---\")\n",
    "        try:\n",
    "            _LOADED_MODEL = load_model(model_path, compile=False) \n",
    "            _CLASS_NAMES = class_names\n",
    "            _IMG_SIZE = img_size\n",
    "            print(f\"‚úÖ Modelo carregado. Input Size: {_IMG_SIZE}\")\n",
    "            print(f\"‚úÖ Tiles ser√£o extra√≠dos em {PATCH_HEIGHT}x{PATCH_WIDTH}\")\n",
    "            print(f\"‚úÖ Redimensionados para {_IMG_SIZE} para infer√™ncia\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERRO ao carregar o modelo: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def extract_valid_patches(\n",
    "    image_path: str,\n",
    "    output_dir: str,\n",
    "    save_tiles: bool,\n",
    "    save_original_size: bool\n",
    ") -> Tuple[List[Tuple], int, str, str]:\n",
    "    \"\"\"\n",
    "    Extrai todos os patches v√°lidos da imagem.\n",
    "    Retorna: (lista_de_patches, total_patches, base_filename, output_dir)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Erro: N√£o foi poss√≠vel ler a imagem em {image_path}\")\n",
    "    \n",
    "    H, W = PATCH_HEIGHT, PATCH_WIDTH\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    height, width, _ = img.shape\n",
    "    \n",
    "    image_filename = os.path.basename(image_path)\n",
    "    base_filename = os.path.splitext(image_filename)[0]\n",
    "    \n",
    "    valid_patches = []  # Lista de (y, x, patch_color)\n",
    "    total_patches = 0\n",
    "    \n",
    "    print(f\"üîç Coletando patches v√°lidos de {width}x{height}...\")\n",
    "    \n",
    "    for y in range(0, height - H + 1, STRIDE_Y):\n",
    "        for x in range(0, width - W + 1, STRIDE_X):\n",
    "            total_patches += 1\n",
    "            \n",
    "            patch_gray = gray[y:y + H, x:x + W]\n",
    "            patch_color = img[y:y + H, x:x + W]\n",
    "            \n",
    "            # Verifica tamanho\n",
    "            if patch_gray.shape[0] != H or patch_gray.shape[1] != W:\n",
    "                continue\n",
    "            \n",
    "            # Filtra por conte√∫do\n",
    "            std_dev = np.std(patch_gray)\n",
    "            if std_dev > CONTENT_THRESHOLD:\n",
    "                valid_patches.append((y, x, patch_color))\n",
    "    \n",
    "    print(f\"‚úÖ {len(valid_patches)} patches v√°lidos encontrados de {total_patches} totais\")\n",
    "    return valid_patches, total_patches, base_filename, output_dir\n",
    "\n",
    "\n",
    "def preprocess_batch(batch_patches: List[Tuple], save_info: Tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pr√©-processa um lote de patches em paralelo.\n",
    "    Retorna tensor pronto para infer√™ncia.\n",
    "    \"\"\"\n",
    "    batch_images = []\n",
    "    base_filename, output_dir, save_tiles, save_original_size = save_info\n",
    "    \n",
    "    for y, x, patch_color in batch_patches:\n",
    "        # Redimensiona para o tamanho do modelo\n",
    "        rgb_patch = cv2.cvtColor(patch_color, cv2.COLOR_BGR2RGB)\n",
    "        rgb_patch = cv2.resize(rgb_patch, (_IMG_SIZE[1], _IMG_SIZE[0]))\n",
    "        \n",
    "        # Normaliza\n",
    "        img_array = rgb_patch.astype('float32') / 255.0\n",
    "        batch_images.append(img_array)\n",
    "        \n",
    "        # Salva tile se necess√°rio (feito em paralelo tamb√©m)\n",
    "        if save_tiles:\n",
    "            if save_original_size:\n",
    "                file_name = f\"{base_filename}_patch_y{y}_x{x}_{PATCH_HEIGHT}x{PATCH_WIDTH}.png\"\n",
    "                cv2.imwrite(os.path.join(output_dir, file_name), patch_color)\n",
    "            else:\n",
    "                file_name = f\"{base_filename}_patch_y{y}_x{x}_{_IMG_SIZE[0]}x{_IMG_SIZE[1]}.png\"\n",
    "                resized_img = (img_array * 255.0).astype('uint8')\n",
    "                resized_img_bgr = cv2.cvtColor(resized_img, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(os.path.join(output_dir, file_name), resized_img_bgr)\n",
    "    \n",
    "    return np.array(batch_images)\n",
    "\n",
    "\n",
    "def process_and_infer_hybrid(\n",
    "    image_path: str,\n",
    "    output_dir_base: str,\n",
    "    save_tiles: bool = True,\n",
    "    save_original_size: bool = False,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    num_threads: int = NUM_THREADS\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Solu√ß√£o h√≠brida otimizada:\n",
    "    - Pr√©-processamento paralelo com m√∫ltiplas threads\n",
    "    - Infer√™ncia em batches para melhor uso da GPU\n",
    "    \"\"\"\n",
    "    global _LOADED_MODEL, _CLASS_NAMES, _IMG_SIZE\n",
    "    \n",
    "    if _LOADED_MODEL is None:\n",
    "        raise ValueError(\"Modelo n√£o carregado. Chame load_inference_model() primeiro.\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- 1. CONFIGURA√á√ÉO INICIAL ---\n",
    "    image_filename = os.path.basename(image_path)\n",
    "    base_filename = os.path.splitext(image_filename)[0]\n",
    "    output_dir = os.path.join(output_dir_base, f\"tiles_{base_filename}\")\n",
    "    \n",
    "    if save_tiles:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"üöÄ Iniciando processamento h√≠brido otimizado\")\n",
    "    print(f\"üìä Configura√ß√£o: {num_threads} threads, batch size: {batch_size}\")\n",
    "    \n",
    "    # --- 2. EXTRA√á√ÉO DE PATCHES V√ÅLIDOS ---\n",
    "    extraction_start = time.time()\n",
    "    valid_patches, total_patches, base_filename, output_dir = extract_valid_patches(\n",
    "        image_path, output_dir, save_tiles, save_original_size\n",
    "    )\n",
    "    extraction_time = time.time() - extraction_start\n",
    "    \n",
    "    if not valid_patches:\n",
    "        total_time = time.time() - start_time\n",
    "        return {\n",
    "            \"error\": \"Nenhum patch com conte√∫do suficiente encontrado para infer√™ncia.\",\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "    \n",
    "    # --- 3. PR√â-PROCESSAMENTO PARALELO EM THREADS ---\n",
    "    preprocess_start = time.time()\n",
    "    \n",
    "    # Divide os patches em lotes para pr√©-processamento\n",
    "    batches = [valid_patches[i:i + batch_size] \n",
    "              for i in range(0, len(valid_patches), batch_size)]\n",
    "    \n",
    "    # Informa√ß√µes para salvar tiles\n",
    "    save_info = (base_filename, output_dir, save_tiles, save_original_size)\n",
    "    \n",
    "    # Pr√©-processa em paralelo usando ThreadPoolExecutor\n",
    "    batch_tensors = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        # Envia todos os lotes para pr√©-processamento paralelo\n",
    "        future_to_batch = {\n",
    "            executor.submit(preprocess_batch, batch, save_info): batch_idx\n",
    "            for batch_idx, batch in enumerate(batches)\n",
    "        }\n",
    "        \n",
    "        # Coleta os resultados mantendo a ordem\n",
    "        results = [None] * len(batches)\n",
    "        for future in future_to_batch:\n",
    "            batch_idx = future_to_batch[future]\n",
    "            try:\n",
    "                batch_tensor = future.result()\n",
    "                results[batch_idx] = batch_tensor\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erro no pr√©-processamento do lote {batch_idx}: {e}\")\n",
    "                results[batch_idx] = np.array([])\n",
    "        \n",
    "        # Filtra resultados vazios\n",
    "        batch_tensors = [tensor for tensor in results if tensor.size > 0]\n",
    "    \n",
    "    preprocess_time = time.time() - preprocess_start\n",
    "    \n",
    "    # --- 4. INFER√äNCIA EM BATCHES (GPU otimizada) ---\n",
    "    inference_start = time.time()\n",
    "    all_probabilities = []\n",
    "    \n",
    "    for batch_tensor in batch_tensors:\n",
    "        if len(batch_tensor) > 0:\n",
    "            # Usa predict_on_batch para maior efici√™ncia\n",
    "            predictions = _LOADED_MODEL.predict_on_batch(batch_tensor)\n",
    "            all_probabilities.extend(predictions)\n",
    "    \n",
    "    inference_time = time.time() - inference_start\n",
    "    \n",
    "    # --- 5. C√ÅLCULO DOS RESULTADOS ---\n",
    "    aggregation_start = time.time()\n",
    "    \n",
    "    mean_probabilities = np.mean(all_probabilities, axis=0)\n",
    "    predicted_index = np.argmax(mean_probabilities)\n",
    "    predicted_class = _CLASS_NAMES[predicted_index]\n",
    "    confidence_score = float(mean_probabilities[predicted_index])\n",
    "    \n",
    "    all_probs_dict = {name: float(prob) for name, prob in zip(_CLASS_NAMES, mean_probabilities)}\n",
    "    \n",
    "    aggregation_time = time.time() - aggregation_start\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # --- 6. RESULTADO FINAL ---\n",
    "    return {\n",
    "        \"image_filename\": image_filename,\n",
    "        \"total_patches_extracted\": total_patches,\n",
    "        \"good_patches_inferred\": len(valid_patches),\n",
    "        \"final_prediction\": predicted_class,\n",
    "        \"confidence_score\": confidence_score,\n",
    "        \"mean_probabilities\": all_probs_dict,\n",
    "        \"time_breakdown\": {\n",
    "            \"extraction_time_s\": extraction_time,\n",
    "            \"preprocess_time_s\": preprocess_time,\n",
    "            \"inference_time_s\": inference_time,\n",
    "            \"aggregation_time_s\": aggregation_time,\n",
    "            \"total_inference_time_s\": inference_time,\n",
    "            \"avg_inference_per_patch_ms\": (inference_time / len(valid_patches)) * 1000 if valid_patches else 0,\n",
    "            \"total_runtime_s\": total_time\n",
    "        },\n",
    "        \"processing_info\": {\n",
    "            \"tile_extraction_size\": f\"{PATCH_HEIGHT}x{PATCH_WIDTH}\",\n",
    "            \"model_input_size\": f\"{_IMG_SIZE[0]}x{_IMG_SIZE[1]}\",\n",
    "            \"resize_applied\": True,\n",
    "            \"parallel_workers\": num_threads,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"optimization_method\": \"hybrid_parallel_batch\",\n",
    "            \"total_batches_processed\": len(batch_tensors)\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51d13260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEFINI√á√ïES DE CAMINHOS ---\n",
    "ROOT_PROJECT = Path(\"/home/ampliar/cancer-classify-citology/citology-pipeline-Train\")\n",
    "MODEL_DIR = ROOT_PROJECT / 'models2'\n",
    "OUTPUT_INFERENCE_DIR = ROOT_PROJECT / 'inferencias'\n",
    "IMAGE_INPUT_PATH = \"/home/ampliar/cancer-classify-citology/citology-pipeline-Train/Dataset/inteiras/3 Classes/Tile/HISIL/HSIL_6 (7).jpg\"\n",
    "\n",
    "# --- PAR√ÇMETROS DO MODELO ---\n",
    "MODEL_PATH = MODEL_DIR / 'robust_checkpoint_lr0p001_final.keras'\n",
    "CLASS_NAMES = ['HISIL', 'LISIL', 'Normal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f00b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verificando o modelo...\n",
      "‚úÖ Modelo espera input shape: (None, 224, 224, 3)\n",
      "‚úÖ Usando tamanho: (224, 224)\n",
      "\n",
      "============================================================\n",
      "üöÄ INICIANDO INFER√äNCIA OTIMIZADA (PARALELA + BATCH)\n",
      "============================================================\n",
      "üöÄ Iniciando processamento h√≠brido otimizado\n",
      "üìä Configura√ß√£o: 8 threads, batch size: 16\n",
      "üîç Coletando patches v√°lidos de 2048x1536...\n",
      "‚úÖ 16 patches v√°lidos encontrados de 16 totais\n",
      "\n",
      "============================================================\n",
      "‚ú® RESULTADO FINAL DA INFER√äNCIA OTIMIZADA ‚ú®\n",
      "============================================================\n",
      "üìÑ IMAGEM ANALISADA: HSIL_6 (7).jpg\n",
      "üî¢ TOTAL DE PATCHES: 16 / 16\n",
      "\n",
      "üè∑Ô∏è  CLASSIFICA√á√ÉO FINAL: **HISIL**\n",
      "üìà CONFIAN√áA: 0.9961\n",
      "\n",
      "--- üìä DETALHES DE PROBABILIDADES ---\n",
      "  ‚Ä¢ HISIL: 0.9961\n",
      "  ‚Ä¢ LISIL: 0.0028\n",
      "  ‚Ä¢ Normal: 0.0012\n",
      "\n",
      "--- ‚öôÔ∏è  INFORMA√á√ïES DE PROCESSAMENTO ---\n",
      "‚Ä¢ M√©todo de otimiza√ß√£o: hybrid_parallel_batch\n",
      "‚Ä¢ Workers paralelos: 8\n",
      "‚Ä¢ Batch size: 16\n",
      "‚Ä¢ Batches processados: 1\n",
      "‚Ä¢ Tamanho dos tiles extra√≠dos: 768x1024\n",
      "‚Ä¢ Tamanho para infer√™ncia: 224x224\n",
      "‚Ä¢ Redimensionamento: SIM\n",
      "\n",
      "--- ‚è±Ô∏è  AN√ÅLISE DE DESEMPENHO DETALHADA ---\n",
      "1. Extra√ß√£o de patches: 0.0519s\n",
      "2. Pr√©-processamento (paralelo): 0.0415s\n",
      "3. Infer√™ncia (batch GPU): 0.0244s\n",
      "4. Agrega√ß√£o de resultados: 0.0001s\n",
      "5. Tempo total infer√™ncia: 0.0244s\n",
      "6. Tempo m√©dio por patch: 1.52ms\n",
      "7. Tempo total execu√ß√£o: 0.1183s\n",
      "8. Throughput: 656.43 patches/segundo\n",
      "============================================================\n",
      "üìÅ Resultados salvos em: /home/ampliar/cancer-classify-citology/citology-pipeline-Train/inferencias/results_HSIL_6 (7).jpg.txt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 1. VERIFICAR E CARREGAR MODELO\n",
    "    print(\"üîç Verificando o modelo...\")\n",
    "    test_model = load_model(str(MODEL_PATH), compile=False)\n",
    "    expected_shape = test_model.input_shape\n",
    "    ACTUAL_IMG_SIZE = (expected_shape[1], expected_shape[2])  # (height, width)\n",
    "    print(f\"‚úÖ Modelo espera input shape: {expected_shape}\")\n",
    "    print(f\"‚úÖ Usando tamanho: {ACTUAL_IMG_SIZE}\")\n",
    "    \n",
    "    # 2. CARREGAR MODELO PARA INFER√äNCIA\n",
    "    load_inference_model(\n",
    "        model_path=str(MODEL_PATH),\n",
    "        class_names=CLASS_NAMES,\n",
    "        img_size=ACTUAL_IMG_SIZE\n",
    "    )\n",
    "    \n",
    "    # 3. EXECUTAR INFER√äNCIA OTIMIZADA\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ INICIANDO INFER√äNCIA OTIMIZADA (PARALELA + BATCH)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = process_and_infer_hybrid(\n",
    "        image_path=str(IMAGE_INPUT_PATH),\n",
    "        output_dir_base=str(OUTPUT_INFERENCE_DIR),\n",
    "        save_tiles=True,\n",
    "        save_original_size=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_threads=NUM_THREADS\n",
    "    )\n",
    "    \n",
    "    # 4. MOSTRAR RESULTADOS DETALHADOS\n",
    "    if \"error\" in result:\n",
    "        print(f\"\\n‚ùå ERRO: {result['error']}\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚ú® RESULTADO FINAL DA INFER√äNCIA OTIMIZADA ‚ú®\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"üìÑ IMAGEM ANALISADA: {result['image_filename']}\")\n",
    "        print(f\"üî¢ TOTAL DE PATCHES: {result['good_patches_inferred']} / {result['total_patches_extracted']}\")\n",
    "        print(f\"\\nüè∑Ô∏è  CLASSIFICA√á√ÉO FINAL: **{result['final_prediction']}**\")\n",
    "        print(f\"üìà CONFIAN√áA: {result['confidence_score']:.4f}\")\n",
    "        \n",
    "        print(\"\\n--- üìä DETALHES DE PROBABILIDADES ---\")\n",
    "        for cls, prob in result['mean_probabilities'].items():\n",
    "            print(f\"  ‚Ä¢ {cls}: {prob:.4f}\")\n",
    "        \n",
    "        print(\"\\n--- ‚öôÔ∏è  INFORMA√á√ïES DE PROCESSAMENTO ---\")\n",
    "        print(f\"‚Ä¢ M√©todo de otimiza√ß√£o: {result['processing_info']['optimization_method']}\")\n",
    "        print(f\"‚Ä¢ Workers paralelos: {result['processing_info']['parallel_workers']}\")\n",
    "        print(f\"‚Ä¢ Batch size: {result['processing_info']['batch_size']}\")\n",
    "        print(f\"‚Ä¢ Batches processados: {result['processing_info']['total_batches_processed']}\")\n",
    "        print(f\"‚Ä¢ Tamanho dos tiles extra√≠dos: {result['processing_info']['tile_extraction_size']}\")\n",
    "        print(f\"‚Ä¢ Tamanho para infer√™ncia: {result['processing_info']['model_input_size']}\")\n",
    "        print(f\"‚Ä¢ Redimensionamento: {'SIM' if result['processing_info']['resize_applied'] else 'N√ÉO'}\")\n",
    "        \n",
    "        print(\"\\n--- ‚è±Ô∏è  AN√ÅLISE DE DESEMPENHO DETALHADA ---\")\n",
    "        print(f\"1. Extra√ß√£o de patches: {result['time_breakdown']['extraction_time_s']:.4f}s\")\n",
    "        print(f\"2. Pr√©-processamento (paralelo): {result['time_breakdown']['preprocess_time_s']:.4f}s\")\n",
    "        print(f\"3. Infer√™ncia (batch GPU): {result['time_breakdown']['inference_time_s']:.4f}s\")\n",
    "        print(f\"4. Agrega√ß√£o de resultados: {result['time_breakdown']['aggregation_time_s']:.4f}s\")\n",
    "        print(f\"5. Tempo total infer√™ncia: {result['time_breakdown']['total_inference_time_s']:.4f}s\")\n",
    "        print(f\"6. Tempo m√©dio por patch: {result['time_breakdown']['avg_inference_per_patch_ms']:.2f}ms\")\n",
    "        print(f\"7. Tempo total execu√ß√£o: {result['time_breakdown']['total_runtime_s']:.4f}s\")\n",
    "        \n",
    "        # C√°lculo de speedup estimado\n",
    "        patches_per_second = result['good_patches_inferred'] / result['time_breakdown']['inference_time_s'] if result['time_breakdown']['inference_time_s'] > 0 else 0\n",
    "        print(f\"8. Throughput: {patches_per_second:.2f} patches/segundo\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Salvar resultados em arquivo para an√°lise posterior\n",
    "        results_file = os.path.join(OUTPUT_INFERENCE_DIR, f\"results_{result['image_filename']}.txt\")\n",
    "        with open(results_file, 'w') as f:\n",
    "            f.write(f\"Image: {result['image_filename']}\\n\")\n",
    "            f.write(f\"Prediction: {result['final_prediction']} (confidence: {result['confidence_score']:.4f})\\n\")\n",
    "            f.write(f\"Patches: {result['good_patches_inferred']}/{result['total_patches_extracted']}\\n\")\n",
    "            f.write(f\"Total time: {result['time_breakdown']['total_runtime_s']:.4f}s\\n\")\n",
    "        \n",
    "        print(f\"üìÅ Resultados salvos em: {results_file}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n‚ö†Ô∏è ERRO: Certifique-se de que o modelo e a imagem existem nos caminhos definidos.\")\n",
    "    print(e)\n",
    "except ValueError as e:\n",
    "    print(f\"\\n‚ö†Ô∏è ERRO: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERRO FATAL: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train-pipeline-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
